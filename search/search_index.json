{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>The laplace package facilitates the application of Laplace approximations for entire neural networks, subnetworks of neural networks, or just their last layer. The package enables posterior approximations, marginal-likelihood estimation, and various posterior predictive computations. The library documentation is available at https://aleximmer.github.io/Laplace.</p> <p>There is also a corresponding paper, Laplace Redux \u2014 Effortless Bayesian Deep Learning, which introduces the library, provides an introduction to the Laplace approximation, reviews its use in deep learning, and empirically demonstrates its versatility and competitiveness. Please consider referring to the paper when using our library:</p> <pre><code>@inproceedings{laplace2021,\n  title={Laplace Redux--Effortless {B}ayesian Deep Learning},\n  author={Erik Daxberger and Agustinus Kristiadi and Alexander Immer\n          and Runa Eschenhagen and Matthias Bauer and Philipp Hennig},\n  booktitle={{N}eur{IPS}},\n  year={2021}\n}\n</code></pre> <p>The code to reproduce the experiments in the paper is also publicly available; it provides examples of how to use our library for predictive uncertainty quantification, model selection, and continual learning.</p> <p>Important</p> <p>As a user, one should not expect Laplace to work automatically. That is, one should experiment with different Laplace's options (Hessian factorization, prior precision tuning method, predictive method, backend, etc!). Try looking at various papers that use Laplace for references on how to set all those options depending on the applications/problems at hand.</p>"},{"location":"#setup","title":"Setup","text":"<p>Important</p> <p>We assume Python &gt;= 3.9 since lower versions are (soon to be) deprecated. PyTorch version 2.0 and up is also required for full compatibility.</p> <p>To install laplace with <code>pip</code>, run the following:</p> <pre><code>pip install laplace-torch\n</code></pre> <p>Additionally, if you want to use the <code>asdfghjkl</code> backend, please install it via:</p> <pre><code>pip install git+https://git@github.com/wiseodd/asdl@asdfghjkl\n</code></pre>"},{"location":"#quickstart","title":"Quickstart","text":""},{"location":"#simple-usage","title":"Simple usage","text":"<p>In the following example, a pre-trained model is loaded, then the Laplace approximation is fit to the training data (using a diagonal Hessian approximation over all parameters), and the prior precision is optimized with cross-validation <code>\"gridsearch\"</code>. After that, the resulting LA is used for prediction with the <code>\"probit\"</code> predictive for classification.</p> <p>Important</p> <p>Laplace expects all data loaders, e.g. <code>train_loader</code> and <code>val_loader</code> below, to be instances of PyTorch <code>DataLoader</code>. Each batch, <code>next(iter(data_loader))</code> must either be the standard <code>(X, y)</code> tensors or a dict-like object containing at least the keys specified in <code>dict_key_x</code> and <code>dict_key_y</code> in Laplace's constructor.</p> <p>Important</p> <p>The total number of data points in all data loaders must be accessible via <code>len(train_loader.dataset)</code>.</p> <p>Important</p> <p>In <code>optimize_prior_precision</code>, make sure to match the arguments with the ones you want to pass in <code>la(x, ...)</code> during prediction.</p> <pre><code>from laplace import Laplace\n\n# Pre-trained model\nmodel = load_map_model()\n\n# User-specified LA flavor\nla = Laplace(model, \"classification\",\n             subset_of_weights=\"all\",\n             hessian_structure=\"diag\")\nla.fit(train_loader)\nla.optimize_prior_precision(\n    method=\"gridsearch\",\n    pred_type=\"glm\",\n    link_approx=\"probit\",\n    val_loader=val_loader\n)\n\n# User-specified predictive approx.\npred = la(x, pred_type=\"glm\", link_approx=\"probit\")\n</code></pre>"},{"location":"#marginal-likelihood","title":"Marginal likelihood","text":"<p>The marginal likelihood can be used for model selection [10] and is differentiable for continuous hyperparameters like the prior precision or observation noise. Here, we fit the library default, KFAC last-layer LA and differentiate the log marginal likelihood.</p> <pre><code>from laplace import Laplace\n\n# Un- or pre-trained model\nmodel = load_model()\n\n# Default to recommended last-layer KFAC LA:\nla = Laplace(model, likelihood=\"regression\")\nla.fit(train_loader)\n\n# ML w.r.t. prior precision and observation noise\nml = la.log_marginal_likelihood(prior_prec, obs_noise)\nml.backward()\n</code></pre>"},{"location":"#laplace-on-llms","title":"Laplace on LLMs","text":"<p>Tip</p> <p>This library also supports Huggingface models and parameter-efficient fine-tuning. See Huggingface LLM example for the full exposition.</p> <p>First, we need to wrap the pretrained model so that the <code>forward</code> method takes a dict-like input. Note that when you iterate over a Huggingface dataloader, this is what you get by default. Having a dict-like input is nice since different models have different number of inputs (e.g. GPT-like LLMs only take <code>input_ids</code>, while BERT-like ones take both <code>input_ids</code> and <code>attention_mask</code>, etc.). Inside this <code>forward</code> method you can do your usual preprocessing like moving the tensor inputs into the correct device.</p> <pre><code>class MyGPT2(nn.Module):\n    def __init__(self, tokenizer: PreTrainedTokenizer) -    None:\n        super().__init__()\n        config = GPT2Config.from_pretrained(\"gpt2\")\n        config.pad_token_id = tokenizer.pad_token_id\n        config.num_labels = 2\n        self.hf_model = GPT2ForSequenceClassification.from_pretrained(\n            \"gpt2\", config=config\n        )\n\n    def forward(self, data: MutableMapping) -    torch.Tensor:\n        device = next(self.parameters()).device\n        input_ids = data[\"input_ids\"].to(device)\n        attn_mask = data[\"attention_mask\"].to(device)\n        output_dict = self.hf_model(input_ids=input_ids, attention_mask=attn_mask)\n        return output_dict.logits\n</code></pre> <p>Then you can \"select\" which parameters of the LLM you want to apply the Laplace approximation on, by switching off the gradients of the \"unneeded\" parameters. For example, we can replicate a last-layer Laplace: (in actual practice, use <code>Laplace(..., subset_of_weights='last_layer', ...)</code> instead, though!)</p> <pre><code>model = MyGPT2(tokenizer)\nmodel.eval()\n\n# Enable grad only for the last layer\nfor p in model.hf_model.parameters():\n    p.requires_grad = False\nfor p in model.hf_model.score.parameters():\n    p.requires_grad = True\n\nla = Laplace(\n    model,\n    likelihood=\"classification\",\n    # Will only hit the last-layer since it's the only one that is grad-enabled\n    subset_of_weights=\"all\",\n    hessian_structure=\"diag\",\n)\nla.fit(dataloader)\nla.optimize_prior_precision()\n\ntest_data = next(iter(dataloader))\npred = la(test_data)\n</code></pre> <p>This is useful because we can apply the LA only on the parameter-efficient finetuning weights. E.g., we can fix the LLM itself, and apply the Laplace approximation only on the LoRA weights. Huggingface will automatically switch off the non-LoRA weights' gradients.</p> <pre><code>def get_lora_model():\n    model = MyGPT2(tokenizer)  # Note we don't disable grad\n    config = LoraConfig(\n        r=4,\n        lora_alpha=16,\n        target_modules=[\"c_attn\"],  # LoRA on the attention weights\n        lora_dropout=0.1,\n        bias=\"none\",\n    )\n    lora_model = get_peft_model(model, config)\n    return lora_model\n\nlora_model = get_lora_model()\n\n# Train it as usual here...\n\nlora_model.eval()\n\nlora_la = Laplace(\n    lora_model,\n    likelihood=\"classification\",\n    subset_of_weights=\"all\",\n    hessian_structure=\"diag\",\n    backend=AsdlGGN,\n)\n\ntest_data = next(iter(dataloader))\nlora_pred = lora_la(test_data)\n</code></pre>"},{"location":"#subnetwork-laplace","title":"Subnetwork Laplace","text":"<p>This example shows how to fit the Laplace approximation over only a subnetwork within a neural network (while keeping all other parameters fixed at their MAP estimates), as proposed in [11]. It also exemplifies different ways to specify the subnetwork to perform inference over.</p> <p>First, we make use of <code>SubnetLaplace</code>, where we specify the subnetwork by generating a list of indices for the active model parameters.</p> <pre><code>from laplace import Laplace\n\n# Pre-trained model\nmodel = load_model()\n\n# Examples of different ways to specify the subnetwork\n# via indices of the vectorized model parameters\n#\n# Example 1: select the 128 parameters with the largest magnitude\nfrom laplace.utils import LargestMagnitudeSubnetMask\nsubnetwork_mask = LargestMagnitudeSubnetMask(model, n_params_subnet=128)\nsubnetwork_indices = subnetwork_mask.select()\n\n# Example 2: specify the layers that define the subnetwork\nfrom laplace.utils import ModuleNameSubnetMask\nsubnetwork_mask = ModuleNameSubnetMask(model, module_names=[\"layer.1\", \"layer.3\"])\nsubnetwork_mask.select()\nsubnetwork_indices = subnetwork_mask.indices\n\n# Example 3: manually define the subnetwork via custom subnetwork indices\nimport torch\nsubnetwork_indices = torch.tensor([0, 4, 11, 42, 123, 2021])\n\n# Define and fit subnetwork LA using the specified subnetwork indices\nla = Laplace(model, \"classification\",\n             subset_of_weights=\"subnetwork\",\n             hessian_structure=\"full\",\n             subnetwork_indices=subnetwork_indices)\nla.fit(train_loader)\n</code></pre> <p>Besides <code>SubnetLaplace</code>, you can, as already mentioned, also treat the last layer only using <code>Laplace(..., subset_of_weights='last_layer')</code>, which uses <code>LLLaplace</code>. As a third method, you may define a subnetwork by disabling gradients of fixed model parameters. The different methods target different use cases. Each method has pros and cons, please see this discussion for details. In summary</p> <ul> <li>Disable-grad: General method to perform Laplace on specific types of   layer/parameter, e.g. in an LLM with LoRA. Can be used to emulate <code>LLLaplace</code>   as well. Always use <code>subset_of_weights='all'</code> for this method.</li> <li>subnet selection by disabling grads is more efficient than     <code>SubnetLaplace</code> since it avoids calculating full Jacobians first</li> <li>disabling grads can only be performed on <code>Parameter</code> level and not for     individual weights, so this doesn't cover all cases that <code>SubnetLaplace</code>     offers such as <code>Largest*SubnetMask</code> or <code>RandomSubnetMask</code></li> <li><code>LLLaplace</code>: last-layer specific code with improved performance (#145)</li> <li><code>SubnetLaplace</code>: more fine-grained partitioning such as   <code>LargestMagnitudeSubnetMask</code></li> </ul>"},{"location":"#serialization","title":"Serialization","text":"<p>As with plain <code>torch</code>, we support to ways to serialize data.</p> <p>One is the familiar <code>state_dict</code> approach. Here you need to save and re-create both <code>model</code> and <code>Laplace</code>. Use this for long-term storage of models and sharing of a fitted <code>Laplace</code> instance.</p> <pre><code># Save model and Laplace instance\ntorch.save(model.state_dict(), \"model_state_dict.bin\")\ntorch.save(la.state_dict(), \"la_state_dict.bin\")\n\n# Load serialized data\nmodel2 = MyModel(...)\nmodel2.load_state_dict(torch.load(\"model_state_dict.bin\"))\nla2 = Laplace(model2, \"classification\",\n              subset_of_weights=\"all\",\n              hessian_structure=\"diag\")\nla2.load_state_dict(torch.load(\"la_state_dict.bin\"))\n</code></pre> <p>The second approach is to save the whole <code>Laplace</code> object, including <code>self.model</code>. This is less verbose and more convenient since you have the trained model and the fitted <code>Laplace</code> data stored in one place, but also comes with some drawbacks. Use this for quick save-load cycles during experiments, say.</p> <pre><code># Save Laplace, including la.model\ntorch.save(la, \"la.pt\")\n\n# Load both\ntorch.load(\"la.pt\")\n</code></pre> <p>Some Laplace variants such as <code>LLLaplace</code> might have trouble being serialized using the default <code>pickle</code> module, which <code>torch.save()</code> and <code>torch.load()</code> use (<code>AttributeError: Can't pickle local object ...</code>). In this case, the <code>dill</code> package will come in handy.</p> <pre><code>import dill\n\ntorch.save(la, \"la.pt\", pickle_module=dill)\n</code></pre> <p>With both methods, you are free to switch devices, for instance when you trained on a GPU but want to run predictions on CPU. In this case, use</p> <pre><code>torch.load(..., map_location=\"cpu\")\n</code></pre> <p>Warning</p> <p>Currently, this library always assumes that the model has an output tensor of shape <code>(batch_size, ..., n_classes)</code>, so in the case of image outputs, you need to rearrange from NCHW to NHWC.</p>"},{"location":"#when-to-use-which-backend","title":"When to use which backend","text":"<p>Tip</p> <p>Each backend as its own caveat/behavior. The use the following to guide you picking the suitable backend, depending on you model &amp; application.</p> <ul> <li>Small, simple MLP, or last-layer Laplace: Any backend should work well.   <code>CurvlinopsGGN</code> or <code>CurvlinopsEF</code> is recommended if   <code>hessian_factorization = 'kron'</code>, but it's inefficient for other factorizations.</li> <li>LLMs with PEFT (e.g. LoRA): <code>AsdlGGN</code> and <code>AsdlEF</code> are recommended.</li> <li>Continuous Bayesian optimization: <code>CurvlinopsGGN/EF</code> and <code>BackpackGGN/EF</code> are   recommended since they are the only ones supporting backprop over Jacobians.</li> </ul> <p>Caution</p> <p>The <code>curvlinops</code> backends are inefficient for full and diagonal factorizations. Moreover, they're also inefficient for computing the Jacobians of large models since they rely on <code>torch.func.jacrev</code> along <code>torch.func.vmap</code>! Finally, <code>curvlinops</code> only computes K-FAC (<code>hessian_factorization = 'kron'</code>) for <code>nn.Linear</code> and <code>nn.Conv2d</code> modules (including those inside larger modules like Attention).</p> <p>Caution</p> <p>The <code>BackPack</code> backends are limited to models expressed as <code>nn.Sequential</code>. Also, they're not compatible with normalization layers.</p>"},{"location":"#references","title":"References","text":"<p>This package relies on various improvements to the Laplace approximation for neural networks, which was originally due to MacKay [1]. Please consider citing the respective papers if you use any of their proposed methods via our laplace library.</p> <ul> <li>[1] MacKay, DJC. A Practical Bayesian Framework for Backpropagation Networks. Neural Computation 1992.</li> <li>[2] Gibbs, M. N. Bayesian Gaussian Processes for Regression and Classification. PhD Thesis 1997.</li> <li>[3] Snoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N., Patwary, M., Prabhat, M., Adams, R. Scalable Bayesian Optimization Using Deep Neural Networks. ICML 2015.</li> <li>[4] Ritter, H., Botev, A., Barber, D. A Scalable Laplace Approximation for Neural Networks. ICLR 2018.</li> <li>[5] Foong, A. Y., Li, Y., Hern\u00e1ndez-Lobato, J. M., Turner, R. E. 'In-Between' Uncertainty in Bayesian Neural Networks. ICML UDL Workshop 2019.</li> <li>[6] Khan, M. E., Immer, A., Abedi, E., Korzepa, M. Approximate Inference Turns Deep Networks into Gaussian Processes. NeurIPS 2019.</li> <li>[7] Kristiadi, A., Hein, M., Hennig, P. Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks. ICML 2020.</li> <li>[8] Immer, A., Korzepa, M., Bauer, M. Improving predictions of Bayesian neural nets via local linearization. AISTATS 2021.</li> <li>[9] Sharma, A., Azizan, N., Pavone, M. Sketching Curvature for Efficient Out-of-Distribution Detection for Deep Neural Networks. UAI 2021.</li> <li>[10] Immer, A., Bauer, M., Fortuin, V., R\u00e4tsch, G., Khan, EM. Scalable Marginal Likelihood Estimation for Model Selection in Deep Learning. ICML 2021.</li> <li>[11] Daxberger, E., Nalisnick, E., Allingham, JU., Antor\u00e1n, J., Hern\u00e1ndez-Lobato, JM. Bayesian Deep Learning via Subnetwork Inference. ICML 2021.</li> </ul>"},{"location":"calibration_example/","title":"Example: Calibration","text":"<p>An advantage of the Laplace approximation over variational Bayes and Markov Chain Monte Carlo methods is its post-hoc nature. That means we can apply LA on (almost) any pre-trained neural network. In this example, we will see how we can apply the last-layer LA on a deep WideResNet model, trained on CIFAR-10.</p>"},{"location":"calibration_example/#data-loading","title":"Data loading","text":"<p>First, let us load the CIFAR-10 dataset. The helper scripts for CIFAR-10 and WideResNet are available in the <code>examples/helper</code> directory in the main repository.</p> <pre><code>import torch\nimport torch.distributions as dists\nimport numpy as np\nimport helper.wideresnet as wrn\nimport helper.dataloaders as dl\nfrom helper import util\nfrom netcal.metrics import ECE\n\nfrom laplace import Laplace\n\n\nnp.random.seed(7777)\ntorch.manual_seed(7777)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = True\n\ntrain_loader = dl.CIFAR10(train=True)\ntest_loader = dl.CIFAR10(train=False)\ntargets = torch.cat([y for x, y in test_loader], dim=0).numpy()\n</code></pre>"},{"location":"calibration_example/#load-a-pre-trained-model","title":"Load a pre-trained model","text":"<p>Next, we will load a pre-trained WideResNet-16-4 model. Note that a GPU with CUDA support is needed for this example.</p> <pre><code># The model is a standard WideResNet 16-4\n# Taken as is from https://github.com/hendrycks/outlier-exposure\nmodel = wrn.WideResNet(16, 4, num_classes=10).cuda().eval()\n\nutil.download_pretrained_model()\nmodel.load_state_dict(torch.load(\"./temp/CIFAR10_plain.pt\"))\n</code></pre> <p>To simplify the downstream tasks, we will use the following helper function to make predictions. It simply iterates through all minibatches and obtains the predictive probabilities of the CIFAR-10 classes.</p> <pre><code>@torch.no_grad()\ndef predict(dataloader, model, laplace=False):\n    py = []\n\n    for x, _ in dataloader:\n        if laplace:\n            py.append(model(x.cuda()))\n        else:\n            py.append(torch.softmax(model(x.cuda()), dim=-1))\n\n    return torch.cat(py).cpu().numpy()\n</code></pre>"},{"location":"calibration_example/#the-calibration-of-map","title":"The calibration of MAP","text":"<p>We are now ready to see how calibrated is the model. The metrics we use are the expected calibration error (ECE, Naeni et al., AAAI 2015) and the negative (Categorical) log-likelihood. Note that lower values are better for both these metrics.</p> <p>First, let us inspect the MAP model. We shall use the <code>netcal</code> library to easily compute the ECE.</p> <pre><code>probs_map = predict(test_loader, model, laplace=False)\nacc_map = (probs_map.argmax(-1) == targets).float().mean()\nece_map = ECE(bins=15).measure(probs_map.numpy(), targets.numpy())\nnll_map = -dists.Categorical(probs_map).log_prob(targets).mean()\n\nprint(f\"[MAP] Acc.: {acc_map:.1%}; ECE: {ece_map:.1%}; NLL: {nll_map:.3}\")\n</code></pre> <p>Running this snippet, we would get:</p> <pre><code>[MAP] Acc.: 94.8%; ECE: 2.0%; NLL: 0.172\n</code></pre>"},{"location":"calibration_example/#the-calibration-of-laplace","title":"The calibration of Laplace","text":"<p>Now we inspect the benefit of the LA. Let us apply the simple last-layer LA model, and optimize the prior precision hyperparameter using a post-hoc marginal likelihood maximization.</p> <pre><code># Laplace\nla = Laplace(model, \"classification\",\n             subset_of_weights=\"last_layer\",\n             hessian_structure=\"kron\")\nla.fit(train_loader)\nla.optimize_prior_precision(method=\"marglik\")\n</code></pre> <p>Then, we are ready to see how well does LA improves the calibration of the MAP model:</p> <pre><code>probs_laplace = predict(test_loader, la, laplace=True)\nacc_laplace = (probs_laplace.argmax(-1) == targets).float().mean()\nece_laplace = ECE(bins=15).measure(probs_laplace.numpy(), targets.numpy())\nnll_laplace = -dists.Categorical(probs_laplace).log_prob(targets).mean()\n\nprint(f\"[Laplace] Acc.: {acc_laplace:.1%}; ECE: {ece_laplace:.1%}; NLL: {nll_laplace:.3}\")\n</code></pre> <p>Running this snippet, we obtain:</p> <pre><code>[Laplace] Acc.: 94.8%; ECE: 0.8%; NLL: 0.157\n</code></pre> <p>Notice that the last-layer LA does not do any harm to the accuracy, yet it improves the calibration of the MAP model substantially.</p>"},{"location":"calibration_gp_example/","title":"Example: GP Inference","text":"<p>Applying the General-Gauss-Newton (GGN) approximation to the Hessian in the Laplace approximation (LA) of the BNN posterior turns the underlying probabilistic model from a BNN into a generalized linear model (GLM). This GLM is equivalent to a Gaussian Process (GP) with a particular kernel [1, 2].</p> <p>In this notebook, we will show how to use <code>laplace</code> library to perform GP inference on top of a pre-trained neural network.</p> <p>Note that a GPU with CUDA support is needed for this example. We recommend using a GPU with at least 24 GB of memory. If less memory is available, we suggest reducing <code>BATCH_SIZE</code> below.</p>"},{"location":"calibration_gp_example/#data-loading","title":"Data loading","text":"<p>First, let us load the FMIST dataset. The helper scripts for FMNIST and pre-trained CNN are available in the <code>examples/helper</code> directory in the main repository.</p> <pre><code>import numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nimport torch.distributions as dists\nfrom netcal.metrics import ECE\n\nfrom helper.util_gp import get_dataset, CIFAR10Net\nfrom laplace import Laplace\n\nnp.random.seed(7777)\ntorch.manual_seed(7777)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = True\n\nassert torch.cuda.is_available()\n\nDATASET = 'FMNIST'\nBATCH_SIZE = 256\nds_train, ds_test = get_dataset(DATASET, False, 'cuda')\ntrain_loader = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(ds_test, batch_size=BATCH_SIZE, shuffle=False)\ntargets = torch.cat([y for x, y in test_loader], dim=0).cpu()\n</code></pre>"},{"location":"calibration_gp_example/#load-a-pre-trained-model","title":"Load a pre-trained model","text":"<p>Next, we load a pre-trained CNN model. The code to train the model can be found in BNN-predictions repo.</p> <pre><code>MODEL_NAME = 'FMNIST_CNN_10_2.2e+02.pt'\nmodel = CIFAR10Net(ds_train.channels, ds_train.K, use_tanh=True).to('cuda')\nstate = torch.load(f'helper/models/{MODEL_NAME}')\nmodel.load_state_dict(state['model'])\nmodel = model.cuda()\nprior_precision = state['delta']\n</code></pre> <p>To simplify the downstream tasks, we will use the following helper function to make predictions. It simply iterates through all minibatches and obtains the predictive probabilities of the FMNIST classes.</p> <pre><code>@torch.no_grad()\ndef predict(dataloader, model, laplace=False):\n    py = []\n\n    for x, _ in dataloader:\n        if laplace:\n            py.append(model(x.cuda()))\n        else:\n            py.append(torch.softmax(model(x.cuda()), dim=-1))\n\n    return torch.cat(py).cpu().numpy()\n</code></pre>"},{"location":"calibration_gp_example/#the-calibration-of-map","title":"The calibration of MAP","text":"<p>We are now ready to see how calibrated is the model. The metrics we use are the expected calibration error (ECE, Naeni et al., AAAI 2015) and the negative (Categorical) log-likelihood. Note that lower values are better for both these metrics.</p> <p>First, let us inspect the MAP model. We shall use the <code>netcal</code> library to easily compute the ECE.</p> <pre><code>probs_map = predict(test_loader, model, laplace=False)\nacc_map = (probs_map.argmax(-1) == targets).float().mean()\nece_map = ECE(bins=15).measure(probs_map.numpy(), targets.numpy())\nnll_map = -dists.Categorical(probs_map).log_prob(targets).mean()\n\nprint(f'[MAP] Acc.: {acc_map:.1%}; ECE: {ece_map:.1%}; NLL: {nll_map:.3}')\n</code></pre> <p>Running this snippet, we would get:</p> <pre><code>[MAP] Acc.: 91.7%; ECE: 1.6%; NLL: 0.253\n</code></pre>"},{"location":"calibration_gp_example/#the-calibration-of-laplace","title":"The calibration of Laplace","text":"<p>Next, we run Laplace-GP inference to calibrate neural network's predictions. Since running exact GP inference is computationally infeasible, we perform Subset-of-Datapoints (SoD) [3] approximation here. In the code below, <code>m</code>denotes the number of datapoints used in the SoD posterior.</p> <p>Execution of the cell below can take up to 5min (depending on the exact hardware used).</p> <pre><code>for m in [50, 200, 800, 1600]:\n    print(f'Fitting Laplace-GP for m={m}')\n    la = Laplace(model, 'classification',\n                 subset_of_weights='all',\n                 hessian_structure='gp',\n                 diagonal_kernel=True,\n                 num_data=m,\n                 prior_precision=prior_precision)\n    la.fit(train_loader)\n\n    probs_laplace = predict(test_loader, la, laplace=True)\n    acc_laplace = (probs_laplace.argmax(-1) == targets).float().mean()\n    ece_laplace = ECE(bins=15).measure(probs_laplace.numpy(), targets.numpy())\n    nll_laplace = -dists.Categorical(probs_laplace).log_prob(targets).mean()\n\n    print(f'[Laplace-GP, m={m}] Acc.: {acc_laplace:.1%}; ECE: {ece_laplace:.1%}; NLL: {nll_laplace:.3}')\n</code></pre> <pre><code>Fitting Laplace-GP for m=50\n[Laplace] Acc.: 91.6%; ECE: 1.5%; NLL: 0.252\nFitting Laplace-GP for m=200\n[Laplace] Acc.: 91.5%; ECE: 1.1%; NLL: 0.252\nFitting Laplace-GP for m=800\n[Laplace] Acc.: 91.4%; ECE: 0.8%; NLL: 0.254\nFitting Laplace-GP for m=1600\n[Laplace] Acc.: 91.3%; ECE: 0.7%; NLL: 0.257\n</code></pre> <p>Notice that the post-hoc Laplace-GP inference does not have a significant impact on the accuracy, yet it improves the calibration (in terms of ECE) of the MAP model substantially.</p>"},{"location":"calibration_gp_example/#references","title":"References","text":"<p>[1] Khan, Mohammad Emtiyaz E., et al. \"Approximate inference turns deep networks into gaussian processes.\" Advances in neural information processing systems 32 (2019)</p> <p>[2] Immer, Alexander, Maciej Korzepa, and Matthias Bauer. \"Improving predictions of Bayesian neural nets via local linearization.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2021</p> <p>[3] Rasmussen, Carl Edward. \"Gaussian processes in machine learning.\" Springer, 2004</p>"},{"location":"devs_guide/","title":"Developer's Guide","text":""},{"location":"devs_guide/#setup-dev-environment","title":"Setup dev environment","text":"<p>For development purposes, e.g. if you would like to make contributions, follow the following steps:</p> <p>With <code>uv</code></p> <ol> <li>Install <code>uv</code>, e.g. <code>pip install --upgrade uv</code></li> <li>Then clone this repository and install the development dependencies:</li> </ol> <pre><code>git clone git@github.com:aleximmer/Laplace.git\nuv sync --all-extras\n</code></pre> <ol> <li><code>laplace-torch</code> is now available in editable mode, e.g. you can run:</li> </ol> <pre><code>uv run python examples/regression_example.py\n\n# Or, equivalently:\nsource .venv/bin/activate\npython examples/regression_example.py\n</code></pre> <p>With <code>pip</code></p> <pre><code>git clone git@github.com:aleximmer/Laplace.git\n\n# Recommended to create a virtualenv before the following step\npip install -e \".[dev]\"\n\n# Run as usual, e.g.\npython examples/regression_examples.py\n</code></pre>"},{"location":"devs_guide/#contributing","title":"Contributing","text":"<p>Pull requests are very welcome. Please follow these guidelines:</p> <ol> <li>Follow the development setup.</li> <li>Use ruff as autoformatter. Please refer to the following makefile and run it via <code>make ruff</code>. Please note that the order of <code>ruff check --fix</code> and <code>ruff format</code> is important!</li> <li>Also use ruff as linter. Please manually fix all linting errors/warnings before opening a pull request.</li> <li>Fully document your changes in the form of Python docstrings, typehinting, and (if applicable) code/markdown examples in the <code>./examples</code> subdirectory.</li> <li>See <code>docs/api_reference/*.md</code> on how to include a newly added class in the docs.</li> <li>Provide as many test cases as possible. Make sure all test cases pass.</li> </ol> <p>Issues, bug reports, and ideas are also very welcome!</p>"},{"location":"devs_guide/#documentation","title":"Documentation","text":"<p>The documentation is available here or can be generated and/or viewed locally:</p> <p>With <code>uv</code></p> <pre><code># assuming the repository was cloned\nuv sync --all-extras\n# serve the docs locally\nuv run mkdocs serve\n</code></pre> <p>With <code>pip</code></p> <pre><code># assuming the repository was cloned\npip install -e \".[dev]\"\n# serve the docs locally\nmkdocs serve\n</code></pre>"},{"location":"devs_guide/#publishing-the-laplace-torch-package-to-pypi","title":"Publishing the <code>laplace-torch</code> package to PyPi","text":"<p>With <code>uv</code>, this is done via: https://docs.astral.sh/uv/guides/publish/.</p> <p>If you want to make your life much easier, you can use <code>pdm</code>:</p> <pre><code>pip install --upgrade pdm\npdm publish\n</code></pre>"},{"location":"devs_guide/#structure","title":"Structure","text":"<p>The laplace package consists of two main components:</p> <ol> <li>The subclasses of <code>laplace.BaseLaplace</code> that implement different sparsity structures: different subsets of weights (<code>'all'</code>, <code>'subnetwork'</code> and <code>'last_layer'</code>) and different structures of the Hessian approximation (<code>'full'</code>, <code>'kron'</code>, <code>'lowrank'</code>, <code>'diag'</code> and <code>'gp'</code>). This results in ten currently available options: <code>laplace.FullLaplace</code>, <code>laplace.KronLaplace</code>, <code>laplace.DiagLaplace</code>, <code>laplace.FunctionalLaplace</code> the corresponding last-layer variations <code>laplace.FullLLLaplace</code>, <code>laplace.KronLLLaplace</code>, <code>laplace.DiagLLLaplace</code> and <code>laplace.FunctionalLLLaplace</code> (which are all subclasses of <code>laplace.LLLaplace</code>), <code>laplace.SubnetLaplace</code> (which only supports <code>'full'</code> and <code>'diag'</code> Hessian approximations) and <code>laplace.LowRankLaplace</code> (which only supports inference over <code>'all'</code> weights). All of these can be conveniently accessed via the <code>laplace.Laplace</code> function.</li> <li>The backends in <code>laplace.curvature</code> which provide access to Hessian approximations of    the corresponding sparsity structures, for example, the diagonal GGN.</li> </ol> <p>Additionally, the package provides utilities for decomposing a neural network into feature extractor and last layer for <code>LLLaplace</code> subclasses (<code>laplace.utils.feature_extractor</code>) and effectively dealing with Kronecker factors (<code>laplace.utils.matrix</code>).</p> <p>Finally, the package implements several options to select/specify a subnetwork for <code>SubnetLaplace</code> (as subclasses of <code>laplace.utils.subnetmask.SubnetMask</code>). Automatic subnetwork selection strategies include: uniformly at random (<code>laplace.utils.subnetmask.RandomSubnetMask</code>), by largest parameter magnitudes (<code>LargestMagnitudeSubnetMask</code>), and by largest marginal parameter variances (<code>LargestVarianceDiagLaplaceSubnetMask</code> and <code>LargestVarianceSWAGSubnetMask</code>). In addition to that, subnetworks can also be specified manually, by listing the names of either the model parameters (<code>ParamNameSubnetMask</code>) or modules (<code>ModuleNameSubnetMask</code>) to perform Laplace inference over.</p>"},{"location":"devs_guide/#extendability","title":"Extendability","text":"<p>To extend the laplace package, new <code>BaseLaplace</code> subclasses can be designed, for example, Laplace with a block-diagonal Hessian structure. One can also implement custom subnetwork selection strategies as new subclasses of <code>SubnetMask</code>.</p> <p>Alternatively, extending or integrating backends (subclasses of <code>curvature.curvature</code>) allows to provide different Hessian approximations to the Laplace approximations. For example, currently the <code>curvature.CurvlinopsInterface</code> based on Curvlinops and the native <code>torch.func</code> (previously known as <code>functorch</code>), <code>curvature.BackPackInterface</code> based on BackPACK and <code>curvature.AsdlInterface</code> based on ASDL are available.</p>"},{"location":"huggingface_example/","title":"Example: Huggingface LLMs","text":"<p>In this example, we will see how to apply Laplace on a GPT2 Huggingface (HF) model. Laplace only has lightweight requirements for this; namely that the model's <code>forward</code> method must only take a single dict-like object (<code>dict</code>, <code>UserDict</code>, or in general, <code>collections.abc.MutableMapping</code>). This is entirely compatible with HF since HF's data loaders are assumed to emit an object derived from <code>UserDict</code>. However, you need to ensure this yourself --- you need to wrap the standard HF model to conform to that requirement. Also, you need to e.g. do <code>torch.to(device)</code> inside the said <code>forward</code> method.</p> <p>Let's start with as usual with importing stuff.</p> <pre><code>from collections.abc import MutableMapping\nfrom collections import UserDict\nimport numpy\nimport torch\nfrom torch import nn\nimport torch.utils.data as data_utils\n\nfrom laplace import Laplace\n\nimport logging\nimport warnings\n\nlogging.basicConfig(level=\"ERROR\")\nwarnings.filterwarnings(\"ignore\")\n\nfrom transformers import ( # noqa: E402\n    GPT2Config,\n    GPT2ForSequenceClassification,\n    GPT2Tokenizer,\n    DataCollatorWithPadding,\n    PreTrainedTokenizer,\n)\nfrom peft import LoraConfig, get_peft_model # noqa: E402\nfrom datasets import Dataset # noqa: E402\n\n# make deterministic\n\ntorch.manual_seed(0)\nnumpy.random.seed(0)\n</code></pre> <p>Next, we create a toy dataset. You can use any HF datasets or your own, of course.</p> <pre><code>tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\ndata = [\n    {\"text\": \"Today is hot, but I will manage!!!!\", \"label\": 1},\n    {\"text\": \"Tomorrow is cold\", \"label\": 0},\n    {\"text\": \"Carpe diem\", \"label\": 1},\n    {\"text\": \"Tempus fugit\", \"label\": 1},\n]\ndataset = Dataset.from_list(data)\n\ndef tokenize(row):\n    return tokenizer(row[\"text\"])\n\ndataset = dataset.map(tokenize, remove_columns=[\"text\"])\ndataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\ndataloader = data_utils.DataLoader(\n    dataset, batch_size=100, collate_fn=DataCollatorWithPadding(tokenizer)\n)\n\ndata = next(iter(dataloader))\nprint(\n    f\"Huggingface data defaults to UserDict, which is a MutableMapping? {isinstance(data, UserDict)}\"\n)\nfor k, v in data.items():\n    print(k, v.shape)\n</code></pre> <p>This is the output:</p> <pre><code>Huggingface data defaults to UserDict, which is a MutableMapping? True\ninput_ids torch.Size([4, 9])\nattention_mask torch.Size([4, 9])\nlabels torch.Size([4])\n</code></pre>"},{"location":"huggingface_example/#laplace-on-a-subset-of-an-llms-weights","title":"Laplace on a subset of an LLM's weights","text":"<p>Now, let's do the main \"meat\" of this example: Wrapping the HF model into a model that is compatible with Laplace. Notice that this wrapper just wraps the HF model and nothing else. Notice also we do <code>inputs.to(device)</code> inside <code>self.forward()</code>.</p> <pre><code>class MyGPT2(nn.Module):\n    \"\"\"\n    Huggingface LLM wrapper.\n\n    Args:\n        tokenizer: The tokenizer used for preprocessing the text data. Needed\n            since the model needs to know the padding token id.\n    \"\"\"\n\n    def __init__(self, tokenizer: PreTrainedTokenizer) -&gt; None:\n        super().__init__()\n        config = GPT2Config.from_pretrained(\"gpt2\")\n        config.pad_token_id = tokenizer.pad_token_id\n        config.num_labels = 2\n        self.hf_model = GPT2ForSequenceClassification.from_pretrained(\n            \"gpt2\", config=config\n        )\n\n    def forward(self, data: MutableMapping) -&gt; torch.Tensor:\n        \"\"\"\n        Custom forward function. Handles things like moving the\n        input tensor to the correct device inside.\n\n        Args:\n            data: A dict-like data structure with `input_ids` inside.\n                This is the default data structure assumed by Huggingface\n                dataloaders.\n\n        Returns:\n            logits: An `(batch_size, n_classes)`-sized tensor of logits.\n        \"\"\"\n        device = next(self.parameters()).device\n        input_ids = data[\"input_ids\"].to(device)\n        attn_mask = data[\"attention_mask\"].to(device)\n        output_dict = self.hf_model(input_ids=input_ids, attention_mask=attn_mask)\n        return output_dict.logits\n\nmodel = MyGPT2(tokenizer)\n</code></pre> <p>Now, let's apply Laplace. Let's do a last-layer Laplace first. Notice that we add an argument <code>feature_reduction</code> there. This is because Huggingface models reduce the logits and not the features.</p> <pre><code>model = MyGPT2(tokenizer)\nmodel.eval()\n\nla = Laplace(\n    model,\n    likelihood=\"classification\",\n    subset_of_weights=\"last_layer\",\n    hessian_structure=\"full\",\n    # This must reflect faithfully the reduction technique used in the model\n    # Otherwise, correctness is not guaranteed\n    feature_reduction=\"pick_last\",\n)\nla.fit(dataloader)\nla.optimize_prior_precision()\n\nX_test = next(iter(dataloader))\nprint(f\"[Last-layer Laplace] The predictive tensor is of shape: {la(X_test).shape}.\")\n</code></pre> <p>Here's the output:</p> <pre><code>[Last-layer Laplace] The predictive tensor is of shape: torch.Size([4, 2]).\n</code></pre>"},{"location":"huggingface_example/#subnetwork-laplace","title":"Subnetwork Laplace","text":"<p>Also, we can do the same thing by switching off the gradients of all layers except the top layer. Laplace will automatically only compute the Hessian (and Jacobians) of the parameters in which <code>requires_grad</code> is <code>True</code>.</p> <p>Notice that you can \"mix-and-match\" this gradient switching. You can do a subnetwork Laplace easily by doing so!</p> <pre><code>model.eval()\n\n# Enable grad only for the last layer\n\nfor p in model.hf_model.parameters():\n    p.requires_grad = False\n\nfor p in model.hf_model.score.parameters():\n    p.requires_grad = True\n\nla = Laplace(\n    model,\n    # Will only hit the last-layer since it's the only one that is grad-enabled\n    likelihood=\"classification\",\n    subset_of_weights=\"all\",\n    hessian_structure=\"diag\",\n)\nla.fit(dataloader)\nla.optimize_prior_precision()\n\nX_test = next(iter(dataloader))\nprint(f\"[Subnetwork Laplace] The predictive tensor is of shape: {la(X_test).shape}.\")\n</code></pre> <p>Here are the outputs to validate that Laplace works:</p> <pre><code>[Subnetwork Laplace] The predictive tensor is of shape: torch.Size([4, 2]).\n</code></pre>"},{"location":"huggingface_example/#full-laplace-on-lora-parameters-only","title":"Full Laplace on LoRA parameters only","text":"<p>Of course, you can also apply Laplace on the parameter-efficient fine tuning weights (like LoRA). To do this, simply extend your LLM with LoRA, using HF's <code>peft</code> library, and apply Laplace as usual. Note that <code>peft</code> automatically switches off the non-LoRA weights.</p> <pre><code>def get_lora_model():\n    model = MyGPT2(tokenizer) # Note we don't disable grad\n    config = LoraConfig(\n        r=4,\n        lora_alpha=16,\n        target_modules=[\"c_attn\"], # LoRA on the attention weights\n        lora_dropout=0.1,\n        bias=\"none\",\n    )\n    lora_model = get_peft_model(model, config)\n    return lora_model\n\nlora_model = get_lora_model()\n\n# Train it as usual\n\nlora_model.eval()\n\nlora_la = Laplace(\n    lora_model,\n    likelihood=\"classification\",\n    subset_of_weights=\"all\",\n    hessian_structure=\"kron\",\n)\nlora_la.fit(dataloader)\n\nX_test = next(iter(dataloader))\nprint(f\"[LoRA-LLM] The predictive tensor is of shape: {lora_la(X_test).shape}.\")\n</code></pre> <p>Here is the output, as expected:</p> <pre><code>[LoRA-LLM] The predictive tensor is of shape: torch.Size([4, 2]).\n</code></pre> <p>As a final note, the dict-like input requirement of Laplace is very flexible. It can essentially be applicable to any tasks and any models. You just need to wrap the said model and make sure that your data loaders emit dict-like objects, where the input tensors are the dicts' values.</p>"},{"location":"huggingface_example/#caveats","title":"Caveats","text":"<p>Currently, diagonal EF with the Curvlinops backend is unsupported for dict-based inputs. This is because we use <code>torch.func</code>'s <code>vmap</code> to compute the diag-EF, and it only accepts tensor input in the model's <code>forward</code>. See this issue. So, if you can write down your Huggingface model's <code>forward</code> to accept only a single tensor, this is much preferable.</p> <p>For instance, in the case of causal LLM like GPTs, only <code>input_ids</code> tensor is necessary. Then, any backend and any hessian factorization can be used in this case.</p> <p>Otherwise, if you must use dict-based inputs, choose the following backends:</p> <ul> <li><code>CurvlinopsGGN</code> for <code>hessian_factorization = {\"kron\", \"diag\"}</code></li> <li><code>CurvlinopsEF</code> for <code>hessian_factorization = {\"kron\"}</code></li> <li><code>AsdlGGN</code> for <code>hessian_factorization = {\"kron\", \"diag\"}</code></li> <li><code>AsdlEF</code> for <code>hessian_factorization = {\"kron\", \"diag\"}</code></li> </ul>"},{"location":"regression_example/","title":"Example: Regression","text":""},{"location":"regression_example/#sinusoidal-toy-data","title":"Sinusoidal toy data","text":"<p>We show how the marginal likelihood can be used after training a MAP network on a simple sinusoidal regression task. Subsequently, we use the optimized LA to predict which provides uncertainty on top of the MAP prediction. We also show how the <code>marglik_training</code> utility method can be used to jointly train the MAP and hyperparameters. First, we set up the training data for the problem with observation noise \\(\\sigma=0.3\\):</p> <pre><code>from laplace.baselaplace import FullLaplace\nfrom laplace.curvature.backpack import BackPackGGN\nimport numpy as np\nimport torch\n\nfrom laplace import Laplace, marglik_training\n\nfrom helper.dataloaders import get_sinusoid_example\nfrom helper.util import plot_regression\n\nn_epochs = 1000\ntorch.manual_seed(711)\n# sample toy data example\nX_train, y_train, train_loader, X_test = get_sinusoid_example(sigma_noise=0.3)\n</code></pre>"},{"location":"regression_example/#training-a-map","title":"Training a MAP","text":"<p>We now use <code>pytorch</code> to train a neural network with single hidden layer and Tanh activation. The trained neural network will be our MAP estimate. This is standard so nothing new here, yet:</p> <pre><code># create and train MAP model\ndef get_model():\n    torch.manual_seed(711)\n    return torch.nn.Sequential(\n        torch.nn.Linear(1, 50), torch.nn.Tanh(), torch.nn.Linear(50, 1)\n    )\nmodel = get_model()\n\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\nfor i in range(n_epochs):\n    for X, y in train_loader:\n        optimizer.zero_grad()\n        loss = criterion(model(X), y)\n        loss.backward()\n        optimizer.step()\n</code></pre>"},{"location":"regression_example/#fitting-and-optimizing-the-laplace-approximation-using-empirical-bayes","title":"Fitting and optimizing the Laplace approximation using empirical Bayes","text":"<p>With the MAP-trained model at hand, we can estimate the prior precision and observation noise using empirical Bayes after training. The <code>Laplace</code> method is called to construct a LA for <code>\"regression\"</code> with <code>\"all\"</code> weights. As default <code>Laplace</code> returns a Kronecker factored LA, we use <code>\"full\"</code> instead on this small example. We fit the LA to the training data and initialize <code>log_prior</code> and <code>log_sigma</code>. Using Adam, we minimize the negative log marginal likelihood for <code>n_epochs</code>.</p> <pre><code>la = Laplace(model, \"regression\", subset_of_weights=\"all\", hessian_structure=\"full\")\nla.fit(train_loader)\nlog_prior, log_sigma = torch.ones(1, requires_grad=True), torch.ones(1, requires_grad=True)\nhyper_optimizer = torch.optim.Adam([log_prior, log_sigma], lr=1e-1)\nfor i in range(n_epochs):\n    hyper_optimizer.zero_grad()\n    neg_marglik = - la.log_marginal_likelihood(log_prior.exp(), log_sigma.exp())\n    neg_marglik.backward()\n    hyper_optimizer.step()\n</code></pre> <p>The obtained observation noise is close to the ground truth with a value of \\(\\sigma \\approx 0.28\\) without the need for any validation data. The resulting prior precision is \\(\\delta \\approx 0.10\\).</p>"},{"location":"regression_example/#bayesian-predictive","title":"Bayesian predictive","text":"<p>Here, we compare the MAP prediction to the obtained LA prediction. For LA, we have a closed-form predictive distribution on the output \\(f\\) which is a Gaussian \\(\\mathcal{N}(f(x;\\theta\\_{MAP}), \\mathbb{V}[f] + \\sigma^2)\\):</p> <pre><code>x = X_test.flatten().cpu().numpy()\nf_mu, f_var = la(X_test)\nf_mu = f_mu.squeeze().detach().cpu().numpy()\nf_sigma = f_var.squeeze().sqrt().cpu().numpy()\npred_std = np.sqrt(f_sigma**2 + la.sigma_noise.item()**2)\n\nplot_regression(X_train, y_train, x, f_mu, pred_std)\n</code></pre> <p></p> <p>In comparison to the MAP, the predictive shows useful uncertainties. When our MAP is over or underfit, the Laplace approximation cannot fix this anymore. In this case, joint optimization of MAP and marginal likelihood can be useful.</p>"},{"location":"regression_example/#jointly-optimize-map-and-hyperparameters-using-online-empirical-bayes","title":"Jointly optimize MAP and hyperparameters using online empirical Bayes","text":"<p>We provide a utility method <code>marglik_training</code> that implements the algorithm proposed in [1]. The method optimizes the neural network and the hyperparameters in an interleaved way and returns an optimally regularized LA. Below, we use this method and plot the corresponding predictive uncertainties again:</p> <pre><code>model = get_model()\nla, model, margliks, losses = marglik_training(\n    model=model, train_loader=train_loader, likelihood=\"regression\",\n    hessian_structure=\"full\", backend=BackPackGGN, n_epochs=n_epochs,\n    optimizer_kwargs={\"lr\": 1e-2}, prior_structure=\"scalar\"\n)\n\nf_mu, f_var = la(X_test)\nf_mu = f_mu.squeeze().detach().cpu().numpy()\nf_sigma = f_var.squeeze().sqrt().cpu().numpy()\npred_std = np.sqrt(f_sigma**2 + la.sigma_noise.item()**2)\n\nplot_regression(X_train, y_train, x, f_mu, pred_std)\n</code></pre> <p></p>"},{"location":"reward_modeling_example/","title":"Example: Reward Modeling","text":"<p>The <code>laplace-torch</code> library can also be used to \"Bayesianize\" a pretrained Bradley-Terry reward model, popular in large language models. See http://arxiv.org/abs/2009.01325 for a primer in reward modeling.</p>"},{"location":"reward_modeling_example/#defining-a-preference-dataset","title":"Defining a preference dataset","text":"<p>First order of business, let's define our comparison dataset. We will use the <code>datasets</code> library from Huggingface to handle the data.</p> <pre><code>import numpy as np\nimport torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nimport torch.utils.data as data_utils\n\nfrom datasets import Dataset\n\nfrom laplace import Laplace\n\nimport logging\nimport warnings\n\nlogging.basicConfig(level=\"ERROR\")\nwarnings.filterwarnings(\"ignore\")\n\n# make deterministic\ntorch.manual_seed(0)\nnp.random.seed(0)\n\n\n# Pairwise comparison dataset. The label indicates which `x0` or `x1` is preferred.\ndata_dict = [\n    {\n        \"x0\": torch.randn(3),\n        \"x1\": torch.randn(3),\n        \"label\": torch.randint(2, size=(1,)).item(),\n    }\n    for _ in range(10)\n]\ndataset = Dataset.from_list(data_dict)\n</code></pre>"},{"location":"reward_modeling_example/#defining-a-reward-model","title":"Defining a reward model","text":"<p>Now, let's define the reward model. During training, it assumes that <code>x</code> is a tensor of shape <code>(batch_size, 2, dim)</code>, which is a concatenation of <code>x0</code> and <code>x1</code> above. The second dimension of size 2 is preserved through the forward pass, resulting in a logit tensor of shape <code>(batch_size, 2)</code> (the network itself is single-output). Then, the standard cross-entropy loss is applied.</p> <p>Note that this requirement is quite weak and can covers general cases. However, if you prefer to use the dict-like inputs as in Huggingface LLM models, this can also be done. Simply combine what you have learned from this example with the Huggingface LLM example provided in this library.</p> <p>During testing, this model behaves like a standard single-output regression model.</p> <pre><code>class SimpleRewardModel(nn.Module):\n    \"\"\"A simple reward model, compatible with the Bradley-Terry likelihood.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(3, 100), nn.ReLU(), nn.Linear(100, 1))\n\n    def forward(self, x):\n        \"\"\"Args:\n            x: torch.Tensor\n                If training == True then shape (batch_size, 2, dim)\n                Else shape (batch_size, dim)\n\n        Returns:\n            logits: torch.Tensor\n                If training then shape (batch_size, 2)\n                Else shape (batch_size, 1)\n        \"\"\"\n        if len(x.shape) == 3:\n            batch_size, _, dim = x.shape\n\n            # Flatten to (batch_size*2, dim)\n            flat_x = x.reshape(-1, dim)\n\n            # Forward\n            flat_logits = self.net(flat_x)  # (batch_size*2, 1)\n\n            # Reshape back to (batch_size, 2)\n            return flat_logits.reshape(batch_size, 2)\n        else:\n            logits = self.net(x)  # (batch_size, 1)\n            return logits\n</code></pre>"},{"location":"reward_modeling_example/#data-preprocessing","title":"Data preprocessing","text":"<p>To fulfill the 3D tensor requirement, we need to preprocess the dict-based dataset.</p> <pre><code># Preprocess to coalesce x0 and x1 into a single array/tensor\ndef append_x0_x1(row):\n    # The tensor values above are automatically casted as lists by `Dataset`\n    row[\"x\"] = np.stack([row[\"x0\"], row[\"x1\"]])  # (2, dim)\n    return row\n\n\ntensor_dataset = dataset.map(append_x0_x1, remove_columns=[\"x0\", \"x1\"])\ntensor_dataset.set_format(type=\"torch\", columns=[\"x\", \"label\"])\ntensor_dataloader = data_utils.DataLoader(\n    data_utils.TensorDataset(tensor_dataset[\"x\"], tensor_dataset[\"label\"]), batch_size=3\n)\n</code></pre>"},{"location":"reward_modeling_example/#map-training","title":"MAP training","text":"<p>Then, we can train as usual using the cross entropy loss.</p> <pre><code>reward_model = SimpleRewardModel()\nopt = optim.AdamW(reward_model.parameters(), weight_decay=1e-3)\n\n# Train as usual\nfor epoch in range(10):\n    for x, y in tensor_dataloader:\n        opt.zero_grad()\n        out = reward_model(x)\n        loss = F.cross_entropy(out, y)\n        loss.backward()\n        opt.step()\n</code></pre>"},{"location":"reward_modeling_example/#applying-laplace","title":"Applying Laplace","text":"<p>Applying Laplace to this model is a breeze. Simply state that the likelihood is <code>reward_modeling</code>.</p> <pre><code># Laplace !!! Notice the likelihood !!!\nreward_model.eval()\nla = Laplace(reward_model, likelihood=\"reward_modeling\", subset_of_weights=\"all\")\nla.fit(tensor_dataloader)\nla.optimize_prior_precision()\n</code></pre> <p>As we can see, during prediction, even though we train &amp; fit Laplace using the cross entropy loss (i.e. classification), in test time, the model behaves like a regression model. So, you don't get probability vectors as outputs. Instead, you get two tensors containing the predictive means and predictive variance.</p> <pre><code>x_test = torch.randn(5, 3)\npred_mean, pred_var = la(x_test)\nprint(\n    f\"Input shape {tuple(x_test.shape)}, predictive mean of shape \"\n    + f\"{tuple(pred_mean.shape)}, predictive covariance of shape \"\n    + f\"{tuple(pred_var.shape)}\"\n)\n</code></pre> <p>Here's the output:</p> <pre><code>Input shape (5, 3), predictive mean of shape (5, 1), predictive covariance of shape (5, 1, 1)\n</code></pre>"},{"location":"api_reference/baselaplace/","title":"Base Laplace","text":""},{"location":"api_reference/baselaplace/#laplace.baselaplace","title":"laplace.baselaplace","text":"<p>Classes:</p> <ul> <li> <code>BaseLaplace</code>           \u2013            <p>Baseclass for all Laplace approximations in this library.</p> </li> </ul>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace","title":"BaseLaplace","text":"<pre><code>BaseLaplace(model: Module, likelihood: Likelihood | str, sigma_noise: float | Tensor = 1.0, prior_precision: float | Tensor = 1.0, prior_mean: float | Tensor = 0.0, temperature: float = 1.0, enable_backprop: bool = False, dict_key_x: str = 'input_ids', dict_key_y: str = 'labels', backend: type[CurvatureInterface] | None = None, backend_kwargs: dict[str, Any] | None = None, asdl_fisher_kwargs: dict[str, Any] | None = None)\n</code></pre> <p>Baseclass for all Laplace approximations in this library.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>optimize_prior_precision</code>             \u2013              <p>Optimize the prior precision post-hoc using the <code>method</code></p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>log_likelihood</code>               (<code>Tensor</code>)           \u2013            <p>Compute log likelihood on the training data after <code>.fit()</code> has been called.</p> </li> <li> <code>prior_precision_diag</code>               (<code>Tensor</code>)           \u2013            <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    sigma_noise: float | torch.Tensor = 1.0,\n    prior_precision: float | torch.Tensor = 1.0,\n    prior_mean: float | torch.Tensor = 0.0,\n    temperature: float = 1.0,\n    enable_backprop: bool = False,\n    dict_key_x: str = \"input_ids\",\n    dict_key_y: str = \"labels\",\n    backend: type[CurvatureInterface] | None = None,\n    backend_kwargs: dict[str, Any] | None = None,\n    asdl_fisher_kwargs: dict[str, Any] | None = None,\n) -&gt; None:\n    if likelihood not in [lik.value for lik in Likelihood]:\n        raise ValueError(f\"Invalid likelihood type {likelihood}\")\n\n    self.model: nn.Module = model\n    self.likelihood: Likelihood | str = likelihood\n\n    # Only do Laplace on params that require grad\n    self.params: list[torch.Tensor] = []\n    self.is_subset_params: bool = False\n    for p in model.parameters():\n        if p.requires_grad:\n            self.params.append(p)\n        else:\n            self.is_subset_params = True\n\n    self.n_params: int = sum(p.numel() for p in self.params)\n    self.n_layers: int = len(self.params)\n    self.prior_precision: float | torch.Tensor = prior_precision\n    self.prior_mean: float | torch.Tensor = prior_mean\n    if sigma_noise != 1 and likelihood != Likelihood.REGRESSION:\n        raise ValueError(\"Sigma noise != 1 only available for regression.\")\n\n    self.sigma_noise: float | torch.Tensor = sigma_noise\n    self.temperature: float = temperature\n    self.enable_backprop: bool = enable_backprop\n\n    # For models with dict-like inputs (e.g. Huggingface LLMs)\n    self.dict_key_x = dict_key_x\n    self.dict_key_y = dict_key_y\n\n    if backend is None:\n        backend = CurvlinopsGGN\n    else:\n        if self.is_subset_params and (\n            \"backpack\" in backend.__name__.lower()\n            or \"asdfghjkl\" in backend.__name__.lower()\n        ):\n            raise ValueError(\n                \"If some grad are switched off, the BackPACK and Asdfghjkl backends\"\n                \" are not supported.\"\n            )\n\n    self._backend: CurvatureInterface | None = None\n    self._backend_cls: type[CurvatureInterface] = backend\n    self._backend_kwargs: dict[str, Any] = (\n        dict() if backend_kwargs is None else backend_kwargs\n    )\n    self._asdl_fisher_kwargs: dict[str, Any] = (\n        dict() if asdl_fisher_kwargs is None else asdl_fisher_kwargs\n    )\n\n    # log likelihood = g(loss)\n    self.loss: float = 0.0\n    self.n_outputs: int = 0\n    self.n_data: int = 0\n\n    # Declare attributes\n    self._prior_mean: torch.Tensor\n    self._prior_precision: torch.Tensor\n    self._sigma_noise: torch.Tensor\n    self._posterior_scale: torch.Tensor | None\n</code></pre>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace(model)","title":"<code>model</code>","text":"(<code>Module</code>)           \u2013"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace(likelihood)","title":"<code>likelihood</code>","text":"(<code>Likelihood or str in {'classification', 'regression', 'reward_modeling'}</code>)           \u2013            <p>determines the log likelihood Hessian approximation. In the case of 'reward_modeling', it fits Laplace using the classification likelihood, then does prediction as in regression likelihood. The model needs to be defined accordingly: The forward pass during training takes <code>x.shape == (batch_size, 2, dim)</code> with <code>y.shape = (batch_size,)</code>. Meanwhile, during evaluation <code>x.shape == (batch_size, dim)</code>. Note that 'reward_modeling' only supports <code>KronLaplace</code> and <code>DiagLaplace</code>.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace(sigma_noise)","title":"<code>sigma_noise</code>","text":"(<code>Tensor or float</code>, default:                   <code>1</code> )           \u2013            <p>observation noise for the regression setting; must be 1 for classification</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace(prior_precision)","title":"<code>prior_precision</code>","text":"(<code>Tensor or float</code>, default:                   <code>1</code> )           \u2013            <p>prior precision of a Gaussian prior (= weight decay); can be scalar, per-layer, or diagonal in the most general case</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace(prior_mean)","title":"<code>prior_mean</code>","text":"(<code>Tensor or float</code>, default:                   <code>0</code> )           \u2013            <p>prior mean of a Gaussian prior, useful for continual learning</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace(temperature)","title":"<code>temperature</code>","text":"(<code>float</code>, default:                   <code>1</code> )           \u2013            <p>temperature of the likelihood; lower temperature leads to more concentrated posterior and vice versa.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to enable backprop to the input <code>x</code> through the Laplace predictive. Useful for e.g. Bayesian optimization.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace(dict_key_x)","title":"<code>dict_key_x</code>","text":"(<code>str</code>, default:                   <code>'input_ids'</code> )           \u2013            <p>The dictionary key under which the input tensor <code>x</code> is stored. Only has effect when the model takes a <code>MutableMapping</code> as the input. Useful for Huggingface LLM models.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace(dict_key_y)","title":"<code>dict_key_y</code>","text":"(<code>str</code>, default:                   <code>'labels'</code> )           \u2013            <p>The dictionary key under which the target tensor <code>y</code> is stored. Only has effect when the model takes a <code>MutableMapping</code> as the input. Useful for Huggingface LLM models.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace(backend)","title":"<code>backend</code>","text":"(<code>subclasses of `laplace.curvature.CurvatureInterface`</code>, default:                   <code>None</code> )           \u2013            <p>backend for access to curvature/Hessian approximations. Defaults to CurvlinopsGGN if None.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace(backend_kwargs)","title":"<code>backend_kwargs</code>","text":"(<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>arguments passed to the backend on initialization, for example to set the number of MC samples for stochastic approximations.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace(asdl_fisher_kwargs)","title":"<code>asdl_fisher_kwargs</code>","text":"(<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>arguments passed to the ASDL backend specifically on initialization.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace.log_likelihood","title":"log_likelihood","text":"<pre><code>log_likelihood: Tensor\n</code></pre> <p>Compute log likelihood on the training data after <code>.fit()</code> has been called. The log likelihood is computed on-demand based on the loss and, for example, the observation noise which makes it differentiable in the latter for iterative updates.</p> <p>Returns:</p> <ul> <li> <code>log_likelihood</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace.prior_precision_diag","title":"prior_precision_diag","text":"<pre><code>prior_precision_diag: Tensor\n</code></pre> <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either a scalar, layer-wise, or diagonal prior precision.</p> <p>Returns:</p> <ul> <li> <code>prior_precision_diag</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace.optimize_prior_precision","title":"optimize_prior_precision","text":"<pre><code>optimize_prior_precision(pred_type: PredType | str, method: TuningMethod | str = MARGLIK, n_steps: int = 100, lr: float = 0.1, init_prior_prec: float | Tensor = 1.0, prior_structure: PriorStructure | str = DIAG, val_loader: DataLoader | None = None, loss: Metric | Callable[[Tensor], Tensor | float] | None = None, log_prior_prec_min: float = -4, log_prior_prec_max: float = 4, grid_size: int = 100, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, verbose: bool = False, progress_bar: bool = False) -&gt; None\n</code></pre> <p>Optimize the prior precision post-hoc using the <code>method</code> specified by the user.</p> <p>Parameters:</p> Source code in <code>laplace/baselaplace.py</code> <pre><code>def optimize_prior_precision(\n    self,\n    pred_type: PredType | str,\n    method: TuningMethod | str = TuningMethod.MARGLIK,\n    n_steps: int = 100,\n    lr: float = 1e-1,\n    init_prior_prec: float | torch.Tensor = 1.0,\n    prior_structure: PriorStructure | str = PriorStructure.DIAG,\n    val_loader: DataLoader | None = None,\n    loss: torchmetrics.Metric\n    | Callable[[torch.Tensor], torch.Tensor | float]\n    | None = None,\n    log_prior_prec_min: float = -4,\n    log_prior_prec_max: float = 4,\n    grid_size: int = 100,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    verbose: bool = False,\n    progress_bar: bool = False,\n) -&gt; None:\n    \"\"\"Optimize the prior precision post-hoc using the `method`\n    specified by the user.\n\n    Parameters\n    ----------\n    pred_type : PredType or str in {'glm', 'nn'}\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictiv. The GLM predictive is consistent with the\n        curvature approximations used here.\n    method : TuningMethod or str in {'marglik', 'gridsearch'}, default=PredType.MARGLIK\n        specifies how the prior precision should be optimized.\n    n_steps : int, default=100\n        the number of gradient descent steps to take.\n    lr : float, default=1e-1\n        the learning rate to use for gradient descent.\n    init_prior_prec : float or tensor, default=1.0\n        initial prior precision before the first optimization step.\n    prior_structure : PriorStructure or str in {'scalar', 'layerwise', 'diag'}, default=PriorStructure.SCALAR\n        if init_prior_prec is scalar, the prior precision is optimized with this structure.\n        otherwise, the structure of init_prior_prec is maintained.\n    val_loader : torch.data.utils.DataLoader, default=None\n        DataLoader for the validation set; each iterate is a training batch (X, y).\n    loss : callable or torchmetrics.Metric, default=None\n        loss function to use for CV. If callable, the loss is computed offline (memory intensive).\n        If torchmetrics.Metric, running loss is computed (efficient). The default\n        depends on the likelihood: `RunningNLLMetric()` for classification and\n        reward modeling, running `MeanSquaredError()` for regression.\n    log_prior_prec_min : float, default=-4\n        lower bound of gridsearch interval.\n    log_prior_prec_max : float, default=4\n        upper bound of gridsearch interval.\n    grid_size : int, default=100\n        number of values to consider inside the gridsearch interval.\n    link_approx : LinkApprox or str in {'mc', 'probit', 'bridge'}, default=LinkApprox.PROBIT\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only `'mc'` is possible.\n    n_samples : int, default=100\n        number of samples for `link_approx='mc'`.\n    verbose : bool, default=False\n        if true, the optimized prior precision will be printed\n        (can be a large tensor if the prior has a diagonal covariance).\n    progress_bar : bool, default=False\n        whether to show a progress bar; updated at every batch-Hessian computation.\n        Useful for very large model and large amount of data, esp. when `subset_of_weights='all'`.\n    \"\"\"\n    likelihood = (\n        Likelihood.CLASSIFICATION\n        if self.likelihood == Likelihood.REWARD_MODELING\n        else self.likelihood\n    )\n\n    if likelihood == Likelihood.CLASSIFICATION:\n        warnings.warn(\n            \"By default `link_approx` is `probit`. Make sure to set it equals to \"\n            \"the way you want to call `la(test_data, pred_type=..., link_approx=...)`.\"\n        )\n\n    if method == TuningMethod.MARGLIK:\n        if val_loader is not None:\n            warnings.warn(\n                \"`val_loader` will be ignored when `method` == 'marglik'. \"\n                \"Do you mean to set `method = 'gridsearch'`?\"\n            )\n\n        self.prior_precision = (\n            init_prior_prec\n            if isinstance(init_prior_prec, torch.Tensor)\n            else torch.as_tensor(init_prior_prec)\n        )\n\n        if (\n            len(self.prior_precision) == 1\n            and prior_structure != PriorStructure.SCALAR\n        ):\n            self.prior_precision = fix_prior_prec_structure(\n                self.prior_precision.item(),\n                prior_structure,\n                self.n_layers,\n                self.n_params,\n                self._device,\n                self._dtype,\n            )\n\n        log_prior_prec = self.prior_precision.log()\n        log_prior_prec.requires_grad = True\n        optimizer = torch.optim.Adam([log_prior_prec], lr=lr)\n\n        if progress_bar:\n            pbar = tqdm.trange(n_steps)\n            pbar.set_description(\"[Optimizing marginal likelihood]\")\n        else:\n            pbar = range(n_steps)\n\n        for _ in pbar:\n            optimizer.zero_grad()\n            prior_prec = log_prior_prec.exp()\n            neg_log_marglik = -self.log_marginal_likelihood(\n                prior_precision=prior_prec\n            )\n            neg_log_marglik.backward()\n            optimizer.step()\n\n        self.prior_precision = log_prior_prec.detach().exp()\n    elif method == TuningMethod.GRIDSEARCH:\n        if val_loader is None:\n            raise ValueError(\"gridsearch requires a validation set DataLoader\")\n\n        interval = torch.logspace(log_prior_prec_min, log_prior_prec_max, grid_size)\n\n        if loss is None:\n            loss = (\n                torchmetrics.MeanSquaredError(num_outputs=self.n_outputs).to(\n                    self._device\n                )\n                if likelihood == Likelihood.REGRESSION\n                else RunningNLLMetric().to(self._device)\n            )\n\n        self.prior_precision = self._gridsearch(\n            loss,\n            interval,\n            val_loader,\n            pred_type=pred_type,\n            link_approx=link_approx,\n            n_samples=n_samples,\n            progress_bar=progress_bar,\n        )\n    else:\n        raise ValueError(\"For now only marglik and gridsearch is implemented.\")\n\n    if verbose:\n        print(f\"Optimized prior precision is {self.prior_precision}.\")\n</code></pre>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace.optimize_prior_precision(pred_type)","title":"<code>pred_type</code>","text":"(<code>PredType or str in {'glm', 'nn'}</code>)           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictiv. The GLM predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace.optimize_prior_precision(method)","title":"<code>method</code>","text":"(<code>TuningMethod or str in {'marglik', 'gridsearch'}</code>, default:                   <code>PredType.MARGLIK</code> )           \u2013            <p>specifies how the prior precision should be optimized.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace.optimize_prior_precision(n_steps)","title":"<code>n_steps</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>the number of gradient descent steps to take.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace.optimize_prior_precision(lr)","title":"<code>lr</code>","text":"(<code>float</code>, default:                   <code>1e-1</code> )           \u2013            <p>the learning rate to use for gradient descent.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace.optimize_prior_precision(init_prior_prec)","title":"<code>init_prior_prec</code>","text":"(<code>float or tensor</code>, default:                   <code>1.0</code> )           \u2013            <p>initial prior precision before the first optimization step.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace.optimize_prior_precision(prior_structure)","title":"<code>prior_structure</code>","text":"(<code>PriorStructure or str in {'scalar', 'layerwise', 'diag'}</code>, default:                   <code>PriorStructure.SCALAR</code> )           \u2013            <p>if init_prior_prec is scalar, the prior precision is optimized with this structure. otherwise, the structure of init_prior_prec is maintained.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace.optimize_prior_precision(val_loader)","title":"<code>val_loader</code>","text":"(<code>DataLoader</code>, default:                   <code>None</code> )           \u2013            <p>DataLoader for the validation set; each iterate is a training batch (X, y).</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace.optimize_prior_precision(loss)","title":"<code>loss</code>","text":"(<code>callable or Metric</code>, default:                   <code>None</code> )           \u2013            <p>loss function to use for CV. If callable, the loss is computed offline (memory intensive). If torchmetrics.Metric, running loss is computed (efficient). The default depends on the likelihood: <code>RunningNLLMetric()</code> for classification and reward modeling, running <code>MeanSquaredError()</code> for regression.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace.optimize_prior_precision(log_prior_prec_min)","title":"<code>log_prior_prec_min</code>","text":"(<code>float</code>, default:                   <code>-4</code> )           \u2013            <p>lower bound of gridsearch interval.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace.optimize_prior_precision(log_prior_prec_max)","title":"<code>log_prior_prec_max</code>","text":"(<code>float</code>, default:                   <code>4</code> )           \u2013            <p>upper bound of gridsearch interval.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace.optimize_prior_precision(grid_size)","title":"<code>grid_size</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of values to consider inside the gridsearch interval.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace.optimize_prior_precision(link_approx)","title":"<code>link_approx</code>","text":"(<code>LinkApprox or str in {'mc', 'probit', 'bridge'}</code>, default:                   <code>LinkApprox.PROBIT</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only <code>'mc'</code> is possible.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace.optimize_prior_precision(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace.optimize_prior_precision(verbose)","title":"<code>verbose</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>if true, the optimized prior precision will be printed (can be a large tensor if the prior has a diagonal covariance).</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace.optimize_prior_precision(progress_bar)","title":"<code>progress_bar</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to show a progress bar; updated at every batch-Hessian computation. Useful for very large model and large amount of data, esp. when <code>subset_of_weights='all'</code>.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace._glm_forward_call","title":"_glm_forward_call","text":"<pre><code>_glm_forward_call(x: Tensor | MutableMapping, likelihood: Likelihood | str, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code> for \"glm\" pred type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_forward_call(\n    self,\n    x: torch.Tensor | MutableMapping,\n    likelihood: Likelihood | str,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x` for \"glm\" pred type.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    likelihood : Likelihood or str in {'classification', 'regression', 'reward_modeling'}\n        determines the log likelihood Hessian approximation.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` and `link_approx='mc'`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    f_mu, f_var = self._glm_predictive_distribution(\n        x, joint=joint and likelihood == Likelihood.REGRESSION\n    )\n\n    if likelihood == Likelihood.REGRESSION:\n        if diagonal_output and not joint:\n            f_var = torch.diagonal(f_var, dim1=-2, dim2=-1)\n        return f_mu, f_var\n\n    if link_approx == LinkApprox.MC:\n        return self._glm_predictive_samples(\n            f_mu,\n            f_var,\n            n_samples=n_samples,\n            diagonal_output=diagonal_output,\n        ).mean(dim=0)\n    elif link_approx == LinkApprox.PROBIT:\n        kappa = 1 / torch.sqrt(1.0 + np.pi / 8 * f_var.diagonal(dim1=1, dim2=2))\n        return torch.softmax(kappa * f_mu, dim=-1)\n    elif \"bridge\" in link_approx:\n        # zero mean correction\n        f_mu -= (\n            f_var.sum(-1)\n            * f_mu.sum(-1).reshape(-1, 1)\n            / f_var.sum(dim=(1, 2)).reshape(-1, 1)\n        )\n        f_var -= torch.einsum(\n            \"bi,bj-&gt;bij\", f_var.sum(-1), f_var.sum(-2)\n        ) / f_var.sum(dim=(1, 2)).reshape(-1, 1, 1)\n\n        # Laplace Bridge\n        _, K = f_mu.size(0), f_mu.size(-1)\n        f_var_diag = torch.diagonal(f_var, dim1=1, dim2=2)\n\n        # optional: variance correction\n        if link_approx == LinkApprox.BRIDGE_NORM:\n            f_var_diag_mean = f_var_diag.mean(dim=1)\n            f_var_diag_mean /= torch.as_tensor(\n                [K / 2], device=self._device, dtype=self._dtype\n            ).sqrt()\n            f_mu /= f_var_diag_mean.sqrt().unsqueeze(-1)\n            f_var_diag /= f_var_diag_mean.unsqueeze(-1)\n\n        sum_exp = torch.exp(-f_mu).sum(dim=1).unsqueeze(-1)\n        alpha = (1 - 2 / K + f_mu.exp() / K**2 * sum_exp) / f_var_diag\n        return torch.nan_to_num(alpha / alpha.sum(dim=1).unsqueeze(-1), nan=1.0)\n    else:\n        raise ValueError(\n            \"Prediction path invalid. Check the likelihood, pred_type, link_approx combination!\"\n        )\n</code></pre>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace._glm_forward_call(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace._glm_forward_call(likelihood)","title":"<code>likelihood</code>","text":"(<code>Likelihood or str in {'classification', 'regression', 'reward_modeling'}</code>)           \u2013            <p>determines the log likelihood Hessian approximation.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace._glm_forward_call(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace._glm_forward_call(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace._glm_forward_call(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace._glm_forward_call(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> and <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace._glm_functional_samples","title":"_glm_functional_samples","text":"<pre><code>_glm_functional_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior functional on input data <code>x</code> using \"glm\" prediction type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_functional_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior functional on input data `x` using \"glm\" prediction\n    type.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    assert f_var.shape == torch.Size([f_mu.shape[0], f_mu.shape[1], f_mu.shape[1]])\n\n    if diagonal_output:\n        f_var = torch.diagonal(f_var, dim1=1, dim2=2)\n\n    return normal_samples(f_mu, f_var, n_samples, generator)\n</code></pre>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace._glm_functional_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace._glm_functional_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace._glm_functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace._glm_functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace._glm_functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace._glm_predictive_samples","title":"_glm_predictive_samples","text":"<pre><code>_glm_predictive_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code> using \"glm\" prediction type. I.e., the inverse-link function correponding to the likelihood is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_predictive_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x` using \"glm\" prediction\n    type. I.e., the inverse-link function correponding to the likelihood is applied\n    on top of the functional sample.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    f_samples = self._glm_functional_samples(\n        f_mu, f_var, n_samples, diagonal_output, generator\n    )\n\n    if self.likelihood == Likelihood.REGRESSION:\n        return f_samples\n    else:\n        return torch.softmax(f_samples, dim=-1)\n</code></pre>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace._glm_predictive_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace._glm_predictive_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace._glm_predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace._glm_predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/baselaplace/#laplace.baselaplace.BaseLaplace._glm_predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/curvatures/","title":"Curvatures","text":""},{"location":"api_reference/curvatures/#laplace.curvature","title":"laplace.curvature","text":"<p>Classes:</p> <ul> <li> <code>CurvatureInterface</code>           \u2013            <p>Interface to access curvature for a model and corresponding likelihood.</p> </li> <li> <code>GGNInterface</code>           \u2013            <p>Generalized Gauss-Newton or Fisher Curvature Interface.</p> </li> <li> <code>EFInterface</code>           \u2013            <p>Interface for Empirical Fisher as Hessian approximation.</p> </li> <li> <code>AsdlInterface</code>           \u2013            <p>Interface for asdfghjkl backend.</p> </li> <li> <code>AsdlGGN</code>           \u2013            <p>Implementation of the <code>GGNInterface</code> using asdfghjkl.</p> </li> <li> <code>AsdlEF</code>           \u2013            <p>Implementation of the <code>EFInterface</code> using asdfghjkl.</p> </li> <li> <code>AsdlHessian</code>           \u2013            </li> <li> <code>BackPackInterface</code>           \u2013            <p>Interface for Backpack backend.</p> </li> <li> <code>BackPackGGN</code>           \u2013            <p>Implementation of the <code>GGNInterface</code> using Backpack.</p> </li> <li> <code>BackPackEF</code>           \u2013            <p>Implementation of <code>EFInterface</code> using Backpack.</p> </li> <li> <code>CurvlinopsInterface</code>           \u2013            <p>Interface for Curvlinops backend. https://github.com/f-dangel/curvlinops</p> </li> <li> <code>CurvlinopsGGN</code>           \u2013            <p>Implementation of the <code>GGNInterface</code> using Curvlinops.</p> </li> <li> <code>CurvlinopsEF</code>           \u2013            <p>Implementation of <code>EFInterface</code> using Curvlinops.</p> </li> <li> <code>CurvlinopsHessian</code>           \u2013            <p>Implementation of the full Hessian using Curvlinops.</p> </li> </ul>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface","title":"CurvatureInterface","text":"<pre><code>CurvatureInterface(model: Module, likelihood: Likelihood | str, last_layer: bool = False, subnetwork_indices: LongTensor | None = None, dict_key_x: str = 'input_ids', dict_key_y: str = 'labels')\n</code></pre> <p>Interface to access curvature for a model and corresponding likelihood. A <code>CurvatureInterface</code> must inherit from this baseclass and implement the necessary functions <code>jacobians</code>, <code>full</code>, <code>kron</code>, and <code>diag</code>. The interface might be extended in the future to account for other curvature structures, for example, a block-diagonal one.</p> <p>Parameters:</p> <p>Attributes:</p> <ul> <li> <code>lossfunc</code>               (<code>MSELoss or CrossEntropyLoss</code>)           \u2013            </li> <li> <code>factor</code>               (<code>float</code>)           \u2013            <p>conversion factor between torch losses and base likelihoods For example, \\(\\frac{1}{2}\\) to get to \\(\\mathcal{N}(f, 1)\\) from MSELoss.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_{\\theta} f(x;\\theta)\\) at current parameter \\(\\theta\\),</p> </li> <li> <code>last_layer_jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\)</p> </li> <li> <code>gradients</code>             \u2013              <p>Compute batch gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at</p> </li> <li> <code>full</code>             \u2013              <p>Compute a dense curvature (approximation) in the form of a \\(P \\times P\\) matrix</p> </li> <li> <code>kron</code>             \u2013              <p>Compute a Kronecker factored curvature approximation (such as KFAC).</p> </li> <li> <code>diag</code>             \u2013              <p>Compute a diagonal Hessian approximation to \\(H\\) and is represented as a</p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    last_layer: bool = False,\n    subnetwork_indices: torch.LongTensor | None = None,\n    dict_key_x: str = \"input_ids\",\n    dict_key_y: str = \"labels\",\n):\n    assert likelihood in [Likelihood.REGRESSION, Likelihood.CLASSIFICATION]\n    self.likelihood: Likelihood | str = likelihood\n    self.model: nn.Module = model\n    self.last_layer: bool = last_layer\n    self.subnetwork_indices: torch.LongTensor | None = subnetwork_indices\n    self.dict_key_x = dict_key_x\n    self.dict_key_y = dict_key_y\n\n    if likelihood == \"regression\":\n        self.lossfunc: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = (\n            MSELoss(reduction=\"sum\")\n        )\n        self.factor: float = 0.5\n    else:\n        self.lossfunc: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = (\n            CrossEntropyLoss(reduction=\"sum\")\n        )\n        self.factor: float = 1.0\n\n    self.params: list[nn.Parameter] = [\n        p for p in self._model.parameters() if p.requires_grad\n    ]\n    self.params_dict: dict[str, nn.Parameter] = {\n        k: v for k, v in self._model.named_parameters() if v.requires_grad\n    }\n    self.buffers_dict: dict[str, torch.Tensor] = {\n        k: v for k, v in self.model.named_buffers()\n    }\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface(model)","title":"<code>model</code>","text":"(<code>torch.nn.Module or `laplace.utils.feature_extractor.FeatureExtractor`</code>)           \u2013            <p>torch model (neural network)</p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface(likelihood)","title":"<code>likelihood</code>","text":"(<code>('classification', 'regression')</code>, default:                   <code>'classification'</code> )           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface(last_layer)","title":"<code>last_layer</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>only consider curvature of last layer</p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface(subnetwork_indices)","title":"<code>subnetwork_indices</code>","text":"(<code>LongTensor</code>, default:                   <code>None</code> )           \u2013            <p>indices of the vectorized model parameters that define the subnetwork to apply the Laplace approximation over</p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface(dict_key_x)","title":"<code>dict_key_x</code>","text":"(<code>str</code>, default:                   <code>'input_ids'</code> )           \u2013            <p>The dictionary key under which the input tensor <code>x</code> is stored. Only has effect when the model takes a <code>MutableMapping</code> as the input. Useful for Huggingface LLM models.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface(dict_key_y)","title":"<code>dict_key_y</code>","text":"(<code>str</code>, default:                   <code>'labels'</code> )           \u2013            <p>The dictionary key under which the target tensor <code>y</code> is stored. Only has effect when the model takes a <code>MutableMapping</code> as the input. Useful for Huggingface LLM models.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface.jacobians","title":"jacobians","text":"<pre><code>jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_{\\theta} f(x;\\theta)\\) at current parameter \\(\\theta\\), via torch.func.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, parameters, outputs)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_{\\\\theta} f(x;\\\\theta)\\\\) at current parameter \\\\(\\\\theta\\\\),\n    via torch.func.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)` on compatible device with model.\n    enable_backprop : bool, default = False\n        whether to enable backprop through the Js and f w.r.t. x\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, parameters, outputs)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n\n    def model_fn_params_only(params_dict, buffers_dict):\n        out = torch.func.functional_call(self.model, (params_dict, buffers_dict), x)\n        return out, out\n\n    Js, f = torch.func.jacrev(model_fn_params_only, has_aux=True)(\n        self.params_dict, self.buffers_dict\n    )\n\n    # Concatenate over flattened parameters\n    Js = [\n        j.flatten(start_dim=-p.dim())\n        for j, p in zip(Js.values(), self.params_dict.values())\n    ]\n    Js = torch.cat(Js, dim=-1)\n\n    if self.subnetwork_indices is not None:\n        Js = Js[:, :, self.subnetwork_indices]\n\n    return (Js, f) if enable_backprop else (Js.detach(), f.detach())\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface.jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface.jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>= False</code> )           \u2013            <p>whether to enable backprop through the Js and f w.r.t. x</p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface.last_layer_jacobians","title":"last_layer_jacobians","text":"<pre><code>last_layer_jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\) only at current last-layer parameter \\(\\theta_{\\textrm{last}}\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, outputs, last-layer-parameters)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def last_layer_jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_{\\\\theta_\\\\textrm{last}} f(x;\\\\theta_\\\\textrm{last})\\\\)\n    only at current last-layer parameter \\\\(\\\\theta_{\\\\textrm{last}}\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n    enable_backprop : bool, default=False\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, outputs, last-layer-parameters)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n    f, phi = self.model.forward_with_features(x)\n    bsize = phi.shape[0]\n    output_size = int(f.numel() / bsize)\n\n    # calculate Jacobians using the feature vector 'phi'\n    p = next(self.model.parameters())\n    identity = (\n        torch.eye(output_size, device=p.device, dtype=p.dtype)\n        .unsqueeze(0)\n        .tile(bsize, 1, 1)\n    )\n    # Jacobians are batch x output x params\n    Js = torch.einsum(\"kp,kij-&gt;kijp\", phi, identity).reshape(bsize, output_size, -1)\n    if self.model.last_layer.bias is not None:\n        Js = torch.cat([Js, identity], dim=2)\n\n    return (Js, f) if enable_backprop else (Js.detach(), f.detach())\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface.last_layer_jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface.last_layer_jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface.gradients","title":"gradients","text":"<pre><code>gradients(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute batch gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter \\(\\theta\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Gs</code> (              <code>Tensor</code> )          \u2013            <p>gradients <code>(batch, parameters)</code></p> </li> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def gradients(\n    self, x: torch.Tensor | MutableMapping[str, torch.Tensor | Any], y: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute batch gradients \\\\(\\\\nabla_\\\\theta \\\\ell(f(x;\\\\theta, y)\\\\) at\n    current parameter \\\\(\\\\theta\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)` on compatible device with model.\n    y : torch.Tensor\n\n    Returns\n    -------\n    Gs : torch.Tensor\n        gradients `(batch, parameters)`\n    loss : torch.Tensor\n    \"\"\"\n\n    def loss_single(x, y, params_dict, buffers_dict):\n        \"\"\"Compute the gradient for a single sample.\"\"\"\n        x, y = x.unsqueeze(0), y.unsqueeze(0)  # vmap removes the batch dimension\n        output = torch.func.functional_call(\n            self.model, (params_dict, buffers_dict), x\n        )\n        loss = torch.func.functional_call(self.lossfunc, {}, (output, y))\n        return loss, loss\n\n    grad_fn = torch.func.grad(loss_single, argnums=2, has_aux=True)\n    batch_grad_fn = torch.func.vmap(grad_fn, in_dims=(0, 0, None, None))\n\n    batch_grad, batch_loss = batch_grad_fn(\n        x, y, self.params_dict, self.buffers_dict\n    )\n    Gs = torch.cat([bg.flatten(start_dim=1) for bg in batch_grad.values()], dim=1)\n\n    if self.subnetwork_indices is not None:\n        Gs = Gs[:, self.subnetwork_indices]\n\n    loss = batch_loss.sum(0)\n\n    return Gs, loss\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface.gradients(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface.gradients(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface.full","title":"full","text":"<pre><code>full(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor, **kwargs: dict[str, Any])\n</code></pre> <p>Compute a dense curvature (approximation) in the form of a \\(P \\times P\\) matrix \\(H\\) with respect to parameters \\(\\theta \\in \\mathbb{R}^P\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> <li> <code>H</code> (              <code>Tensor</code> )          \u2013            <p>Hessian approximation <code>(parameters, parameters)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def full(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    y: torch.Tensor,\n    **kwargs: dict[str, Any],\n):\n    \"\"\"Compute a dense curvature (approximation) in the form of a \\\\(P \\\\times P\\\\) matrix\n    \\\\(H\\\\) with respect to parameters \\\\(\\\\theta \\\\in \\\\mathbb{R}^P\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)`\n    y : torch.Tensor\n        labels `(batch, label_shape)`\n\n    Returns\n    -------\n    loss : torch.Tensor\n    H : torch.Tensor\n        Hessian approximation `(parameters, parameters)`\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface.full(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface.full(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013            <p>labels <code>(batch, label_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface.kron","title":"kron","text":"<pre><code>kron(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor, N: int, **kwargs: dict[str, Any]) -&gt; tuple[Tensor, Kron]\n</code></pre> <p>Compute a Kronecker factored curvature approximation (such as KFAC). The approximation to \\(H\\) takes the form of two Kronecker factors \\(Q, H\\), i.e., \\(H \\approx Q \\otimes H\\) for each Module in the neural network permitting such curvature. \\(Q\\) is quadratic in the input-dimension of a module \\(p_{in} \\times p_{in}\\) and \\(H\\) in the output-dimension \\(p_{out} \\times p_{out}\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> <li> <code>H</code> (              <code>`laplace.utils.matrix.Kron`</code> )          \u2013            <p>Kronecker factored Hessian approximation.</p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def kron(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    y: torch.Tensor,\n    N: int,\n    **kwargs: dict[str, Any],\n) -&gt; tuple[torch.Tensor, Kron]:\n    \"\"\"Compute a Kronecker factored curvature approximation (such as KFAC).\n    The approximation to \\\\(H\\\\) takes the form of two Kronecker factors \\\\(Q, H\\\\),\n    i.e., \\\\(H \\\\approx Q \\\\otimes H\\\\) for each Module in the neural network permitting\n    such curvature.\n    \\\\(Q\\\\) is quadratic in the input-dimension of a module \\\\(p_{in} \\\\times p_{in}\\\\)\n    and \\\\(H\\\\) in the output-dimension \\\\(p_{out} \\\\times p_{out}\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)`\n    y : torch.Tensor\n        labels `(batch, label_shape)`\n    N : int\n        total number of data points\n\n    Returns\n    -------\n    loss : torch.Tensor\n    H : `laplace.utils.matrix.Kron`\n        Kronecker factored Hessian approximation.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface.kron(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface.kron(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013            <p>labels <code>(batch, label_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface.kron(N)","title":"<code>N</code>","text":"(<code>int</code>)           \u2013            <p>total number of data points</p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface.diag","title":"diag","text":"<pre><code>diag(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor, **kwargs: dict[str, Any])\n</code></pre> <p>Compute a diagonal Hessian approximation to \\(H\\) and is represented as a vector of the dimensionality of parameters \\(\\theta\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> <li> <code>H</code> (              <code>Tensor</code> )          \u2013            <p>vector representing the diagonal of H</p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def diag(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    y: torch.Tensor,\n    **kwargs: dict[str, Any],\n):\n    \"\"\"Compute a diagonal Hessian approximation to \\\\(H\\\\) and is represented as a\n    vector of the dimensionality of parameters \\\\(\\\\theta\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)`\n    y : torch.Tensor\n        labels `(batch, label_shape)`\n\n    Returns\n    -------\n    loss : torch.Tensor\n    H : torch.Tensor\n        vector representing the diagonal of H\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface.diag(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvatureInterface.diag(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013            <p>labels <code>(batch, label_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface","title":"GGNInterface","text":"<pre><code>GGNInterface(model: Module, likelihood: Likelihood | str, last_layer: bool = False, subnetwork_indices: LongTensor | None = None, dict_key_x: str = 'input_ids', dict_key_y: str = 'labels', stochastic: bool = False, num_samples: int = 1)\n</code></pre> <p>               Bases: <code>CurvatureInterface</code></p> <p>Generalized Gauss-Newton or Fisher Curvature Interface. The GGN is equal to the Fisher information for the available likelihoods. In addition to <code>CurvatureInterface</code>, methods for Jacobians are required by subclasses.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_{\\theta} f(x;\\theta)\\) at current parameter \\(\\theta\\),</p> </li> <li> <code>last_layer_jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\)</p> </li> <li> <code>gradients</code>             \u2013              <p>Compute batch gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at</p> </li> <li> <code>kron</code>             \u2013              <p>Compute a Kronecker factored curvature approximation (such as KFAC).</p> </li> <li> <code>full</code>             \u2013              <p>Compute the full GGN \\(P \\times P\\) matrix as Hessian approximation</p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    last_layer: bool = False,\n    subnetwork_indices: torch.LongTensor | None = None,\n    dict_key_x: str = \"input_ids\",\n    dict_key_y: str = \"labels\",\n    stochastic: bool = False,\n    num_samples: int = 1,\n) -&gt; None:\n    self.stochastic: bool = stochastic\n    self.num_samples: int = num_samples\n\n    super().__init__(\n        model, likelihood, last_layer, subnetwork_indices, dict_key_x, dict_key_y\n    )\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface(model)","title":"<code>model</code>","text":"(<code>torch.nn.Module or `laplace.utils.feature_extractor.FeatureExtractor`</code>)           \u2013            <p>torch model (neural network)</p>"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface(likelihood)","title":"<code>likelihood</code>","text":"(<code>('classification', 'regression')</code>, default:                   <code>'classification'</code> )           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface(last_layer)","title":"<code>last_layer</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>only consider curvature of last layer</p>"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface(subnetwork_indices)","title":"<code>subnetwork_indices</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>indices of the vectorized model parameters that define the subnetwork to apply the Laplace approximation over</p>"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface(dict_key_x)","title":"<code>dict_key_x</code>","text":"(<code>str</code>, default:                   <code>'input_ids'</code> )           \u2013            <p>The dictionary key under which the input tensor <code>x</code> is stored. Only has effect when the model takes a <code>MutableMapping</code> as the input. Useful for Huggingface LLM models.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface(dict_key_y)","title":"<code>dict_key_y</code>","text":"(<code>str</code>, default:                   <code>'labels'</code> )           \u2013            <p>The dictionary key under which the target tensor <code>y</code> is stored. Only has effect when the model takes a <code>MutableMapping</code> as the input. Useful for Huggingface LLM models.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface(stochastic)","title":"<code>stochastic</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Fisher if stochastic else GGN</p>"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface(num_samples)","title":"<code>num_samples</code>","text":"(<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of samples used to approximate the stochastic Fisher</p>"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface.jacobians","title":"jacobians","text":"<pre><code>jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_{\\theta} f(x;\\theta)\\) at current parameter \\(\\theta\\), via torch.func.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, parameters, outputs)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_{\\\\theta} f(x;\\\\theta)\\\\) at current parameter \\\\(\\\\theta\\\\),\n    via torch.func.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)` on compatible device with model.\n    enable_backprop : bool, default = False\n        whether to enable backprop through the Js and f w.r.t. x\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, parameters, outputs)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n\n    def model_fn_params_only(params_dict, buffers_dict):\n        out = torch.func.functional_call(self.model, (params_dict, buffers_dict), x)\n        return out, out\n\n    Js, f = torch.func.jacrev(model_fn_params_only, has_aux=True)(\n        self.params_dict, self.buffers_dict\n    )\n\n    # Concatenate over flattened parameters\n    Js = [\n        j.flatten(start_dim=-p.dim())\n        for j, p in zip(Js.values(), self.params_dict.values())\n    ]\n    Js = torch.cat(Js, dim=-1)\n\n    if self.subnetwork_indices is not None:\n        Js = Js[:, :, self.subnetwork_indices]\n\n    return (Js, f) if enable_backprop else (Js.detach(), f.detach())\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface.jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface.jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>= False</code> )           \u2013            <p>whether to enable backprop through the Js and f w.r.t. x</p>"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface.last_layer_jacobians","title":"last_layer_jacobians","text":"<pre><code>last_layer_jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\) only at current last-layer parameter \\(\\theta_{\\textrm{last}}\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, outputs, last-layer-parameters)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def last_layer_jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_{\\\\theta_\\\\textrm{last}} f(x;\\\\theta_\\\\textrm{last})\\\\)\n    only at current last-layer parameter \\\\(\\\\theta_{\\\\textrm{last}}\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n    enable_backprop : bool, default=False\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, outputs, last-layer-parameters)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n    f, phi = self.model.forward_with_features(x)\n    bsize = phi.shape[0]\n    output_size = int(f.numel() / bsize)\n\n    # calculate Jacobians using the feature vector 'phi'\n    p = next(self.model.parameters())\n    identity = (\n        torch.eye(output_size, device=p.device, dtype=p.dtype)\n        .unsqueeze(0)\n        .tile(bsize, 1, 1)\n    )\n    # Jacobians are batch x output x params\n    Js = torch.einsum(\"kp,kij-&gt;kijp\", phi, identity).reshape(bsize, output_size, -1)\n    if self.model.last_layer.bias is not None:\n        Js = torch.cat([Js, identity], dim=2)\n\n    return (Js, f) if enable_backprop else (Js.detach(), f.detach())\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface.last_layer_jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface.last_layer_jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface.gradients","title":"gradients","text":"<pre><code>gradients(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute batch gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter \\(\\theta\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Gs</code> (              <code>Tensor</code> )          \u2013            <p>gradients <code>(batch, parameters)</code></p> </li> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def gradients(\n    self, x: torch.Tensor | MutableMapping[str, torch.Tensor | Any], y: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute batch gradients \\\\(\\\\nabla_\\\\theta \\\\ell(f(x;\\\\theta, y)\\\\) at\n    current parameter \\\\(\\\\theta\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)` on compatible device with model.\n    y : torch.Tensor\n\n    Returns\n    -------\n    Gs : torch.Tensor\n        gradients `(batch, parameters)`\n    loss : torch.Tensor\n    \"\"\"\n\n    def loss_single(x, y, params_dict, buffers_dict):\n        \"\"\"Compute the gradient for a single sample.\"\"\"\n        x, y = x.unsqueeze(0), y.unsqueeze(0)  # vmap removes the batch dimension\n        output = torch.func.functional_call(\n            self.model, (params_dict, buffers_dict), x\n        )\n        loss = torch.func.functional_call(self.lossfunc, {}, (output, y))\n        return loss, loss\n\n    grad_fn = torch.func.grad(loss_single, argnums=2, has_aux=True)\n    batch_grad_fn = torch.func.vmap(grad_fn, in_dims=(0, 0, None, None))\n\n    batch_grad, batch_loss = batch_grad_fn(\n        x, y, self.params_dict, self.buffers_dict\n    )\n    Gs = torch.cat([bg.flatten(start_dim=1) for bg in batch_grad.values()], dim=1)\n\n    if self.subnetwork_indices is not None:\n        Gs = Gs[:, self.subnetwork_indices]\n\n    loss = batch_loss.sum(0)\n\n    return Gs, loss\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface.gradients(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface.gradients(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface.kron","title":"kron","text":"<pre><code>kron(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor, N: int, **kwargs: dict[str, Any]) -&gt; tuple[Tensor, Kron]\n</code></pre> <p>Compute a Kronecker factored curvature approximation (such as KFAC). The approximation to \\(H\\) takes the form of two Kronecker factors \\(Q, H\\), i.e., \\(H \\approx Q \\otimes H\\) for each Module in the neural network permitting such curvature. \\(Q\\) is quadratic in the input-dimension of a module \\(p_{in} \\times p_{in}\\) and \\(H\\) in the output-dimension \\(p_{out} \\times p_{out}\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> <li> <code>H</code> (              <code>`laplace.utils.matrix.Kron`</code> )          \u2013            <p>Kronecker factored Hessian approximation.</p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def kron(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    y: torch.Tensor,\n    N: int,\n    **kwargs: dict[str, Any],\n) -&gt; tuple[torch.Tensor, Kron]:\n    \"\"\"Compute a Kronecker factored curvature approximation (such as KFAC).\n    The approximation to \\\\(H\\\\) takes the form of two Kronecker factors \\\\(Q, H\\\\),\n    i.e., \\\\(H \\\\approx Q \\\\otimes H\\\\) for each Module in the neural network permitting\n    such curvature.\n    \\\\(Q\\\\) is quadratic in the input-dimension of a module \\\\(p_{in} \\\\times p_{in}\\\\)\n    and \\\\(H\\\\) in the output-dimension \\\\(p_{out} \\\\times p_{out}\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)`\n    y : torch.Tensor\n        labels `(batch, label_shape)`\n    N : int\n        total number of data points\n\n    Returns\n    -------\n    loss : torch.Tensor\n    H : `laplace.utils.matrix.Kron`\n        Kronecker factored Hessian approximation.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface.kron(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface.kron(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013            <p>labels <code>(batch, label_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface.kron(N)","title":"<code>N</code>","text":"(<code>int</code>)           \u2013            <p>total number of data points</p>"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface._get_mc_functional_fisher","title":"_get_mc_functional_fisher","text":"<pre><code>_get_mc_functional_fisher(f: Tensor) -&gt; Tensor\n</code></pre> <p>Approximate the Fisher's middle matrix (expected outer product of the functional gradient) using MC integral with <code>self.num_samples</code> many samples.</p> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def _get_mc_functional_fisher(self, f: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Approximate the Fisher's middle matrix (expected outer product of the functional gradient)\n    using MC integral with `self.num_samples` many samples.\n    \"\"\"\n    F = 0\n\n    for _ in range(self.num_samples):\n        if self.likelihood == \"regression\":\n            # N(y | f, 1)\n            y_sample = f + torch.randn(f.shape, device=f.device, dtype=f.dtype)\n            grad_sample = f - y_sample  # functional MSE grad\n        else:  # classification with softmax\n            y_sample = torch.distributions.Multinomial(logits=f).sample()\n            # First functional derivative of the loglik is p - y\n            p = torch.softmax(f, dim=-1)\n            grad_sample = p - y_sample\n\n        F += (\n            1\n            / self.num_samples\n            * torch.einsum(\"bc,bk-&gt;bck\", grad_sample, grad_sample)\n        )\n\n    return F\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface.full","title":"full","text":"<pre><code>full(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor, **kwargs: dict[str, Any]) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute the full GGN \\(P \\times P\\) matrix as Hessian approximation \\(H_{ggn}\\) with respect to parameters \\(\\theta \\in \\mathbb{R}^P\\). For last-layer, reduced to \\(\\theta_{last}\\)</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> <li> <code>H</code> (              <code>Tensor</code> )          \u2013            <p>GGN <code>(parameters, parameters)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def full(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    y: torch.Tensor,\n    **kwargs: dict[str, Any],\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the full GGN \\\\(P \\\\times P\\\\) matrix as Hessian approximation\n    \\\\(H_{ggn}\\\\) with respect to parameters \\\\(\\\\theta \\\\in \\\\mathbb{R}^P\\\\).\n    For last-layer, reduced to \\\\(\\\\theta_{last}\\\\)\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)`\n    y : torch.Tensor\n        labels `(batch, label_shape)`\n\n    Returns\n    -------\n    loss : torch.Tensor\n    H : torch.Tensor\n        GGN `(parameters, parameters)`\n    \"\"\"\n    Js, f = self.last_layer_jacobians(x) if self.last_layer else self.jacobians(x)\n    H_lik = (\n        self._get_mc_functional_fisher(f)\n        if self.stochastic\n        else self._get_functional_hessian(f)\n    )\n\n    if H_lik is not None:\n        H = torch.einsum(\"bcp,bck,bkq-&gt;pq\", Js, H_lik, Js)\n    else:  # The case of exact GGN for regression\n        H = torch.einsum(\"bcp,bcq-&gt;pq\", Js, Js)\n    loss = self.factor * self.lossfunc(f, y)\n\n    return loss.detach(), H.detach()\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface.full(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.GGNInterface.full(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013            <p>labels <code>(batch, label_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.EFInterface","title":"EFInterface","text":"<pre><code>EFInterface(model: Module, likelihood: Likelihood | str, last_layer: bool = False, subnetwork_indices: LongTensor | None = None, dict_key_x: str = 'input_ids', dict_key_y: str = 'labels')\n</code></pre> <p>               Bases: <code>CurvatureInterface</code></p> <p>Interface for Empirical Fisher as Hessian approximation. In addition to <code>CurvatureInterface</code>, methods for gradients are required by subclasses.</p> <p>Parameters:</p> <p>Attributes:</p> <ul> <li> <code>lossfunc</code>               (<code>MSELoss or CrossEntropyLoss</code>)           \u2013            </li> <li> <code>factor</code>               (<code>float</code>)           \u2013            <p>conversion factor between torch losses and base likelihoods For example, \\(\\frac{1}{2}\\) to get to \\(\\mathcal{N}(f, 1)\\) from MSELoss.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_{\\theta} f(x;\\theta)\\) at current parameter \\(\\theta\\),</p> </li> <li> <code>last_layer_jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\)</p> </li> <li> <code>gradients</code>             \u2013              <p>Compute batch gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at</p> </li> <li> <code>kron</code>             \u2013              <p>Compute a Kronecker factored curvature approximation (such as KFAC).</p> </li> <li> <code>full</code>             \u2013              <p>Compute the full EF \\(P \\times P\\) matrix as Hessian approximation</p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    last_layer: bool = False,\n    subnetwork_indices: torch.LongTensor | None = None,\n    dict_key_x: str = \"input_ids\",\n    dict_key_y: str = \"labels\",\n):\n    assert likelihood in [Likelihood.REGRESSION, Likelihood.CLASSIFICATION]\n    self.likelihood: Likelihood | str = likelihood\n    self.model: nn.Module = model\n    self.last_layer: bool = last_layer\n    self.subnetwork_indices: torch.LongTensor | None = subnetwork_indices\n    self.dict_key_x = dict_key_x\n    self.dict_key_y = dict_key_y\n\n    if likelihood == \"regression\":\n        self.lossfunc: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = (\n            MSELoss(reduction=\"sum\")\n        )\n        self.factor: float = 0.5\n    else:\n        self.lossfunc: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = (\n            CrossEntropyLoss(reduction=\"sum\")\n        )\n        self.factor: float = 1.0\n\n    self.params: list[nn.Parameter] = [\n        p for p in self._model.parameters() if p.requires_grad\n    ]\n    self.params_dict: dict[str, nn.Parameter] = {\n        k: v for k, v in self._model.named_parameters() if v.requires_grad\n    }\n    self.buffers_dict: dict[str, torch.Tensor] = {\n        k: v for k, v in self.model.named_buffers()\n    }\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.EFInterface(model)","title":"<code>model</code>","text":"(<code>torch.nn.Module or `laplace.utils.feature_extractor.FeatureExtractor`</code>)           \u2013            <p>torch model (neural network)</p>"},{"location":"api_reference/curvatures/#laplace.curvature.EFInterface(likelihood)","title":"<code>likelihood</code>","text":"(<code>('classification', 'regression')</code>, default:                   <code>'classification'</code> )           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.EFInterface(last_layer)","title":"<code>last_layer</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>only consider curvature of last layer</p>"},{"location":"api_reference/curvatures/#laplace.curvature.EFInterface(subnetwork_indices)","title":"<code>subnetwork_indices</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>indices of the vectorized model parameters that define the subnetwork to apply the Laplace approximation over</p>"},{"location":"api_reference/curvatures/#laplace.curvature.EFInterface(dict_key_x)","title":"<code>dict_key_x</code>","text":"(<code>str</code>, default:                   <code>'input_ids'</code> )           \u2013            <p>The dictionary key under which the input tensor <code>x</code> is stored. Only has effect when the model takes a <code>MutableMapping</code> as the input. Useful for Huggingface LLM models.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.EFInterface(dict_key_y)","title":"<code>dict_key_y</code>","text":"(<code>str</code>, default:                   <code>'labels'</code> )           \u2013            <p>The dictionary key under which the target tensor <code>y</code> is stored. Only has effect when the model takes a <code>MutableMapping</code> as the input. Useful for Huggingface LLM models.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.EFInterface.jacobians","title":"jacobians","text":"<pre><code>jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_{\\theta} f(x;\\theta)\\) at current parameter \\(\\theta\\), via torch.func.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, parameters, outputs)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_{\\\\theta} f(x;\\\\theta)\\\\) at current parameter \\\\(\\\\theta\\\\),\n    via torch.func.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)` on compatible device with model.\n    enable_backprop : bool, default = False\n        whether to enable backprop through the Js and f w.r.t. x\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, parameters, outputs)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n\n    def model_fn_params_only(params_dict, buffers_dict):\n        out = torch.func.functional_call(self.model, (params_dict, buffers_dict), x)\n        return out, out\n\n    Js, f = torch.func.jacrev(model_fn_params_only, has_aux=True)(\n        self.params_dict, self.buffers_dict\n    )\n\n    # Concatenate over flattened parameters\n    Js = [\n        j.flatten(start_dim=-p.dim())\n        for j, p in zip(Js.values(), self.params_dict.values())\n    ]\n    Js = torch.cat(Js, dim=-1)\n\n    if self.subnetwork_indices is not None:\n        Js = Js[:, :, self.subnetwork_indices]\n\n    return (Js, f) if enable_backprop else (Js.detach(), f.detach())\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.EFInterface.jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.EFInterface.jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>= False</code> )           \u2013            <p>whether to enable backprop through the Js and f w.r.t. x</p>"},{"location":"api_reference/curvatures/#laplace.curvature.EFInterface.last_layer_jacobians","title":"last_layer_jacobians","text":"<pre><code>last_layer_jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\) only at current last-layer parameter \\(\\theta_{\\textrm{last}}\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, outputs, last-layer-parameters)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def last_layer_jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_{\\\\theta_\\\\textrm{last}} f(x;\\\\theta_\\\\textrm{last})\\\\)\n    only at current last-layer parameter \\\\(\\\\theta_{\\\\textrm{last}}\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n    enable_backprop : bool, default=False\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, outputs, last-layer-parameters)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n    f, phi = self.model.forward_with_features(x)\n    bsize = phi.shape[0]\n    output_size = int(f.numel() / bsize)\n\n    # calculate Jacobians using the feature vector 'phi'\n    p = next(self.model.parameters())\n    identity = (\n        torch.eye(output_size, device=p.device, dtype=p.dtype)\n        .unsqueeze(0)\n        .tile(bsize, 1, 1)\n    )\n    # Jacobians are batch x output x params\n    Js = torch.einsum(\"kp,kij-&gt;kijp\", phi, identity).reshape(bsize, output_size, -1)\n    if self.model.last_layer.bias is not None:\n        Js = torch.cat([Js, identity], dim=2)\n\n    return (Js, f) if enable_backprop else (Js.detach(), f.detach())\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.EFInterface.last_layer_jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.EFInterface.last_layer_jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.EFInterface.gradients","title":"gradients","text":"<pre><code>gradients(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute batch gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter \\(\\theta\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Gs</code> (              <code>Tensor</code> )          \u2013            <p>gradients <code>(batch, parameters)</code></p> </li> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def gradients(\n    self, x: torch.Tensor | MutableMapping[str, torch.Tensor | Any], y: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute batch gradients \\\\(\\\\nabla_\\\\theta \\\\ell(f(x;\\\\theta, y)\\\\) at\n    current parameter \\\\(\\\\theta\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)` on compatible device with model.\n    y : torch.Tensor\n\n    Returns\n    -------\n    Gs : torch.Tensor\n        gradients `(batch, parameters)`\n    loss : torch.Tensor\n    \"\"\"\n\n    def loss_single(x, y, params_dict, buffers_dict):\n        \"\"\"Compute the gradient for a single sample.\"\"\"\n        x, y = x.unsqueeze(0), y.unsqueeze(0)  # vmap removes the batch dimension\n        output = torch.func.functional_call(\n            self.model, (params_dict, buffers_dict), x\n        )\n        loss = torch.func.functional_call(self.lossfunc, {}, (output, y))\n        return loss, loss\n\n    grad_fn = torch.func.grad(loss_single, argnums=2, has_aux=True)\n    batch_grad_fn = torch.func.vmap(grad_fn, in_dims=(0, 0, None, None))\n\n    batch_grad, batch_loss = batch_grad_fn(\n        x, y, self.params_dict, self.buffers_dict\n    )\n    Gs = torch.cat([bg.flatten(start_dim=1) for bg in batch_grad.values()], dim=1)\n\n    if self.subnetwork_indices is not None:\n        Gs = Gs[:, self.subnetwork_indices]\n\n    loss = batch_loss.sum(0)\n\n    return Gs, loss\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.EFInterface.gradients(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.EFInterface.gradients(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.EFInterface.kron","title":"kron","text":"<pre><code>kron(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor, N: int, **kwargs: dict[str, Any]) -&gt; tuple[Tensor, Kron]\n</code></pre> <p>Compute a Kronecker factored curvature approximation (such as KFAC). The approximation to \\(H\\) takes the form of two Kronecker factors \\(Q, H\\), i.e., \\(H \\approx Q \\otimes H\\) for each Module in the neural network permitting such curvature. \\(Q\\) is quadratic in the input-dimension of a module \\(p_{in} \\times p_{in}\\) and \\(H\\) in the output-dimension \\(p_{out} \\times p_{out}\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> <li> <code>H</code> (              <code>`laplace.utils.matrix.Kron`</code> )          \u2013            <p>Kronecker factored Hessian approximation.</p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def kron(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    y: torch.Tensor,\n    N: int,\n    **kwargs: dict[str, Any],\n) -&gt; tuple[torch.Tensor, Kron]:\n    \"\"\"Compute a Kronecker factored curvature approximation (such as KFAC).\n    The approximation to \\\\(H\\\\) takes the form of two Kronecker factors \\\\(Q, H\\\\),\n    i.e., \\\\(H \\\\approx Q \\\\otimes H\\\\) for each Module in the neural network permitting\n    such curvature.\n    \\\\(Q\\\\) is quadratic in the input-dimension of a module \\\\(p_{in} \\\\times p_{in}\\\\)\n    and \\\\(H\\\\) in the output-dimension \\\\(p_{out} \\\\times p_{out}\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)`\n    y : torch.Tensor\n        labels `(batch, label_shape)`\n    N : int\n        total number of data points\n\n    Returns\n    -------\n    loss : torch.Tensor\n    H : `laplace.utils.matrix.Kron`\n        Kronecker factored Hessian approximation.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.EFInterface.kron(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.EFInterface.kron(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013            <p>labels <code>(batch, label_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.EFInterface.kron(N)","title":"<code>N</code>","text":"(<code>int</code>)           \u2013            <p>total number of data points</p>"},{"location":"api_reference/curvatures/#laplace.curvature.EFInterface.full","title":"full","text":"<pre><code>full(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor, **kwargs: dict[str, Any]) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute the full EF \\(P \\times P\\) matrix as Hessian approximation \\(H_{ef}\\) with respect to parameters \\(\\theta \\in \\mathbb{R}^P\\). For last-layer, reduced to \\(\\theta_{last}\\)</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> <li> <code>H_ef</code> (              <code>Tensor</code> )          \u2013            <p>EF <code>(parameters, parameters)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def full(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    y: torch.Tensor,\n    **kwargs: dict[str, Any],\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the full EF \\\\(P \\\\times P\\\\) matrix as Hessian approximation\n    \\\\(H_{ef}\\\\) with respect to parameters \\\\(\\\\theta \\\\in \\\\mathbb{R}^P\\\\).\n    For last-layer, reduced to \\\\(\\\\theta_{last}\\\\)\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)`\n    y : torch.Tensor\n        labels `(batch, label_shape)`\n\n    Returns\n    -------\n    loss : torch.Tensor\n    H_ef : torch.Tensor\n        EF `(parameters, parameters)`\n    \"\"\"\n    Gs, loss = self.gradients(x, y)\n    Gs, loss = Gs.detach(), loss.detach()\n    H_ef = torch.einsum(\"bp,bq-&gt;pq\", Gs, Gs)\n    return self.factor * loss.detach(), self.factor * H_ef\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.EFInterface.full(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.EFInterface.full(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013            <p>labels <code>(batch, label_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlInterface","title":"AsdlInterface","text":"<pre><code>AsdlInterface(model: Module, likelihood: Likelihood | str, last_layer: bool = False, subnetwork_indices: LongTensor | None = None, dict_key_x: str = 'input_ids', dict_key_y: str = 'labels')\n</code></pre> <p>               Bases: <code>CurvatureInterface</code></p> <p>Interface for asdfghjkl backend.</p> <p>Methods:</p> <ul> <li> <code>last_layer_jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\)</p> </li> <li> <code>full</code>             \u2013              <p>Compute a dense curvature (approximation) in the form of a \\(P \\times P\\) matrix</p> </li> <li> <code>jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_\\theta f(x;\\theta)\\) at current parameter \\(\\theta\\)</p> </li> <li> <code>gradients</code>             \u2013              <p>Compute gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter</p> </li> </ul> Source code in <code>laplace/curvature/asdl.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    last_layer: bool = False,\n    subnetwork_indices: torch.LongTensor | None = None,\n    dict_key_x: str = \"input_ids\",\n    dict_key_y: str = \"labels\",\n):\n    super().__init__(\n        model, likelihood, last_layer, subnetwork_indices, dict_key_x, dict_key_y\n    )\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlInterface.last_layer_jacobians","title":"last_layer_jacobians","text":"<pre><code>last_layer_jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\) only at current last-layer parameter \\(\\theta_{\\textrm{last}}\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, outputs, last-layer-parameters)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def last_layer_jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_{\\\\theta_\\\\textrm{last}} f(x;\\\\theta_\\\\textrm{last})\\\\)\n    only at current last-layer parameter \\\\(\\\\theta_{\\\\textrm{last}}\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n    enable_backprop : bool, default=False\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, outputs, last-layer-parameters)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n    f, phi = self.model.forward_with_features(x)\n    bsize = phi.shape[0]\n    output_size = int(f.numel() / bsize)\n\n    # calculate Jacobians using the feature vector 'phi'\n    p = next(self.model.parameters())\n    identity = (\n        torch.eye(output_size, device=p.device, dtype=p.dtype)\n        .unsqueeze(0)\n        .tile(bsize, 1, 1)\n    )\n    # Jacobians are batch x output x params\n    Js = torch.einsum(\"kp,kij-&gt;kijp\", phi, identity).reshape(bsize, output_size, -1)\n    if self.model.last_layer.bias is not None:\n        Js = torch.cat([Js, identity], dim=2)\n\n    return (Js, f) if enable_backprop else (Js.detach(), f.detach())\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlInterface.last_layer_jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlInterface.last_layer_jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlInterface.full","title":"full","text":"<pre><code>full(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor, **kwargs: dict[str, Any])\n</code></pre> <p>Compute a dense curvature (approximation) in the form of a \\(P \\times P\\) matrix \\(H\\) with respect to parameters \\(\\theta \\in \\mathbb{R}^P\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> <li> <code>H</code> (              <code>Tensor</code> )          \u2013            <p>Hessian approximation <code>(parameters, parameters)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def full(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    y: torch.Tensor,\n    **kwargs: dict[str, Any],\n):\n    \"\"\"Compute a dense curvature (approximation) in the form of a \\\\(P \\\\times P\\\\) matrix\n    \\\\(H\\\\) with respect to parameters \\\\(\\\\theta \\\\in \\\\mathbb{R}^P\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)`\n    y : torch.Tensor\n        labels `(batch, label_shape)`\n\n    Returns\n    -------\n    loss : torch.Tensor\n    H : torch.Tensor\n        Hessian approximation `(parameters, parameters)`\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlInterface.full(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlInterface.full(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013            <p>labels <code>(batch, label_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlInterface.jacobians","title":"jacobians","text":"<pre><code>jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_\\theta f(x;\\theta)\\) at current parameter \\(\\theta\\) using asdfghjkl's gradient per output dimension.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, parameters, outputs)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/asdl.py</code> <pre><code>def jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_\\\\theta f(x;\\\\theta)\\\\) at current parameter \\\\(\\\\theta\\\\)\n    using asdfghjkl's gradient per output dimension.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping (e.g. dict, UserDict)\n        input data `(batch, input_shape)` on compatible device with model if torch.Tensor.\n        If MutableMapping, then at least contains `self.dict_key_x`.\n        The latter is specific for reward modeling.\n    enable_backprop : bool, default = False\n        whether to enable backprop through the Js and f w.r.t. x\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, parameters, outputs)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n    Js = list()\n    for i in range(self.model.output_size):\n\n        def closure():\n            self.model.zero_grad()\n            f = self.model(x)\n            loss = f[:, i].sum()\n            loss.backward(\n                create_graph=enable_backprop, retain_graph=enable_backprop\n            )\n            return f\n\n        Ji, f = batch_gradient(\n            self.model,\n            closure,\n            return_outputs=True,\n            batch_size=self._get_batch_size(x),\n        )\n        if self.subnetwork_indices is not None:\n            Ji = Ji[:, self.subnetwork_indices]\n        Js.append(Ji)\n    Js = torch.stack(Js, dim=1)\n    return Js, f\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlInterface.jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping(dict, UserDict)</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model if torch.Tensor. If MutableMapping, then at least contains <code>self.dict_key_x</code>. The latter is specific for reward modeling.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlInterface.jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>= False</code> )           \u2013            <p>whether to enable backprop through the Js and f w.r.t. x</p>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlInterface.gradients","title":"gradients","text":"<pre><code>gradients(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter \\(\\theta\\) using asdfghjkl's backend.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> <li> <code>Gs</code> (              <code>Tensor</code> )          \u2013            <p>gradients <code>(batch, parameters)</code></p> </li> </ul> Source code in <code>laplace/curvature/asdl.py</code> <pre><code>def gradients(\n    self, x: torch.Tensor | MutableMapping[str, torch.Tensor | Any], y: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute gradients \\\\(\\\\nabla_\\\\theta \\\\ell(f(x;\\\\theta, y)\\\\) at current parameter\n    \\\\(\\\\theta\\\\) using asdfghjkl's backend.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)` on compatible device with model.\n    y : torch.Tensor\n\n    Returns\n    -------\n    loss : torch.Tensor\n    Gs : torch.Tensor\n        gradients `(batch, parameters)`\n    \"\"\"\n\n    def closure():\n        self.model.zero_grad()\n        loss = self.lossfunc(self.model(x), y)\n        loss.backward()\n        return loss\n\n    Gs, loss = batch_gradient(\n        self.model, closure, return_outputs=True, batch_size=self._get_batch_size(x)\n    )\n    if self.subnetwork_indices is not None:\n        Gs = Gs[:, self.subnetwork_indices]\n    return Gs, loss\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlInterface.gradients(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlInterface.gradients(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlInterface._get_batch_size","title":"_get_batch_size","text":"<pre><code>_get_batch_size(x: Tensor | MutableMapping[str, Tensor | Any]) -&gt; int | None\n</code></pre> <p>ASDL assumes that all leading dimensions are the batch size by default (batch_size = None). Here, we want to specify that only the first dimension is the actual batch size. This is the case for LLMs.</p> Source code in <code>laplace/curvature/asdl.py</code> <pre><code>def _get_batch_size(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n) -&gt; int | None:\n    \"\"\"\n    ASDL assumes that all leading dimensions are the batch size by default (batch_size = None).\n    Here, we want to specify that only the first dimension is the actual batch size.\n    This is the case for LLMs.\n    \"\"\"\n    if isinstance(x, MutableMapping):\n        return x[self.dict_key_x].shape[0]\n    else:\n        return None  # Use ASDL default behavior\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlGGN","title":"AsdlGGN","text":"<pre><code>AsdlGGN(model: Module, likelihood: Likelihood | str, last_layer: bool = False, subnetwork_indices: LongTensor | None = None, dict_key_x: str = 'input_ids', dict_key_y: str = 'labels', stochastic: bool = False)\n</code></pre> <p>               Bases: <code>AsdlInterface</code>, <code>GGNInterface</code></p> <p>Implementation of the <code>GGNInterface</code> using asdfghjkl.</p> <p>Methods:</p> <ul> <li> <code>jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_\\theta f(x;\\theta)\\) at current parameter \\(\\theta\\)</p> </li> <li> <code>last_layer_jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\)</p> </li> <li> <code>gradients</code>             \u2013              <p>Compute gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter</p> </li> <li> <code>full</code>             \u2013              <p>Compute the full GGN \\(P \\times P\\) matrix as Hessian approximation</p> </li> </ul> Source code in <code>laplace/curvature/asdl.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    last_layer: bool = False,\n    subnetwork_indices: torch.LongTensor | None = None,\n    dict_key_x: str = \"input_ids\",\n    dict_key_y: str = \"labels\",\n    stochastic: bool = False,\n):\n    super().__init__(\n        model, likelihood, last_layer, subnetwork_indices, dict_key_x, dict_key_y\n    )\n    self.stochastic = stochastic\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlGGN.jacobians","title":"jacobians","text":"<pre><code>jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_\\theta f(x;\\theta)\\) at current parameter \\(\\theta\\) using asdfghjkl's gradient per output dimension.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, parameters, outputs)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/asdl.py</code> <pre><code>def jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_\\\\theta f(x;\\\\theta)\\\\) at current parameter \\\\(\\\\theta\\\\)\n    using asdfghjkl's gradient per output dimension.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping (e.g. dict, UserDict)\n        input data `(batch, input_shape)` on compatible device with model if torch.Tensor.\n        If MutableMapping, then at least contains `self.dict_key_x`.\n        The latter is specific for reward modeling.\n    enable_backprop : bool, default = False\n        whether to enable backprop through the Js and f w.r.t. x\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, parameters, outputs)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n    Js = list()\n    for i in range(self.model.output_size):\n\n        def closure():\n            self.model.zero_grad()\n            f = self.model(x)\n            loss = f[:, i].sum()\n            loss.backward(\n                create_graph=enable_backprop, retain_graph=enable_backprop\n            )\n            return f\n\n        Ji, f = batch_gradient(\n            self.model,\n            closure,\n            return_outputs=True,\n            batch_size=self._get_batch_size(x),\n        )\n        if self.subnetwork_indices is not None:\n            Ji = Ji[:, self.subnetwork_indices]\n        Js.append(Ji)\n    Js = torch.stack(Js, dim=1)\n    return Js, f\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlGGN.jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping(dict, UserDict)</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model if torch.Tensor. If MutableMapping, then at least contains <code>self.dict_key_x</code>. The latter is specific for reward modeling.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlGGN.jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>= False</code> )           \u2013            <p>whether to enable backprop through the Js and f w.r.t. x</p>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlGGN.last_layer_jacobians","title":"last_layer_jacobians","text":"<pre><code>last_layer_jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\) only at current last-layer parameter \\(\\theta_{\\textrm{last}}\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, outputs, last-layer-parameters)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def last_layer_jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_{\\\\theta_\\\\textrm{last}} f(x;\\\\theta_\\\\textrm{last})\\\\)\n    only at current last-layer parameter \\\\(\\\\theta_{\\\\textrm{last}}\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n    enable_backprop : bool, default=False\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, outputs, last-layer-parameters)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n    f, phi = self.model.forward_with_features(x)\n    bsize = phi.shape[0]\n    output_size = int(f.numel() / bsize)\n\n    # calculate Jacobians using the feature vector 'phi'\n    p = next(self.model.parameters())\n    identity = (\n        torch.eye(output_size, device=p.device, dtype=p.dtype)\n        .unsqueeze(0)\n        .tile(bsize, 1, 1)\n    )\n    # Jacobians are batch x output x params\n    Js = torch.einsum(\"kp,kij-&gt;kijp\", phi, identity).reshape(bsize, output_size, -1)\n    if self.model.last_layer.bias is not None:\n        Js = torch.cat([Js, identity], dim=2)\n\n    return (Js, f) if enable_backprop else (Js.detach(), f.detach())\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlGGN.last_layer_jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlGGN.last_layer_jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlGGN.gradients","title":"gradients","text":"<pre><code>gradients(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter \\(\\theta\\) using asdfghjkl's backend.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> <li> <code>Gs</code> (              <code>Tensor</code> )          \u2013            <p>gradients <code>(batch, parameters)</code></p> </li> </ul> Source code in <code>laplace/curvature/asdl.py</code> <pre><code>def gradients(\n    self, x: torch.Tensor | MutableMapping[str, torch.Tensor | Any], y: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute gradients \\\\(\\\\nabla_\\\\theta \\\\ell(f(x;\\\\theta, y)\\\\) at current parameter\n    \\\\(\\\\theta\\\\) using asdfghjkl's backend.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)` on compatible device with model.\n    y : torch.Tensor\n\n    Returns\n    -------\n    loss : torch.Tensor\n    Gs : torch.Tensor\n        gradients `(batch, parameters)`\n    \"\"\"\n\n    def closure():\n        self.model.zero_grad()\n        loss = self.lossfunc(self.model(x), y)\n        loss.backward()\n        return loss\n\n    Gs, loss = batch_gradient(\n        self.model, closure, return_outputs=True, batch_size=self._get_batch_size(x)\n    )\n    if self.subnetwork_indices is not None:\n        Gs = Gs[:, self.subnetwork_indices]\n    return Gs, loss\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlGGN.gradients(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlGGN.gradients(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlGGN.full","title":"full","text":"<pre><code>full(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor, **kwargs: dict[str, Any]) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute the full GGN \\(P \\times P\\) matrix as Hessian approximation \\(H_{ggn}\\) with respect to parameters \\(\\theta \\in \\mathbb{R}^P\\). For last-layer, reduced to \\(\\theta_{last}\\)</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> <li> <code>H</code> (              <code>Tensor</code> )          \u2013            <p>GGN <code>(parameters, parameters)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def full(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    y: torch.Tensor,\n    **kwargs: dict[str, Any],\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the full GGN \\\\(P \\\\times P\\\\) matrix as Hessian approximation\n    \\\\(H_{ggn}\\\\) with respect to parameters \\\\(\\\\theta \\\\in \\\\mathbb{R}^P\\\\).\n    For last-layer, reduced to \\\\(\\\\theta_{last}\\\\)\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)`\n    y : torch.Tensor\n        labels `(batch, label_shape)`\n\n    Returns\n    -------\n    loss : torch.Tensor\n    H : torch.Tensor\n        GGN `(parameters, parameters)`\n    \"\"\"\n    Js, f = self.last_layer_jacobians(x) if self.last_layer else self.jacobians(x)\n    H_lik = (\n        self._get_mc_functional_fisher(f)\n        if self.stochastic\n        else self._get_functional_hessian(f)\n    )\n\n    if H_lik is not None:\n        H = torch.einsum(\"bcp,bck,bkq-&gt;pq\", Js, H_lik, Js)\n    else:  # The case of exact GGN for regression\n        H = torch.einsum(\"bcp,bcq-&gt;pq\", Js, Js)\n    loss = self.factor * self.lossfunc(f, y)\n\n    return loss.detach(), H.detach()\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlGGN.full(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlGGN.full(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013            <p>labels <code>(batch, label_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlGGN._get_mc_functional_fisher","title":"_get_mc_functional_fisher","text":"<pre><code>_get_mc_functional_fisher(f: Tensor) -&gt; Tensor\n</code></pre> <p>Approximate the Fisher's middle matrix (expected outer product of the functional gradient) using MC integral with <code>self.num_samples</code> many samples.</p> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def _get_mc_functional_fisher(self, f: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Approximate the Fisher's middle matrix (expected outer product of the functional gradient)\n    using MC integral with `self.num_samples` many samples.\n    \"\"\"\n    F = 0\n\n    for _ in range(self.num_samples):\n        if self.likelihood == \"regression\":\n            # N(y | f, 1)\n            y_sample = f + torch.randn(f.shape, device=f.device, dtype=f.dtype)\n            grad_sample = f - y_sample  # functional MSE grad\n        else:  # classification with softmax\n            y_sample = torch.distributions.Multinomial(logits=f).sample()\n            # First functional derivative of the loglik is p - y\n            p = torch.softmax(f, dim=-1)\n            grad_sample = p - y_sample\n\n        F += (\n            1\n            / self.num_samples\n            * torch.einsum(\"bc,bk-&gt;bck\", grad_sample, grad_sample)\n        )\n\n    return F\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlGGN._get_batch_size","title":"_get_batch_size","text":"<pre><code>_get_batch_size(x: Tensor | MutableMapping[str, Tensor | Any]) -&gt; int | None\n</code></pre> <p>ASDL assumes that all leading dimensions are the batch size by default (batch_size = None). Here, we want to specify that only the first dimension is the actual batch size. This is the case for LLMs.</p> Source code in <code>laplace/curvature/asdl.py</code> <pre><code>def _get_batch_size(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n) -&gt; int | None:\n    \"\"\"\n    ASDL assumes that all leading dimensions are the batch size by default (batch_size = None).\n    Here, we want to specify that only the first dimension is the actual batch size.\n    This is the case for LLMs.\n    \"\"\"\n    if isinstance(x, MutableMapping):\n        return x[self.dict_key_x].shape[0]\n    else:\n        return None  # Use ASDL default behavior\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlEF","title":"AsdlEF","text":"<pre><code>AsdlEF(model: Module, likelihood: Likelihood | str, last_layer: bool = False, dict_key_x: str = 'input_ids', dict_key_y: str = 'labels')\n</code></pre> <p>               Bases: <code>AsdlInterface</code>, <code>EFInterface</code></p> <p>Implementation of the <code>EFInterface</code> using asdfghjkl.</p> <p>Methods:</p> <ul> <li> <code>jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_\\theta f(x;\\theta)\\) at current parameter \\(\\theta\\)</p> </li> <li> <code>last_layer_jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\)</p> </li> <li> <code>gradients</code>             \u2013              <p>Compute gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter</p> </li> <li> <code>full</code>             \u2013              <p>Compute the full EF \\(P \\times P\\) matrix as Hessian approximation</p> </li> </ul> Source code in <code>laplace/curvature/asdl.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    last_layer: bool = False,\n    dict_key_x: str = \"input_ids\",\n    dict_key_y: str = \"labels\",\n):\n    super().__init__(model, likelihood, last_layer, None, dict_key_x, dict_key_y)\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlEF.jacobians","title":"jacobians","text":"<pre><code>jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_\\theta f(x;\\theta)\\) at current parameter \\(\\theta\\) using asdfghjkl's gradient per output dimension.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, parameters, outputs)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/asdl.py</code> <pre><code>def jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_\\\\theta f(x;\\\\theta)\\\\) at current parameter \\\\(\\\\theta\\\\)\n    using asdfghjkl's gradient per output dimension.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping (e.g. dict, UserDict)\n        input data `(batch, input_shape)` on compatible device with model if torch.Tensor.\n        If MutableMapping, then at least contains `self.dict_key_x`.\n        The latter is specific for reward modeling.\n    enable_backprop : bool, default = False\n        whether to enable backprop through the Js and f w.r.t. x\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, parameters, outputs)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n    Js = list()\n    for i in range(self.model.output_size):\n\n        def closure():\n            self.model.zero_grad()\n            f = self.model(x)\n            loss = f[:, i].sum()\n            loss.backward(\n                create_graph=enable_backprop, retain_graph=enable_backprop\n            )\n            return f\n\n        Ji, f = batch_gradient(\n            self.model,\n            closure,\n            return_outputs=True,\n            batch_size=self._get_batch_size(x),\n        )\n        if self.subnetwork_indices is not None:\n            Ji = Ji[:, self.subnetwork_indices]\n        Js.append(Ji)\n    Js = torch.stack(Js, dim=1)\n    return Js, f\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlEF.jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping(dict, UserDict)</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model if torch.Tensor. If MutableMapping, then at least contains <code>self.dict_key_x</code>. The latter is specific for reward modeling.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlEF.jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>= False</code> )           \u2013            <p>whether to enable backprop through the Js and f w.r.t. x</p>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlEF.last_layer_jacobians","title":"last_layer_jacobians","text":"<pre><code>last_layer_jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\) only at current last-layer parameter \\(\\theta_{\\textrm{last}}\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, outputs, last-layer-parameters)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def last_layer_jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_{\\\\theta_\\\\textrm{last}} f(x;\\\\theta_\\\\textrm{last})\\\\)\n    only at current last-layer parameter \\\\(\\\\theta_{\\\\textrm{last}}\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n    enable_backprop : bool, default=False\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, outputs, last-layer-parameters)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n    f, phi = self.model.forward_with_features(x)\n    bsize = phi.shape[0]\n    output_size = int(f.numel() / bsize)\n\n    # calculate Jacobians using the feature vector 'phi'\n    p = next(self.model.parameters())\n    identity = (\n        torch.eye(output_size, device=p.device, dtype=p.dtype)\n        .unsqueeze(0)\n        .tile(bsize, 1, 1)\n    )\n    # Jacobians are batch x output x params\n    Js = torch.einsum(\"kp,kij-&gt;kijp\", phi, identity).reshape(bsize, output_size, -1)\n    if self.model.last_layer.bias is not None:\n        Js = torch.cat([Js, identity], dim=2)\n\n    return (Js, f) if enable_backprop else (Js.detach(), f.detach())\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlEF.last_layer_jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlEF.last_layer_jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlEF.gradients","title":"gradients","text":"<pre><code>gradients(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter \\(\\theta\\) using asdfghjkl's backend.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> <li> <code>Gs</code> (              <code>Tensor</code> )          \u2013            <p>gradients <code>(batch, parameters)</code></p> </li> </ul> Source code in <code>laplace/curvature/asdl.py</code> <pre><code>def gradients(\n    self, x: torch.Tensor | MutableMapping[str, torch.Tensor | Any], y: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute gradients \\\\(\\\\nabla_\\\\theta \\\\ell(f(x;\\\\theta, y)\\\\) at current parameter\n    \\\\(\\\\theta\\\\) using asdfghjkl's backend.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)` on compatible device with model.\n    y : torch.Tensor\n\n    Returns\n    -------\n    loss : torch.Tensor\n    Gs : torch.Tensor\n        gradients `(batch, parameters)`\n    \"\"\"\n\n    def closure():\n        self.model.zero_grad()\n        loss = self.lossfunc(self.model(x), y)\n        loss.backward()\n        return loss\n\n    Gs, loss = batch_gradient(\n        self.model, closure, return_outputs=True, batch_size=self._get_batch_size(x)\n    )\n    if self.subnetwork_indices is not None:\n        Gs = Gs[:, self.subnetwork_indices]\n    return Gs, loss\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlEF.gradients(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlEF.gradients(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlEF.full","title":"full","text":"<pre><code>full(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor, **kwargs: dict[str, Any]) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute the full EF \\(P \\times P\\) matrix as Hessian approximation \\(H_{ef}\\) with respect to parameters \\(\\theta \\in \\mathbb{R}^P\\). For last-layer, reduced to \\(\\theta_{last}\\)</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> <li> <code>H_ef</code> (              <code>Tensor</code> )          \u2013            <p>EF <code>(parameters, parameters)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def full(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    y: torch.Tensor,\n    **kwargs: dict[str, Any],\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the full EF \\\\(P \\\\times P\\\\) matrix as Hessian approximation\n    \\\\(H_{ef}\\\\) with respect to parameters \\\\(\\\\theta \\\\in \\\\mathbb{R}^P\\\\).\n    For last-layer, reduced to \\\\(\\\\theta_{last}\\\\)\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)`\n    y : torch.Tensor\n        labels `(batch, label_shape)`\n\n    Returns\n    -------\n    loss : torch.Tensor\n    H_ef : torch.Tensor\n        EF `(parameters, parameters)`\n    \"\"\"\n    Gs, loss = self.gradients(x, y)\n    Gs, loss = Gs.detach(), loss.detach()\n    H_ef = torch.einsum(\"bp,bq-&gt;pq\", Gs, Gs)\n    return self.factor * loss.detach(), self.factor * H_ef\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlEF.full(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlEF.full(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013            <p>labels <code>(batch, label_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlEF._get_batch_size","title":"_get_batch_size","text":"<pre><code>_get_batch_size(x: Tensor | MutableMapping[str, Tensor | Any]) -&gt; int | None\n</code></pre> <p>ASDL assumes that all leading dimensions are the batch size by default (batch_size = None). Here, we want to specify that only the first dimension is the actual batch size. This is the case for LLMs.</p> Source code in <code>laplace/curvature/asdl.py</code> <pre><code>def _get_batch_size(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n) -&gt; int | None:\n    \"\"\"\n    ASDL assumes that all leading dimensions are the batch size by default (batch_size = None).\n    Here, we want to specify that only the first dimension is the actual batch size.\n    This is the case for LLMs.\n    \"\"\"\n    if isinstance(x, MutableMapping):\n        return x[self.dict_key_x].shape[0]\n    else:\n        return None  # Use ASDL default behavior\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlHessian","title":"AsdlHessian","text":"<pre><code>AsdlHessian(model: Module, likelihood: Likelihood | str, last_layer: bool = False, dict_key_x: str = 'input_ids', dict_key_y: str = 'labels')\n</code></pre> <p>               Bases: <code>AsdlInterface</code></p> <p>Methods:</p> <ul> <li> <code>jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_\\theta f(x;\\theta)\\) at current parameter \\(\\theta\\)</p> </li> <li> <code>last_layer_jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\)</p> </li> <li> <code>gradients</code>             \u2013              <p>Compute gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter</p> </li> </ul> Source code in <code>laplace/curvature/asdl.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    last_layer: bool = False,\n    dict_key_x: str = \"input_ids\",\n    dict_key_y: str = \"labels\",\n) -&gt; None:\n    super().__init__(\n        model,\n        likelihood,\n        last_layer,\n        subnetwork_indices=None,\n        dict_key_x=dict_key_x,\n        dict_key_y=dict_key_y,\n    )\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlHessian.jacobians","title":"jacobians","text":"<pre><code>jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_\\theta f(x;\\theta)\\) at current parameter \\(\\theta\\) using asdfghjkl's gradient per output dimension.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, parameters, outputs)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/asdl.py</code> <pre><code>def jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_\\\\theta f(x;\\\\theta)\\\\) at current parameter \\\\(\\\\theta\\\\)\n    using asdfghjkl's gradient per output dimension.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping (e.g. dict, UserDict)\n        input data `(batch, input_shape)` on compatible device with model if torch.Tensor.\n        If MutableMapping, then at least contains `self.dict_key_x`.\n        The latter is specific for reward modeling.\n    enable_backprop : bool, default = False\n        whether to enable backprop through the Js and f w.r.t. x\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, parameters, outputs)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n    Js = list()\n    for i in range(self.model.output_size):\n\n        def closure():\n            self.model.zero_grad()\n            f = self.model(x)\n            loss = f[:, i].sum()\n            loss.backward(\n                create_graph=enable_backprop, retain_graph=enable_backprop\n            )\n            return f\n\n        Ji, f = batch_gradient(\n            self.model,\n            closure,\n            return_outputs=True,\n            batch_size=self._get_batch_size(x),\n        )\n        if self.subnetwork_indices is not None:\n            Ji = Ji[:, self.subnetwork_indices]\n        Js.append(Ji)\n    Js = torch.stack(Js, dim=1)\n    return Js, f\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlHessian.jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping(dict, UserDict)</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model if torch.Tensor. If MutableMapping, then at least contains <code>self.dict_key_x</code>. The latter is specific for reward modeling.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlHessian.jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>= False</code> )           \u2013            <p>whether to enable backprop through the Js and f w.r.t. x</p>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlHessian.last_layer_jacobians","title":"last_layer_jacobians","text":"<pre><code>last_layer_jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\) only at current last-layer parameter \\(\\theta_{\\textrm{last}}\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, outputs, last-layer-parameters)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def last_layer_jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_{\\\\theta_\\\\textrm{last}} f(x;\\\\theta_\\\\textrm{last})\\\\)\n    only at current last-layer parameter \\\\(\\\\theta_{\\\\textrm{last}}\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n    enable_backprop : bool, default=False\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, outputs, last-layer-parameters)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n    f, phi = self.model.forward_with_features(x)\n    bsize = phi.shape[0]\n    output_size = int(f.numel() / bsize)\n\n    # calculate Jacobians using the feature vector 'phi'\n    p = next(self.model.parameters())\n    identity = (\n        torch.eye(output_size, device=p.device, dtype=p.dtype)\n        .unsqueeze(0)\n        .tile(bsize, 1, 1)\n    )\n    # Jacobians are batch x output x params\n    Js = torch.einsum(\"kp,kij-&gt;kijp\", phi, identity).reshape(bsize, output_size, -1)\n    if self.model.last_layer.bias is not None:\n        Js = torch.cat([Js, identity], dim=2)\n\n    return (Js, f) if enable_backprop else (Js.detach(), f.detach())\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlHessian.last_layer_jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlHessian.last_layer_jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlHessian.gradients","title":"gradients","text":"<pre><code>gradients(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter \\(\\theta\\) using asdfghjkl's backend.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> <li> <code>Gs</code> (              <code>Tensor</code> )          \u2013            <p>gradients <code>(batch, parameters)</code></p> </li> </ul> Source code in <code>laplace/curvature/asdl.py</code> <pre><code>def gradients(\n    self, x: torch.Tensor | MutableMapping[str, torch.Tensor | Any], y: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute gradients \\\\(\\\\nabla_\\\\theta \\\\ell(f(x;\\\\theta, y)\\\\) at current parameter\n    \\\\(\\\\theta\\\\) using asdfghjkl's backend.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)` on compatible device with model.\n    y : torch.Tensor\n\n    Returns\n    -------\n    loss : torch.Tensor\n    Gs : torch.Tensor\n        gradients `(batch, parameters)`\n    \"\"\"\n\n    def closure():\n        self.model.zero_grad()\n        loss = self.lossfunc(self.model(x), y)\n        loss.backward()\n        return loss\n\n    Gs, loss = batch_gradient(\n        self.model, closure, return_outputs=True, batch_size=self._get_batch_size(x)\n    )\n    if self.subnetwork_indices is not None:\n        Gs = Gs[:, self.subnetwork_indices]\n    return Gs, loss\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlHessian.gradients(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlHessian.gradients(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.AsdlHessian._get_batch_size","title":"_get_batch_size","text":"<pre><code>_get_batch_size(x: Tensor | MutableMapping[str, Tensor | Any]) -&gt; int | None\n</code></pre> <p>ASDL assumes that all leading dimensions are the batch size by default (batch_size = None). Here, we want to specify that only the first dimension is the actual batch size. This is the case for LLMs.</p> Source code in <code>laplace/curvature/asdl.py</code> <pre><code>def _get_batch_size(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n) -&gt; int | None:\n    \"\"\"\n    ASDL assumes that all leading dimensions are the batch size by default (batch_size = None).\n    Here, we want to specify that only the first dimension is the actual batch size.\n    This is the case for LLMs.\n    \"\"\"\n    if isinstance(x, MutableMapping):\n        return x[self.dict_key_x].shape[0]\n    else:\n        return None  # Use ASDL default behavior\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackInterface","title":"BackPackInterface","text":"<pre><code>BackPackInterface(model: Module, likelihood: Likelihood | str, last_layer: bool = False, subnetwork_indices: LongTensor | None = None, dict_key_x: str = 'input_ids', dict_key_y: str = 'labels')\n</code></pre> <p>               Bases: <code>CurvatureInterface</code></p> <p>Interface for Backpack backend.</p> <p>Methods:</p> <ul> <li> <code>last_layer_jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\)</p> </li> <li> <code>full</code>             \u2013              <p>Compute a dense curvature (approximation) in the form of a \\(P \\times P\\) matrix</p> </li> <li> <code>kron</code>             \u2013              <p>Compute a Kronecker factored curvature approximation (such as KFAC).</p> </li> <li> <code>diag</code>             \u2013              <p>Compute a diagonal Hessian approximation to \\(H\\) and is represented as a</p> </li> <li> <code>jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_{\\theta} f(x;\\theta)\\) at current parameter \\(\\theta\\)</p> </li> <li> <code>gradients</code>             \u2013              <p>Compute gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter</p> </li> </ul> Source code in <code>laplace/curvature/backpack.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    last_layer: bool = False,\n    subnetwork_indices: torch.LongTensor | None = None,\n    dict_key_x: str = \"input_ids\",\n    dict_key_y: str = \"labels\",\n) -&gt; None:\n    super().__init__(\n        model, likelihood, last_layer, subnetwork_indices, dict_key_x, dict_key_y\n    )\n\n    extend(self._model)\n    extend(self.lossfunc)\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackInterface.last_layer_jacobians","title":"last_layer_jacobians","text":"<pre><code>last_layer_jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\) only at current last-layer parameter \\(\\theta_{\\textrm{last}}\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, outputs, last-layer-parameters)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def last_layer_jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_{\\\\theta_\\\\textrm{last}} f(x;\\\\theta_\\\\textrm{last})\\\\)\n    only at current last-layer parameter \\\\(\\\\theta_{\\\\textrm{last}}\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n    enable_backprop : bool, default=False\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, outputs, last-layer-parameters)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n    f, phi = self.model.forward_with_features(x)\n    bsize = phi.shape[0]\n    output_size = int(f.numel() / bsize)\n\n    # calculate Jacobians using the feature vector 'phi'\n    p = next(self.model.parameters())\n    identity = (\n        torch.eye(output_size, device=p.device, dtype=p.dtype)\n        .unsqueeze(0)\n        .tile(bsize, 1, 1)\n    )\n    # Jacobians are batch x output x params\n    Js = torch.einsum(\"kp,kij-&gt;kijp\", phi, identity).reshape(bsize, output_size, -1)\n    if self.model.last_layer.bias is not None:\n        Js = torch.cat([Js, identity], dim=2)\n\n    return (Js, f) if enable_backprop else (Js.detach(), f.detach())\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackInterface.last_layer_jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackInterface.last_layer_jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackInterface.full","title":"full","text":"<pre><code>full(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor, **kwargs: dict[str, Any])\n</code></pre> <p>Compute a dense curvature (approximation) in the form of a \\(P \\times P\\) matrix \\(H\\) with respect to parameters \\(\\theta \\in \\mathbb{R}^P\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> <li> <code>H</code> (              <code>Tensor</code> )          \u2013            <p>Hessian approximation <code>(parameters, parameters)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def full(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    y: torch.Tensor,\n    **kwargs: dict[str, Any],\n):\n    \"\"\"Compute a dense curvature (approximation) in the form of a \\\\(P \\\\times P\\\\) matrix\n    \\\\(H\\\\) with respect to parameters \\\\(\\\\theta \\\\in \\\\mathbb{R}^P\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)`\n    y : torch.Tensor\n        labels `(batch, label_shape)`\n\n    Returns\n    -------\n    loss : torch.Tensor\n    H : torch.Tensor\n        Hessian approximation `(parameters, parameters)`\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackInterface.full(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackInterface.full(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013            <p>labels <code>(batch, label_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackInterface.kron","title":"kron","text":"<pre><code>kron(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor, N: int, **kwargs: dict[str, Any]) -&gt; tuple[Tensor, Kron]\n</code></pre> <p>Compute a Kronecker factored curvature approximation (such as KFAC). The approximation to \\(H\\) takes the form of two Kronecker factors \\(Q, H\\), i.e., \\(H \\approx Q \\otimes H\\) for each Module in the neural network permitting such curvature. \\(Q\\) is quadratic in the input-dimension of a module \\(p_{in} \\times p_{in}\\) and \\(H\\) in the output-dimension \\(p_{out} \\times p_{out}\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> <li> <code>H</code> (              <code>`laplace.utils.matrix.Kron`</code> )          \u2013            <p>Kronecker factored Hessian approximation.</p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def kron(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    y: torch.Tensor,\n    N: int,\n    **kwargs: dict[str, Any],\n) -&gt; tuple[torch.Tensor, Kron]:\n    \"\"\"Compute a Kronecker factored curvature approximation (such as KFAC).\n    The approximation to \\\\(H\\\\) takes the form of two Kronecker factors \\\\(Q, H\\\\),\n    i.e., \\\\(H \\\\approx Q \\\\otimes H\\\\) for each Module in the neural network permitting\n    such curvature.\n    \\\\(Q\\\\) is quadratic in the input-dimension of a module \\\\(p_{in} \\\\times p_{in}\\\\)\n    and \\\\(H\\\\) in the output-dimension \\\\(p_{out} \\\\times p_{out}\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)`\n    y : torch.Tensor\n        labels `(batch, label_shape)`\n    N : int\n        total number of data points\n\n    Returns\n    -------\n    loss : torch.Tensor\n    H : `laplace.utils.matrix.Kron`\n        Kronecker factored Hessian approximation.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackInterface.kron(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackInterface.kron(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013            <p>labels <code>(batch, label_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackInterface.kron(N)","title":"<code>N</code>","text":"(<code>int</code>)           \u2013            <p>total number of data points</p>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackInterface.diag","title":"diag","text":"<pre><code>diag(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor, **kwargs: dict[str, Any])\n</code></pre> <p>Compute a diagonal Hessian approximation to \\(H\\) and is represented as a vector of the dimensionality of parameters \\(\\theta\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> <li> <code>H</code> (              <code>Tensor</code> )          \u2013            <p>vector representing the diagonal of H</p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def diag(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    y: torch.Tensor,\n    **kwargs: dict[str, Any],\n):\n    \"\"\"Compute a diagonal Hessian approximation to \\\\(H\\\\) and is represented as a\n    vector of the dimensionality of parameters \\\\(\\\\theta\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)`\n    y : torch.Tensor\n        labels `(batch, label_shape)`\n\n    Returns\n    -------\n    loss : torch.Tensor\n    H : torch.Tensor\n        vector representing the diagonal of H\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackInterface.diag(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackInterface.diag(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013            <p>labels <code>(batch, label_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackInterface.jacobians","title":"jacobians","text":"<pre><code>jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_{\\theta} f(x;\\theta)\\) at current parameter \\(\\theta\\) using backpack's BatchGrad per output dimension. Note that BackPACK doesn't play well with torch.func, so this method has to be overridden.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, parameters, outputs)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/backpack.py</code> <pre><code>def jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_{\\\\theta} f(x;\\\\theta)\\\\) at current parameter \\\\(\\\\theta\\\\)\n    using backpack's BatchGrad per output dimension. Note that BackPACK doesn't play well\n    with torch.func, so this method has to be overridden.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)` on compatible device with model.\n    enable_backprop : bool, default = False\n        whether to enable backprop through the Js and f w.r.t. x\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, parameters, outputs)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n    if isinstance(x, MutableMapping):\n        raise ValueError(\"BackPACK backend does not support dict-like inputs!\")\n\n    model = extend(self.model)\n    to_stack = []\n    for i in range(model.output_size):\n        model.zero_grad()\n        out = model(x)\n        with backpack(BatchGrad()):\n            if model.output_size &gt; 1:\n                out[:, i].sum().backward(\n                    create_graph=enable_backprop, retain_graph=enable_backprop\n                )\n            else:\n                out.sum().backward(\n                    create_graph=enable_backprop, retain_graph=enable_backprop\n                )\n            to_cat = []\n            for param in model.parameters():\n                to_cat.append(param.grad_batch.reshape(x.shape[0], -1))\n                delattr(param, \"grad_batch\")\n            Jk = torch.cat(to_cat, dim=1)\n            if self.subnetwork_indices is not None:\n                Jk = Jk[:, self.subnetwork_indices]\n        to_stack.append(Jk)\n        if i == 0:\n            f = out\n\n    model.zero_grad()\n    CTX.remove_hooks()\n    _cleanup(model)\n    if model.output_size &gt; 1:\n        J = torch.stack(to_stack, dim=2).transpose(1, 2)\n    else:\n        J = Jk.unsqueeze(-1).transpose(1, 2)\n\n    return (J, f) if enable_backprop else (J.detach(), f.detach())\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackInterface.jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackInterface.jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>= False</code> )           \u2013            <p>whether to enable backprop through the Js and f w.r.t. x</p>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackInterface.gradients","title":"gradients","text":"<pre><code>gradients(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter \\(\\theta\\) using Backpack's BatchGrad. Note that BackPACK doesn't play well with torch.func, so this method has to be overridden.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Gs</code> (              <code>Tensor</code> )          \u2013            <p>gradients <code>(batch, parameters)</code></p> </li> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/curvature/backpack.py</code> <pre><code>def gradients(\n    self, x: torch.Tensor | MutableMapping[str, torch.Tensor | Any], y: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute gradients \\\\(\\\\nabla_\\\\theta \\\\ell(f(x;\\\\theta, y)\\\\) at current parameter\n    \\\\(\\\\theta\\\\) using Backpack's BatchGrad. Note that BackPACK doesn't play well\n    with torch.func, so this method has to be overridden.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)` on compatible device with model.\n    y : torch.Tensor\n\n    Returns\n    -------\n    Gs : torch.Tensor\n        gradients `(batch, parameters)`\n    loss : torch.Tensor\n    \"\"\"\n    f = self.model(x)\n    loss = self.lossfunc(f, y)\n    with backpack(BatchGrad()):\n        loss.backward()\n    Gs = torch.cat(\n        [p.grad_batch.data.flatten(start_dim=1) for p in self._model.parameters()],\n        dim=1,\n    )\n    if self.subnetwork_indices is not None:\n        Gs = Gs[:, self.subnetwork_indices]\n    return Gs, loss\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackInterface.gradients(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackInterface.gradients(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackGGN","title":"BackPackGGN","text":"<pre><code>BackPackGGN(model: Module, likelihood: Likelihood | str, last_layer: bool = False, subnetwork_indices: LongTensor | None = None, dict_key_x: str = 'input_ids', dict_key_y: str = 'labels', stochastic: bool = False)\n</code></pre> <p>               Bases: <code>BackPackInterface</code>, <code>GGNInterface</code></p> <p>Implementation of the <code>GGNInterface</code> using Backpack.</p> <p>Methods:</p> <ul> <li> <code>jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_{\\theta} f(x;\\theta)\\) at current parameter \\(\\theta\\)</p> </li> <li> <code>last_layer_jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\)</p> </li> <li> <code>gradients</code>             \u2013              <p>Compute gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter</p> </li> <li> <code>full</code>             \u2013              <p>Compute the full GGN \\(P \\times P\\) matrix as Hessian approximation</p> </li> </ul> Source code in <code>laplace/curvature/backpack.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    last_layer: bool = False,\n    subnetwork_indices: torch.LongTensor | None = None,\n    dict_key_x: str = \"input_ids\",\n    dict_key_y: str = \"labels\",\n    stochastic: bool = False,\n):\n    super().__init__(\n        model, likelihood, last_layer, subnetwork_indices, dict_key_x, dict_key_y\n    )\n    self.stochastic = stochastic\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackGGN.jacobians","title":"jacobians","text":"<pre><code>jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_{\\theta} f(x;\\theta)\\) at current parameter \\(\\theta\\) using backpack's BatchGrad per output dimension. Note that BackPACK doesn't play well with torch.func, so this method has to be overridden.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, parameters, outputs)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/backpack.py</code> <pre><code>def jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_{\\\\theta} f(x;\\\\theta)\\\\) at current parameter \\\\(\\\\theta\\\\)\n    using backpack's BatchGrad per output dimension. Note that BackPACK doesn't play well\n    with torch.func, so this method has to be overridden.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)` on compatible device with model.\n    enable_backprop : bool, default = False\n        whether to enable backprop through the Js and f w.r.t. x\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, parameters, outputs)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n    if isinstance(x, MutableMapping):\n        raise ValueError(\"BackPACK backend does not support dict-like inputs!\")\n\n    model = extend(self.model)\n    to_stack = []\n    for i in range(model.output_size):\n        model.zero_grad()\n        out = model(x)\n        with backpack(BatchGrad()):\n            if model.output_size &gt; 1:\n                out[:, i].sum().backward(\n                    create_graph=enable_backprop, retain_graph=enable_backprop\n                )\n            else:\n                out.sum().backward(\n                    create_graph=enable_backprop, retain_graph=enable_backprop\n                )\n            to_cat = []\n            for param in model.parameters():\n                to_cat.append(param.grad_batch.reshape(x.shape[0], -1))\n                delattr(param, \"grad_batch\")\n            Jk = torch.cat(to_cat, dim=1)\n            if self.subnetwork_indices is not None:\n                Jk = Jk[:, self.subnetwork_indices]\n        to_stack.append(Jk)\n        if i == 0:\n            f = out\n\n    model.zero_grad()\n    CTX.remove_hooks()\n    _cleanup(model)\n    if model.output_size &gt; 1:\n        J = torch.stack(to_stack, dim=2).transpose(1, 2)\n    else:\n        J = Jk.unsqueeze(-1).transpose(1, 2)\n\n    return (J, f) if enable_backprop else (J.detach(), f.detach())\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackGGN.jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackGGN.jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>= False</code> )           \u2013            <p>whether to enable backprop through the Js and f w.r.t. x</p>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackGGN.last_layer_jacobians","title":"last_layer_jacobians","text":"<pre><code>last_layer_jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\) only at current last-layer parameter \\(\\theta_{\\textrm{last}}\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, outputs, last-layer-parameters)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def last_layer_jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_{\\\\theta_\\\\textrm{last}} f(x;\\\\theta_\\\\textrm{last})\\\\)\n    only at current last-layer parameter \\\\(\\\\theta_{\\\\textrm{last}}\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n    enable_backprop : bool, default=False\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, outputs, last-layer-parameters)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n    f, phi = self.model.forward_with_features(x)\n    bsize = phi.shape[0]\n    output_size = int(f.numel() / bsize)\n\n    # calculate Jacobians using the feature vector 'phi'\n    p = next(self.model.parameters())\n    identity = (\n        torch.eye(output_size, device=p.device, dtype=p.dtype)\n        .unsqueeze(0)\n        .tile(bsize, 1, 1)\n    )\n    # Jacobians are batch x output x params\n    Js = torch.einsum(\"kp,kij-&gt;kijp\", phi, identity).reshape(bsize, output_size, -1)\n    if self.model.last_layer.bias is not None:\n        Js = torch.cat([Js, identity], dim=2)\n\n    return (Js, f) if enable_backprop else (Js.detach(), f.detach())\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackGGN.last_layer_jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackGGN.last_layer_jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackGGN.gradients","title":"gradients","text":"<pre><code>gradients(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter \\(\\theta\\) using Backpack's BatchGrad. Note that BackPACK doesn't play well with torch.func, so this method has to be overridden.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Gs</code> (              <code>Tensor</code> )          \u2013            <p>gradients <code>(batch, parameters)</code></p> </li> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/curvature/backpack.py</code> <pre><code>def gradients(\n    self, x: torch.Tensor | MutableMapping[str, torch.Tensor | Any], y: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute gradients \\\\(\\\\nabla_\\\\theta \\\\ell(f(x;\\\\theta, y)\\\\) at current parameter\n    \\\\(\\\\theta\\\\) using Backpack's BatchGrad. Note that BackPACK doesn't play well\n    with torch.func, so this method has to be overridden.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)` on compatible device with model.\n    y : torch.Tensor\n\n    Returns\n    -------\n    Gs : torch.Tensor\n        gradients `(batch, parameters)`\n    loss : torch.Tensor\n    \"\"\"\n    f = self.model(x)\n    loss = self.lossfunc(f, y)\n    with backpack(BatchGrad()):\n        loss.backward()\n    Gs = torch.cat(\n        [p.grad_batch.data.flatten(start_dim=1) for p in self._model.parameters()],\n        dim=1,\n    )\n    if self.subnetwork_indices is not None:\n        Gs = Gs[:, self.subnetwork_indices]\n    return Gs, loss\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackGGN.gradients(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackGGN.gradients(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackGGN.full","title":"full","text":"<pre><code>full(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor, **kwargs: dict[str, Any]) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute the full GGN \\(P \\times P\\) matrix as Hessian approximation \\(H_{ggn}\\) with respect to parameters \\(\\theta \\in \\mathbb{R}^P\\). For last-layer, reduced to \\(\\theta_{last}\\)</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> <li> <code>H</code> (              <code>Tensor</code> )          \u2013            <p>GGN <code>(parameters, parameters)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def full(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    y: torch.Tensor,\n    **kwargs: dict[str, Any],\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the full GGN \\\\(P \\\\times P\\\\) matrix as Hessian approximation\n    \\\\(H_{ggn}\\\\) with respect to parameters \\\\(\\\\theta \\\\in \\\\mathbb{R}^P\\\\).\n    For last-layer, reduced to \\\\(\\\\theta_{last}\\\\)\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)`\n    y : torch.Tensor\n        labels `(batch, label_shape)`\n\n    Returns\n    -------\n    loss : torch.Tensor\n    H : torch.Tensor\n        GGN `(parameters, parameters)`\n    \"\"\"\n    Js, f = self.last_layer_jacobians(x) if self.last_layer else self.jacobians(x)\n    H_lik = (\n        self._get_mc_functional_fisher(f)\n        if self.stochastic\n        else self._get_functional_hessian(f)\n    )\n\n    if H_lik is not None:\n        H = torch.einsum(\"bcp,bck,bkq-&gt;pq\", Js, H_lik, Js)\n    else:  # The case of exact GGN for regression\n        H = torch.einsum(\"bcp,bcq-&gt;pq\", Js, Js)\n    loss = self.factor * self.lossfunc(f, y)\n\n    return loss.detach(), H.detach()\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackGGN.full(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackGGN.full(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013            <p>labels <code>(batch, label_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackGGN._get_mc_functional_fisher","title":"_get_mc_functional_fisher","text":"<pre><code>_get_mc_functional_fisher(f: Tensor) -&gt; Tensor\n</code></pre> <p>Approximate the Fisher's middle matrix (expected outer product of the functional gradient) using MC integral with <code>self.num_samples</code> many samples.</p> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def _get_mc_functional_fisher(self, f: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Approximate the Fisher's middle matrix (expected outer product of the functional gradient)\n    using MC integral with `self.num_samples` many samples.\n    \"\"\"\n    F = 0\n\n    for _ in range(self.num_samples):\n        if self.likelihood == \"regression\":\n            # N(y | f, 1)\n            y_sample = f + torch.randn(f.shape, device=f.device, dtype=f.dtype)\n            grad_sample = f - y_sample  # functional MSE grad\n        else:  # classification with softmax\n            y_sample = torch.distributions.Multinomial(logits=f).sample()\n            # First functional derivative of the loglik is p - y\n            p = torch.softmax(f, dim=-1)\n            grad_sample = p - y_sample\n\n        F += (\n            1\n            / self.num_samples\n            * torch.einsum(\"bc,bk-&gt;bck\", grad_sample, grad_sample)\n        )\n\n    return F\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackEF","title":"BackPackEF","text":"<pre><code>BackPackEF(model: Module, likelihood: Likelihood | str, last_layer: bool = False, subnetwork_indices: LongTensor | None = None, dict_key_x: str = 'input_ids', dict_key_y: str = 'labels')\n</code></pre> <p>               Bases: <code>BackPackInterface</code>, <code>EFInterface</code></p> <p>Implementation of <code>EFInterface</code> using Backpack.</p> <p>Methods:</p> <ul> <li> <code>jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_{\\theta} f(x;\\theta)\\) at current parameter \\(\\theta\\)</p> </li> <li> <code>last_layer_jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\)</p> </li> <li> <code>gradients</code>             \u2013              <p>Compute gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter</p> </li> <li> <code>full</code>             \u2013              <p>Compute the full EF \\(P \\times P\\) matrix as Hessian approximation</p> </li> </ul> Source code in <code>laplace/curvature/backpack.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    last_layer: bool = False,\n    subnetwork_indices: torch.LongTensor | None = None,\n    dict_key_x: str = \"input_ids\",\n    dict_key_y: str = \"labels\",\n) -&gt; None:\n    super().__init__(\n        model, likelihood, last_layer, subnetwork_indices, dict_key_x, dict_key_y\n    )\n\n    extend(self._model)\n    extend(self.lossfunc)\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackEF.jacobians","title":"jacobians","text":"<pre><code>jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_{\\theta} f(x;\\theta)\\) at current parameter \\(\\theta\\) using backpack's BatchGrad per output dimension. Note that BackPACK doesn't play well with torch.func, so this method has to be overridden.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, parameters, outputs)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/backpack.py</code> <pre><code>def jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_{\\\\theta} f(x;\\\\theta)\\\\) at current parameter \\\\(\\\\theta\\\\)\n    using backpack's BatchGrad per output dimension. Note that BackPACK doesn't play well\n    with torch.func, so this method has to be overridden.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)` on compatible device with model.\n    enable_backprop : bool, default = False\n        whether to enable backprop through the Js and f w.r.t. x\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, parameters, outputs)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n    if isinstance(x, MutableMapping):\n        raise ValueError(\"BackPACK backend does not support dict-like inputs!\")\n\n    model = extend(self.model)\n    to_stack = []\n    for i in range(model.output_size):\n        model.zero_grad()\n        out = model(x)\n        with backpack(BatchGrad()):\n            if model.output_size &gt; 1:\n                out[:, i].sum().backward(\n                    create_graph=enable_backprop, retain_graph=enable_backprop\n                )\n            else:\n                out.sum().backward(\n                    create_graph=enable_backprop, retain_graph=enable_backprop\n                )\n            to_cat = []\n            for param in model.parameters():\n                to_cat.append(param.grad_batch.reshape(x.shape[0], -1))\n                delattr(param, \"grad_batch\")\n            Jk = torch.cat(to_cat, dim=1)\n            if self.subnetwork_indices is not None:\n                Jk = Jk[:, self.subnetwork_indices]\n        to_stack.append(Jk)\n        if i == 0:\n            f = out\n\n    model.zero_grad()\n    CTX.remove_hooks()\n    _cleanup(model)\n    if model.output_size &gt; 1:\n        J = torch.stack(to_stack, dim=2).transpose(1, 2)\n    else:\n        J = Jk.unsqueeze(-1).transpose(1, 2)\n\n    return (J, f) if enable_backprop else (J.detach(), f.detach())\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackEF.jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackEF.jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>= False</code> )           \u2013            <p>whether to enable backprop through the Js and f w.r.t. x</p>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackEF.last_layer_jacobians","title":"last_layer_jacobians","text":"<pre><code>last_layer_jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\) only at current last-layer parameter \\(\\theta_{\\textrm{last}}\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, outputs, last-layer-parameters)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def last_layer_jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_{\\\\theta_\\\\textrm{last}} f(x;\\\\theta_\\\\textrm{last})\\\\)\n    only at current last-layer parameter \\\\(\\\\theta_{\\\\textrm{last}}\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n    enable_backprop : bool, default=False\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, outputs, last-layer-parameters)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n    f, phi = self.model.forward_with_features(x)\n    bsize = phi.shape[0]\n    output_size = int(f.numel() / bsize)\n\n    # calculate Jacobians using the feature vector 'phi'\n    p = next(self.model.parameters())\n    identity = (\n        torch.eye(output_size, device=p.device, dtype=p.dtype)\n        .unsqueeze(0)\n        .tile(bsize, 1, 1)\n    )\n    # Jacobians are batch x output x params\n    Js = torch.einsum(\"kp,kij-&gt;kijp\", phi, identity).reshape(bsize, output_size, -1)\n    if self.model.last_layer.bias is not None:\n        Js = torch.cat([Js, identity], dim=2)\n\n    return (Js, f) if enable_backprop else (Js.detach(), f.detach())\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackEF.last_layer_jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackEF.last_layer_jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackEF.gradients","title":"gradients","text":"<pre><code>gradients(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter \\(\\theta\\) using Backpack's BatchGrad. Note that BackPACK doesn't play well with torch.func, so this method has to be overridden.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Gs</code> (              <code>Tensor</code> )          \u2013            <p>gradients <code>(batch, parameters)</code></p> </li> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/curvature/backpack.py</code> <pre><code>def gradients(\n    self, x: torch.Tensor | MutableMapping[str, torch.Tensor | Any], y: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute gradients \\\\(\\\\nabla_\\\\theta \\\\ell(f(x;\\\\theta, y)\\\\) at current parameter\n    \\\\(\\\\theta\\\\) using Backpack's BatchGrad. Note that BackPACK doesn't play well\n    with torch.func, so this method has to be overridden.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)` on compatible device with model.\n    y : torch.Tensor\n\n    Returns\n    -------\n    Gs : torch.Tensor\n        gradients `(batch, parameters)`\n    loss : torch.Tensor\n    \"\"\"\n    f = self.model(x)\n    loss = self.lossfunc(f, y)\n    with backpack(BatchGrad()):\n        loss.backward()\n    Gs = torch.cat(\n        [p.grad_batch.data.flatten(start_dim=1) for p in self._model.parameters()],\n        dim=1,\n    )\n    if self.subnetwork_indices is not None:\n        Gs = Gs[:, self.subnetwork_indices]\n    return Gs, loss\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackEF.gradients(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackEF.gradients(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackEF.full","title":"full","text":"<pre><code>full(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor, **kwargs: dict[str, Any]) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute the full EF \\(P \\times P\\) matrix as Hessian approximation \\(H_{ef}\\) with respect to parameters \\(\\theta \\in \\mathbb{R}^P\\). For last-layer, reduced to \\(\\theta_{last}\\)</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> <li> <code>H_ef</code> (              <code>Tensor</code> )          \u2013            <p>EF <code>(parameters, parameters)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def full(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    y: torch.Tensor,\n    **kwargs: dict[str, Any],\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the full EF \\\\(P \\\\times P\\\\) matrix as Hessian approximation\n    \\\\(H_{ef}\\\\) with respect to parameters \\\\(\\\\theta \\\\in \\\\mathbb{R}^P\\\\).\n    For last-layer, reduced to \\\\(\\\\theta_{last}\\\\)\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)`\n    y : torch.Tensor\n        labels `(batch, label_shape)`\n\n    Returns\n    -------\n    loss : torch.Tensor\n    H_ef : torch.Tensor\n        EF `(parameters, parameters)`\n    \"\"\"\n    Gs, loss = self.gradients(x, y)\n    Gs, loss = Gs.detach(), loss.detach()\n    H_ef = torch.einsum(\"bp,bq-&gt;pq\", Gs, Gs)\n    return self.factor * loss.detach(), self.factor * H_ef\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackEF.full(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.BackPackEF.full(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013            <p>labels <code>(batch, label_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsInterface","title":"CurvlinopsInterface","text":"<pre><code>CurvlinopsInterface(model: Module, likelihood: Likelihood | str, last_layer: bool = False, subnetwork_indices: LongTensor | None = None, dict_key_x: str = 'input_ids', dict_key_y: str = 'labels')\n</code></pre> <p>               Bases: <code>CurvatureInterface</code></p> <p>Interface for Curvlinops backend. https://github.com/f-dangel/curvlinops</p> <p>Methods:</p> <ul> <li> <code>jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_{\\theta} f(x;\\theta)\\) at current parameter \\(\\theta\\),</p> </li> <li> <code>last_layer_jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\)</p> </li> <li> <code>gradients</code>             \u2013              <p>Compute batch gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at</p> </li> <li> <code>diag</code>             \u2013              <p>Compute a diagonal Hessian approximation to \\(H\\) and is represented as a</p> </li> </ul> Source code in <code>laplace/curvature/curvlinops.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    last_layer: bool = False,\n    subnetwork_indices: torch.LongTensor | None = None,\n    dict_key_x: str = \"input_ids\",\n    dict_key_y: str = \"labels\",\n) -&gt; None:\n    super().__init__(\n        model, likelihood, last_layer, subnetwork_indices, dict_key_x, dict_key_y\n    )\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsInterface.jacobians","title":"jacobians","text":"<pre><code>jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_{\\theta} f(x;\\theta)\\) at current parameter \\(\\theta\\), via torch.func.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, parameters, outputs)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_{\\\\theta} f(x;\\\\theta)\\\\) at current parameter \\\\(\\\\theta\\\\),\n    via torch.func.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)` on compatible device with model.\n    enable_backprop : bool, default = False\n        whether to enable backprop through the Js and f w.r.t. x\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, parameters, outputs)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n\n    def model_fn_params_only(params_dict, buffers_dict):\n        out = torch.func.functional_call(self.model, (params_dict, buffers_dict), x)\n        return out, out\n\n    Js, f = torch.func.jacrev(model_fn_params_only, has_aux=True)(\n        self.params_dict, self.buffers_dict\n    )\n\n    # Concatenate over flattened parameters\n    Js = [\n        j.flatten(start_dim=-p.dim())\n        for j, p in zip(Js.values(), self.params_dict.values())\n    ]\n    Js = torch.cat(Js, dim=-1)\n\n    if self.subnetwork_indices is not None:\n        Js = Js[:, :, self.subnetwork_indices]\n\n    return (Js, f) if enable_backprop else (Js.detach(), f.detach())\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsInterface.jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsInterface.jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>= False</code> )           \u2013            <p>whether to enable backprop through the Js and f w.r.t. x</p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsInterface.last_layer_jacobians","title":"last_layer_jacobians","text":"<pre><code>last_layer_jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\) only at current last-layer parameter \\(\\theta_{\\textrm{last}}\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, outputs, last-layer-parameters)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def last_layer_jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_{\\\\theta_\\\\textrm{last}} f(x;\\\\theta_\\\\textrm{last})\\\\)\n    only at current last-layer parameter \\\\(\\\\theta_{\\\\textrm{last}}\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n    enable_backprop : bool, default=False\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, outputs, last-layer-parameters)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n    f, phi = self.model.forward_with_features(x)\n    bsize = phi.shape[0]\n    output_size = int(f.numel() / bsize)\n\n    # calculate Jacobians using the feature vector 'phi'\n    p = next(self.model.parameters())\n    identity = (\n        torch.eye(output_size, device=p.device, dtype=p.dtype)\n        .unsqueeze(0)\n        .tile(bsize, 1, 1)\n    )\n    # Jacobians are batch x output x params\n    Js = torch.einsum(\"kp,kij-&gt;kijp\", phi, identity).reshape(bsize, output_size, -1)\n    if self.model.last_layer.bias is not None:\n        Js = torch.cat([Js, identity], dim=2)\n\n    return (Js, f) if enable_backprop else (Js.detach(), f.detach())\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsInterface.last_layer_jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsInterface.last_layer_jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsInterface.gradients","title":"gradients","text":"<pre><code>gradients(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute batch gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter \\(\\theta\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Gs</code> (              <code>Tensor</code> )          \u2013            <p>gradients <code>(batch, parameters)</code></p> </li> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def gradients(\n    self, x: torch.Tensor | MutableMapping[str, torch.Tensor | Any], y: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute batch gradients \\\\(\\\\nabla_\\\\theta \\\\ell(f(x;\\\\theta, y)\\\\) at\n    current parameter \\\\(\\\\theta\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)` on compatible device with model.\n    y : torch.Tensor\n\n    Returns\n    -------\n    Gs : torch.Tensor\n        gradients `(batch, parameters)`\n    loss : torch.Tensor\n    \"\"\"\n\n    def loss_single(x, y, params_dict, buffers_dict):\n        \"\"\"Compute the gradient for a single sample.\"\"\"\n        x, y = x.unsqueeze(0), y.unsqueeze(0)  # vmap removes the batch dimension\n        output = torch.func.functional_call(\n            self.model, (params_dict, buffers_dict), x\n        )\n        loss = torch.func.functional_call(self.lossfunc, {}, (output, y))\n        return loss, loss\n\n    grad_fn = torch.func.grad(loss_single, argnums=2, has_aux=True)\n    batch_grad_fn = torch.func.vmap(grad_fn, in_dims=(0, 0, None, None))\n\n    batch_grad, batch_loss = batch_grad_fn(\n        x, y, self.params_dict, self.buffers_dict\n    )\n    Gs = torch.cat([bg.flatten(start_dim=1) for bg in batch_grad.values()], dim=1)\n\n    if self.subnetwork_indices is not None:\n        Gs = Gs[:, self.subnetwork_indices]\n\n    loss = batch_loss.sum(0)\n\n    return Gs, loss\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsInterface.gradients(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsInterface.gradients(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsInterface.diag","title":"diag","text":"<pre><code>diag(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor, **kwargs: dict[str, Any])\n</code></pre> <p>Compute a diagonal Hessian approximation to \\(H\\) and is represented as a vector of the dimensionality of parameters \\(\\theta\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> <li> <code>H</code> (              <code>Tensor</code> )          \u2013            <p>vector representing the diagonal of H</p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def diag(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    y: torch.Tensor,\n    **kwargs: dict[str, Any],\n):\n    \"\"\"Compute a diagonal Hessian approximation to \\\\(H\\\\) and is represented as a\n    vector of the dimensionality of parameters \\\\(\\\\theta\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)`\n    y : torch.Tensor\n        labels `(batch, label_shape)`\n\n    Returns\n    -------\n    loss : torch.Tensor\n    H : torch.Tensor\n        vector representing the diagonal of H\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsInterface.diag(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsInterface.diag(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013            <p>labels <code>(batch, label_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsGGN","title":"CurvlinopsGGN","text":"<pre><code>CurvlinopsGGN(model: Module, likelihood: Likelihood | str, last_layer: bool = False, subnetwork_indices: LongTensor | None = None, dict_key_x: str = 'input_ids', dict_key_y: str = 'labels', stochastic: bool = False)\n</code></pre> <p>               Bases: <code>CurvlinopsInterface</code>, <code>GGNInterface</code></p> <p>Implementation of the <code>GGNInterface</code> using Curvlinops.</p> <p>Methods:</p> <ul> <li> <code>jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_{\\theta} f(x;\\theta)\\) at current parameter \\(\\theta\\),</p> </li> <li> <code>last_layer_jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\)</p> </li> <li> <code>gradients</code>             \u2013              <p>Compute batch gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at</p> </li> </ul> Source code in <code>laplace/curvature/curvlinops.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    last_layer: bool = False,\n    subnetwork_indices: torch.LongTensor | None = None,\n    dict_key_x: str = \"input_ids\",\n    dict_key_y: str = \"labels\",\n    stochastic: bool = False,\n) -&gt; None:\n    super().__init__(\n        model, likelihood, last_layer, subnetwork_indices, dict_key_x, dict_key_y\n    )\n    self.stochastic = stochastic\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsGGN.jacobians","title":"jacobians","text":"<pre><code>jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_{\\theta} f(x;\\theta)\\) at current parameter \\(\\theta\\), via torch.func.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, parameters, outputs)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_{\\\\theta} f(x;\\\\theta)\\\\) at current parameter \\\\(\\\\theta\\\\),\n    via torch.func.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)` on compatible device with model.\n    enable_backprop : bool, default = False\n        whether to enable backprop through the Js and f w.r.t. x\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, parameters, outputs)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n\n    def model_fn_params_only(params_dict, buffers_dict):\n        out = torch.func.functional_call(self.model, (params_dict, buffers_dict), x)\n        return out, out\n\n    Js, f = torch.func.jacrev(model_fn_params_only, has_aux=True)(\n        self.params_dict, self.buffers_dict\n    )\n\n    # Concatenate over flattened parameters\n    Js = [\n        j.flatten(start_dim=-p.dim())\n        for j, p in zip(Js.values(), self.params_dict.values())\n    ]\n    Js = torch.cat(Js, dim=-1)\n\n    if self.subnetwork_indices is not None:\n        Js = Js[:, :, self.subnetwork_indices]\n\n    return (Js, f) if enable_backprop else (Js.detach(), f.detach())\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsGGN.jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsGGN.jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>= False</code> )           \u2013            <p>whether to enable backprop through the Js and f w.r.t. x</p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsGGN.last_layer_jacobians","title":"last_layer_jacobians","text":"<pre><code>last_layer_jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\) only at current last-layer parameter \\(\\theta_{\\textrm{last}}\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, outputs, last-layer-parameters)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def last_layer_jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_{\\\\theta_\\\\textrm{last}} f(x;\\\\theta_\\\\textrm{last})\\\\)\n    only at current last-layer parameter \\\\(\\\\theta_{\\\\textrm{last}}\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n    enable_backprop : bool, default=False\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, outputs, last-layer-parameters)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n    f, phi = self.model.forward_with_features(x)\n    bsize = phi.shape[0]\n    output_size = int(f.numel() / bsize)\n\n    # calculate Jacobians using the feature vector 'phi'\n    p = next(self.model.parameters())\n    identity = (\n        torch.eye(output_size, device=p.device, dtype=p.dtype)\n        .unsqueeze(0)\n        .tile(bsize, 1, 1)\n    )\n    # Jacobians are batch x output x params\n    Js = torch.einsum(\"kp,kij-&gt;kijp\", phi, identity).reshape(bsize, output_size, -1)\n    if self.model.last_layer.bias is not None:\n        Js = torch.cat([Js, identity], dim=2)\n\n    return (Js, f) if enable_backprop else (Js.detach(), f.detach())\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsGGN.last_layer_jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsGGN.last_layer_jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsGGN.gradients","title":"gradients","text":"<pre><code>gradients(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute batch gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter \\(\\theta\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Gs</code> (              <code>Tensor</code> )          \u2013            <p>gradients <code>(batch, parameters)</code></p> </li> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def gradients(\n    self, x: torch.Tensor | MutableMapping[str, torch.Tensor | Any], y: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute batch gradients \\\\(\\\\nabla_\\\\theta \\\\ell(f(x;\\\\theta, y)\\\\) at\n    current parameter \\\\(\\\\theta\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)` on compatible device with model.\n    y : torch.Tensor\n\n    Returns\n    -------\n    Gs : torch.Tensor\n        gradients `(batch, parameters)`\n    loss : torch.Tensor\n    \"\"\"\n\n    def loss_single(x, y, params_dict, buffers_dict):\n        \"\"\"Compute the gradient for a single sample.\"\"\"\n        x, y = x.unsqueeze(0), y.unsqueeze(0)  # vmap removes the batch dimension\n        output = torch.func.functional_call(\n            self.model, (params_dict, buffers_dict), x\n        )\n        loss = torch.func.functional_call(self.lossfunc, {}, (output, y))\n        return loss, loss\n\n    grad_fn = torch.func.grad(loss_single, argnums=2, has_aux=True)\n    batch_grad_fn = torch.func.vmap(grad_fn, in_dims=(0, 0, None, None))\n\n    batch_grad, batch_loss = batch_grad_fn(\n        x, y, self.params_dict, self.buffers_dict\n    )\n    Gs = torch.cat([bg.flatten(start_dim=1) for bg in batch_grad.values()], dim=1)\n\n    if self.subnetwork_indices is not None:\n        Gs = Gs[:, self.subnetwork_indices]\n\n    loss = batch_loss.sum(0)\n\n    return Gs, loss\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsGGN.gradients(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsGGN.gradients(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsGGN._get_mc_functional_fisher","title":"_get_mc_functional_fisher","text":"<pre><code>_get_mc_functional_fisher(f: Tensor) -&gt; Tensor\n</code></pre> <p>Approximate the Fisher's middle matrix (expected outer product of the functional gradient) using MC integral with <code>self.num_samples</code> many samples.</p> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def _get_mc_functional_fisher(self, f: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Approximate the Fisher's middle matrix (expected outer product of the functional gradient)\n    using MC integral with `self.num_samples` many samples.\n    \"\"\"\n    F = 0\n\n    for _ in range(self.num_samples):\n        if self.likelihood == \"regression\":\n            # N(y | f, 1)\n            y_sample = f + torch.randn(f.shape, device=f.device, dtype=f.dtype)\n            grad_sample = f - y_sample  # functional MSE grad\n        else:  # classification with softmax\n            y_sample = torch.distributions.Multinomial(logits=f).sample()\n            # First functional derivative of the loglik is p - y\n            p = torch.softmax(f, dim=-1)\n            grad_sample = p - y_sample\n\n        F += (\n            1\n            / self.num_samples\n            * torch.einsum(\"bc,bk-&gt;bck\", grad_sample, grad_sample)\n        )\n\n    return F\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsEF","title":"CurvlinopsEF","text":"<pre><code>CurvlinopsEF(model: Module, likelihood: Likelihood | str, last_layer: bool = False, subnetwork_indices: LongTensor | None = None, dict_key_x: str = 'input_ids', dict_key_y: str = 'labels')\n</code></pre> <p>               Bases: <code>CurvlinopsInterface</code>, <code>EFInterface</code></p> <p>Implementation of <code>EFInterface</code> using Curvlinops.</p> <p>Methods:</p> <ul> <li> <code>jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_{\\theta} f(x;\\theta)\\) at current parameter \\(\\theta\\),</p> </li> <li> <code>last_layer_jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\)</p> </li> <li> <code>gradients</code>             \u2013              <p>Compute batch gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at</p> </li> </ul> Source code in <code>laplace/curvature/curvlinops.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    last_layer: bool = False,\n    subnetwork_indices: torch.LongTensor | None = None,\n    dict_key_x: str = \"input_ids\",\n    dict_key_y: str = \"labels\",\n) -&gt; None:\n    super().__init__(\n        model, likelihood, last_layer, subnetwork_indices, dict_key_x, dict_key_y\n    )\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsEF.jacobians","title":"jacobians","text":"<pre><code>jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_{\\theta} f(x;\\theta)\\) at current parameter \\(\\theta\\), via torch.func.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, parameters, outputs)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_{\\\\theta} f(x;\\\\theta)\\\\) at current parameter \\\\(\\\\theta\\\\),\n    via torch.func.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)` on compatible device with model.\n    enable_backprop : bool, default = False\n        whether to enable backprop through the Js and f w.r.t. x\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, parameters, outputs)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n\n    def model_fn_params_only(params_dict, buffers_dict):\n        out = torch.func.functional_call(self.model, (params_dict, buffers_dict), x)\n        return out, out\n\n    Js, f = torch.func.jacrev(model_fn_params_only, has_aux=True)(\n        self.params_dict, self.buffers_dict\n    )\n\n    # Concatenate over flattened parameters\n    Js = [\n        j.flatten(start_dim=-p.dim())\n        for j, p in zip(Js.values(), self.params_dict.values())\n    ]\n    Js = torch.cat(Js, dim=-1)\n\n    if self.subnetwork_indices is not None:\n        Js = Js[:, :, self.subnetwork_indices]\n\n    return (Js, f) if enable_backprop else (Js.detach(), f.detach())\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsEF.jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsEF.jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>= False</code> )           \u2013            <p>whether to enable backprop through the Js and f w.r.t. x</p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsEF.last_layer_jacobians","title":"last_layer_jacobians","text":"<pre><code>last_layer_jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\) only at current last-layer parameter \\(\\theta_{\\textrm{last}}\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, outputs, last-layer-parameters)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def last_layer_jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_{\\\\theta_\\\\textrm{last}} f(x;\\\\theta_\\\\textrm{last})\\\\)\n    only at current last-layer parameter \\\\(\\\\theta_{\\\\textrm{last}}\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n    enable_backprop : bool, default=False\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, outputs, last-layer-parameters)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n    f, phi = self.model.forward_with_features(x)\n    bsize = phi.shape[0]\n    output_size = int(f.numel() / bsize)\n\n    # calculate Jacobians using the feature vector 'phi'\n    p = next(self.model.parameters())\n    identity = (\n        torch.eye(output_size, device=p.device, dtype=p.dtype)\n        .unsqueeze(0)\n        .tile(bsize, 1, 1)\n    )\n    # Jacobians are batch x output x params\n    Js = torch.einsum(\"kp,kij-&gt;kijp\", phi, identity).reshape(bsize, output_size, -1)\n    if self.model.last_layer.bias is not None:\n        Js = torch.cat([Js, identity], dim=2)\n\n    return (Js, f) if enable_backprop else (Js.detach(), f.detach())\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsEF.last_layer_jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsEF.last_layer_jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsEF.gradients","title":"gradients","text":"<pre><code>gradients(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute batch gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter \\(\\theta\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Gs</code> (              <code>Tensor</code> )          \u2013            <p>gradients <code>(batch, parameters)</code></p> </li> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def gradients(\n    self, x: torch.Tensor | MutableMapping[str, torch.Tensor | Any], y: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute batch gradients \\\\(\\\\nabla_\\\\theta \\\\ell(f(x;\\\\theta, y)\\\\) at\n    current parameter \\\\(\\\\theta\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)` on compatible device with model.\n    y : torch.Tensor\n\n    Returns\n    -------\n    Gs : torch.Tensor\n        gradients `(batch, parameters)`\n    loss : torch.Tensor\n    \"\"\"\n\n    def loss_single(x, y, params_dict, buffers_dict):\n        \"\"\"Compute the gradient for a single sample.\"\"\"\n        x, y = x.unsqueeze(0), y.unsqueeze(0)  # vmap removes the batch dimension\n        output = torch.func.functional_call(\n            self.model, (params_dict, buffers_dict), x\n        )\n        loss = torch.func.functional_call(self.lossfunc, {}, (output, y))\n        return loss, loss\n\n    grad_fn = torch.func.grad(loss_single, argnums=2, has_aux=True)\n    batch_grad_fn = torch.func.vmap(grad_fn, in_dims=(0, 0, None, None))\n\n    batch_grad, batch_loss = batch_grad_fn(\n        x, y, self.params_dict, self.buffers_dict\n    )\n    Gs = torch.cat([bg.flatten(start_dim=1) for bg in batch_grad.values()], dim=1)\n\n    if self.subnetwork_indices is not None:\n        Gs = Gs[:, self.subnetwork_indices]\n\n    loss = batch_loss.sum(0)\n\n    return Gs, loss\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsEF.gradients(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsEF.gradients(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsHessian","title":"CurvlinopsHessian","text":"<pre><code>CurvlinopsHessian(model: Module, likelihood: Likelihood | str, last_layer: bool = False, subnetwork_indices: LongTensor | None = None, dict_key_x: str = 'input_ids', dict_key_y: str = 'labels')\n</code></pre> <p>               Bases: <code>CurvlinopsInterface</code></p> <p>Implementation of the full Hessian using Curvlinops.</p> <p>Methods:</p> <ul> <li> <code>jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_{\\theta} f(x;\\theta)\\) at current parameter \\(\\theta\\),</p> </li> <li> <code>last_layer_jacobians</code>             \u2013              <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\)</p> </li> <li> <code>gradients</code>             \u2013              <p>Compute batch gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at</p> </li> <li> <code>diag</code>             \u2013              <p>Compute a diagonal Hessian approximation to \\(H\\) and is represented as a</p> </li> </ul> Source code in <code>laplace/curvature/curvlinops.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    last_layer: bool = False,\n    subnetwork_indices: torch.LongTensor | None = None,\n    dict_key_x: str = \"input_ids\",\n    dict_key_y: str = \"labels\",\n) -&gt; None:\n    super().__init__(\n        model, likelihood, last_layer, subnetwork_indices, dict_key_x, dict_key_y\n    )\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsHessian.jacobians","title":"jacobians","text":"<pre><code>jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_{\\theta} f(x;\\theta)\\) at current parameter \\(\\theta\\), via torch.func.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, parameters, outputs)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_{\\\\theta} f(x;\\\\theta)\\\\) at current parameter \\\\(\\\\theta\\\\),\n    via torch.func.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)` on compatible device with model.\n    enable_backprop : bool, default = False\n        whether to enable backprop through the Js and f w.r.t. x\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, parameters, outputs)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n\n    def model_fn_params_only(params_dict, buffers_dict):\n        out = torch.func.functional_call(self.model, (params_dict, buffers_dict), x)\n        return out, out\n\n    Js, f = torch.func.jacrev(model_fn_params_only, has_aux=True)(\n        self.params_dict, self.buffers_dict\n    )\n\n    # Concatenate over flattened parameters\n    Js = [\n        j.flatten(start_dim=-p.dim())\n        for j, p in zip(Js.values(), self.params_dict.values())\n    ]\n    Js = torch.cat(Js, dim=-1)\n\n    if self.subnetwork_indices is not None:\n        Js = Js[:, :, self.subnetwork_indices]\n\n    return (Js, f) if enable_backprop else (Js.detach(), f.detach())\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsHessian.jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsHessian.jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>= False</code> )           \u2013            <p>whether to enable backprop through the Js and f w.r.t. x</p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsHessian.last_layer_jacobians","title":"last_layer_jacobians","text":"<pre><code>last_layer_jacobians(x: Tensor | MutableMapping[str, Tensor | Any], enable_backprop: bool = False) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute Jacobians \\(\\nabla_{\\theta_\\textrm{last}} f(x;\\theta_\\textrm{last})\\) only at current last-layer parameter \\(\\theta_{\\textrm{last}}\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Js</code> (              <code>Tensor</code> )          \u2013            <p>Jacobians <code>(batch, outputs, last-layer-parameters)</code></p> </li> <li> <code>f</code> (              <code>Tensor</code> )          \u2013            <p>output function <code>(batch, outputs)</code></p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def last_layer_jacobians(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    enable_backprop: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute Jacobians \\\\(\\\\nabla_{\\\\theta_\\\\textrm{last}} f(x;\\\\theta_\\\\textrm{last})\\\\)\n    only at current last-layer parameter \\\\(\\\\theta_{\\\\textrm{last}}\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n    enable_backprop : bool, default=False\n\n    Returns\n    -------\n    Js : torch.Tensor\n        Jacobians `(batch, outputs, last-layer-parameters)`\n    f : torch.Tensor\n        output function `(batch, outputs)`\n    \"\"\"\n    f, phi = self.model.forward_with_features(x)\n    bsize = phi.shape[0]\n    output_size = int(f.numel() / bsize)\n\n    # calculate Jacobians using the feature vector 'phi'\n    p = next(self.model.parameters())\n    identity = (\n        torch.eye(output_size, device=p.device, dtype=p.dtype)\n        .unsqueeze(0)\n        .tile(bsize, 1, 1)\n    )\n    # Jacobians are batch x output x params\n    Js = torch.einsum(\"kp,kij-&gt;kijp\", phi, identity).reshape(bsize, output_size, -1)\n    if self.model.last_layer.bias is not None:\n        Js = torch.cat([Js, identity], dim=2)\n\n    return (Js, f) if enable_backprop else (Js.detach(), f.detach())\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsHessian.last_layer_jacobians(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsHessian.last_layer_jacobians(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsHessian.gradients","title":"gradients","text":"<pre><code>gradients(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Compute batch gradients \\(\\nabla_\\theta \\ell(f(x;\\theta, y)\\) at current parameter \\(\\theta\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Gs</code> (              <code>Tensor</code> )          \u2013            <p>gradients <code>(batch, parameters)</code></p> </li> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def gradients(\n    self, x: torch.Tensor | MutableMapping[str, torch.Tensor | Any], y: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute batch gradients \\\\(\\\\nabla_\\\\theta \\\\ell(f(x;\\\\theta, y)\\\\) at\n    current parameter \\\\(\\\\theta\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)` on compatible device with model.\n    y : torch.Tensor\n\n    Returns\n    -------\n    Gs : torch.Tensor\n        gradients `(batch, parameters)`\n    loss : torch.Tensor\n    \"\"\"\n\n    def loss_single(x, y, params_dict, buffers_dict):\n        \"\"\"Compute the gradient for a single sample.\"\"\"\n        x, y = x.unsqueeze(0), y.unsqueeze(0)  # vmap removes the batch dimension\n        output = torch.func.functional_call(\n            self.model, (params_dict, buffers_dict), x\n        )\n        loss = torch.func.functional_call(self.lossfunc, {}, (output, y))\n        return loss, loss\n\n    grad_fn = torch.func.grad(loss_single, argnums=2, has_aux=True)\n    batch_grad_fn = torch.func.vmap(grad_fn, in_dims=(0, 0, None, None))\n\n    batch_grad, batch_loss = batch_grad_fn(\n        x, y, self.params_dict, self.buffers_dict\n    )\n    Gs = torch.cat([bg.flatten(start_dim=1) for bg in batch_grad.values()], dim=1)\n\n    if self.subnetwork_indices is not None:\n        Gs = Gs[:, self.subnetwork_indices]\n\n    loss = batch_loss.sum(0)\n\n    return Gs, loss\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsHessian.gradients(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code> on compatible device with model.</p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsHessian.gradients(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsHessian.diag","title":"diag","text":"<pre><code>diag(x: Tensor | MutableMapping[str, Tensor | Any], y: Tensor, **kwargs: dict[str, Any])\n</code></pre> <p>Compute a diagonal Hessian approximation to \\(H\\) and is represented as a vector of the dimensionality of parameters \\(\\theta\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>Tensor</code> )          \u2013            </li> <li> <code>H</code> (              <code>Tensor</code> )          \u2013            <p>vector representing the diagonal of H</p> </li> </ul> Source code in <code>laplace/curvature/curvature.py</code> <pre><code>def diag(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    y: torch.Tensor,\n    **kwargs: dict[str, Any],\n):\n    \"\"\"Compute a diagonal Hessian approximation to \\\\(H\\\\) and is represented as a\n    vector of the dimensionality of parameters \\\\(\\\\theta\\\\).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input data `(batch, input_shape)`\n    y : torch.Tensor\n        labels `(batch, label_shape)`\n\n    Returns\n    -------\n    loss : torch.Tensor\n    H : torch.Tensor\n        vector representing the diagonal of H\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsHessian.diag(x)","title":"<code>x</code>","text":"(<code>Tensor</code>)           \u2013            <p>input data <code>(batch, input_shape)</code></p>"},{"location":"api_reference/curvatures/#laplace.curvature.CurvlinopsHessian.diag(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013            <p>labels <code>(batch, label_shape)</code></p>"},{"location":"api_reference/enums/","title":"Laplace Options","text":""},{"location":"api_reference/enums/#laplace.utils.enums","title":"laplace.utils.enums","text":"<p>Classes:</p> <ul> <li> <code>SubsetOfWeights</code>           \u2013            <p>Valid options for <code>subset_of_weights</code>.</p> </li> <li> <code>HessianStructure</code>           \u2013            <p>Valid options for <code>hessian_structure</code>.</p> </li> <li> <code>Likelihood</code>           \u2013            <p>Valid options for <code>likelihood</code>.</p> </li> <li> <code>PredType</code>           \u2013            <p>Valid options for <code>pred_type</code>.</p> </li> <li> <code>LinkApprox</code>           \u2013            <p>Valid options for <code>link_approx</code>.</p> </li> <li> <code>TuningMethod</code>           \u2013            <p>Valid options for the <code>method</code> parameter in <code>optimize_prior_precision</code>.</p> </li> <li> <code>PriorStructure</code>           \u2013            <p>Valid options for the <code>prior_structure</code> in <code>optimize_prior_precision</code>.</p> </li> </ul>"},{"location":"api_reference/enums/#laplace.utils.enums.SubsetOfWeights","title":"SubsetOfWeights","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Valid options for <code>subset_of_weights</code>.</p> <p>Attributes:</p> <ul> <li> <code>ALL</code>           \u2013            <p>All-layer, all-parameter Laplace.</p> </li> <li> <code>LAST_LAYER</code>           \u2013            <p>Last-layer Laplace.</p> </li> <li> <code>SUBNETWORK</code>           \u2013            <p>Subnetwork Laplace.</p> </li> </ul>"},{"location":"api_reference/enums/#laplace.utils.enums.SubsetOfWeights.ALL","title":"ALL","text":"<pre><code>ALL = 'all'\n</code></pre> <p>All-layer, all-parameter Laplace.</p>"},{"location":"api_reference/enums/#laplace.utils.enums.SubsetOfWeights.LAST_LAYER","title":"LAST_LAYER","text":"<pre><code>LAST_LAYER = 'last_layer'\n</code></pre> <p>Last-layer Laplace.</p>"},{"location":"api_reference/enums/#laplace.utils.enums.SubsetOfWeights.SUBNETWORK","title":"SUBNETWORK","text":"<pre><code>SUBNETWORK = 'subnetwork'\n</code></pre> <p>Subnetwork Laplace.</p>"},{"location":"api_reference/enums/#laplace.utils.enums.HessianStructure","title":"HessianStructure","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Valid options for <code>hessian_structure</code>.</p> <p>Attributes:</p> <ul> <li> <code>FULL</code>           \u2013            <p>Full Hessian (generally very expensive).</p> </li> <li> <code>KRON</code>           \u2013            <p>Kronecker-factored Hessian (preferrable).</p> </li> <li> <code>DIAG</code>           \u2013            <p>Diagonal Hessian.</p> </li> <li> <code>LOWRANK</code>           \u2013            <p>Low-rank Hessian.</p> </li> <li> <code>GP</code>           \u2013            <p>Functional Laplace.</p> </li> </ul>"},{"location":"api_reference/enums/#laplace.utils.enums.HessianStructure.FULL","title":"FULL","text":"<pre><code>FULL = 'full'\n</code></pre> <p>Full Hessian (generally very expensive).</p>"},{"location":"api_reference/enums/#laplace.utils.enums.HessianStructure.KRON","title":"KRON","text":"<pre><code>KRON = 'kron'\n</code></pre> <p>Kronecker-factored Hessian (preferrable).</p>"},{"location":"api_reference/enums/#laplace.utils.enums.HessianStructure.DIAG","title":"DIAG","text":"<pre><code>DIAG = 'diag'\n</code></pre> <p>Diagonal Hessian.</p>"},{"location":"api_reference/enums/#laplace.utils.enums.HessianStructure.LOWRANK","title":"LOWRANK","text":"<pre><code>LOWRANK = 'lowrank'\n</code></pre> <p>Low-rank Hessian.</p>"},{"location":"api_reference/enums/#laplace.utils.enums.HessianStructure.GP","title":"GP","text":"<pre><code>GP = 'gp'\n</code></pre> <p>Functional Laplace.</p>"},{"location":"api_reference/enums/#laplace.utils.enums.Likelihood","title":"Likelihood","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Valid options for <code>likelihood</code>.</p> <p>Attributes:</p> <ul> <li> <code>REGRESSION</code>           \u2013            <p>Homoskedastic regression, assuming <code>loss_fn = nn.MSELoss()</code>.</p> </li> <li> <code>CLASSIFICATION</code>           \u2013            <p>Classification, assuming <code>loss_fn = nn.CrossEntropyLoss()</code>.</p> </li> <li> <code>REWARD_MODELING</code>           \u2013            <p>Bradley-Terry likelihood, for preference learning / reward modeling.</p> </li> </ul>"},{"location":"api_reference/enums/#laplace.utils.enums.Likelihood.REGRESSION","title":"REGRESSION","text":"<pre><code>REGRESSION = 'regression'\n</code></pre> <p>Homoskedastic regression, assuming <code>loss_fn = nn.MSELoss()</code>.</p>"},{"location":"api_reference/enums/#laplace.utils.enums.Likelihood.CLASSIFICATION","title":"CLASSIFICATION","text":"<pre><code>CLASSIFICATION = 'classification'\n</code></pre> <p>Classification, assuming <code>loss_fn = nn.CrossEntropyLoss()</code>.</p>"},{"location":"api_reference/enums/#laplace.utils.enums.Likelihood.REWARD_MODELING","title":"REWARD_MODELING","text":"<pre><code>REWARD_MODELING = 'reward_modeling'\n</code></pre> <p>Bradley-Terry likelihood, for preference learning / reward modeling.</p>"},{"location":"api_reference/enums/#laplace.utils.enums.PredType","title":"PredType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Valid options for <code>pred_type</code>.</p> <p>Attributes:</p> <ul> <li> <code>GLM</code>           \u2013            <p>Linearized, closed-form predictive.</p> </li> <li> <code>NN</code>           \u2013            <p>Monte-Carlo predictive on the NN's weights.</p> </li> <li> <code>GP</code>           \u2013            <p>Gaussian-process predictive, done by inverting the kernel matrix.</p> </li> </ul>"},{"location":"api_reference/enums/#laplace.utils.enums.PredType.GLM","title":"GLM","text":"<pre><code>GLM = 'glm'\n</code></pre> <p>Linearized, closed-form predictive.</p>"},{"location":"api_reference/enums/#laplace.utils.enums.PredType.NN","title":"NN","text":"<pre><code>NN = 'nn'\n</code></pre> <p>Monte-Carlo predictive on the NN's weights.</p>"},{"location":"api_reference/enums/#laplace.utils.enums.PredType.GP","title":"GP","text":"<pre><code>GP = 'gp'\n</code></pre> <p>Gaussian-process predictive, done by inverting the kernel matrix.</p>"},{"location":"api_reference/enums/#laplace.utils.enums.LinkApprox","title":"LinkApprox","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Valid options for <code>link_approx</code>. Only works with <code>likelihood = Likelihood.CLASSIFICATION</code>.</p> <p>Attributes:</p> <ul> <li> <code>MC</code>           \u2013            <p>Monte-Carlo approximation in the function space on top of the GLM predictive.</p> </li> <li> <code>PROBIT</code>           \u2013            <p>Closed-form multiclass probit approximation.</p> </li> <li> <code>BRIDGE</code>           \u2013            <p>Closed-form Laplace Bridge approximation.</p> </li> <li> <code>BRIDGE_NORM</code>           \u2013            <p>Closed-form Laplace Bridge approximation with normalization factor.</p> </li> </ul>"},{"location":"api_reference/enums/#laplace.utils.enums.LinkApprox.MC","title":"MC","text":"<pre><code>MC = 'mc'\n</code></pre> <p>Monte-Carlo approximation in the function space on top of the GLM predictive.</p>"},{"location":"api_reference/enums/#laplace.utils.enums.LinkApprox.PROBIT","title":"PROBIT","text":"<pre><code>PROBIT = 'probit'\n</code></pre> <p>Closed-form multiclass probit approximation.</p>"},{"location":"api_reference/enums/#laplace.utils.enums.LinkApprox.BRIDGE","title":"BRIDGE","text":"<pre><code>BRIDGE = 'bridge'\n</code></pre> <p>Closed-form Laplace Bridge approximation.</p>"},{"location":"api_reference/enums/#laplace.utils.enums.LinkApprox.BRIDGE_NORM","title":"BRIDGE_NORM","text":"<pre><code>BRIDGE_NORM = 'bridge_norm'\n</code></pre> <p>Closed-form Laplace Bridge approximation with normalization factor. Preferable to <code>BRIDGE</code>.</p>"},{"location":"api_reference/enums/#laplace.utils.enums.TuningMethod","title":"TuningMethod","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Valid options for the <code>method</code> parameter in <code>optimize_prior_precision</code>.</p> <p>Attributes:</p> <ul> <li> <code>MARGLIK</code>           \u2013            <p>Marginal-likelihood loss via SGD. Does not require validation data.</p> </li> <li> <code>GRIDSEARCH</code>           \u2013            <p>Grid search. Requires validation data.</p> </li> </ul>"},{"location":"api_reference/enums/#laplace.utils.enums.TuningMethod.MARGLIK","title":"MARGLIK","text":"<pre><code>MARGLIK = 'marglik'\n</code></pre> <p>Marginal-likelihood loss via SGD. Does not require validation data.</p>"},{"location":"api_reference/enums/#laplace.utils.enums.TuningMethod.GRIDSEARCH","title":"GRIDSEARCH","text":"<pre><code>GRIDSEARCH = 'gridsearch'\n</code></pre> <p>Grid search. Requires validation data.</p>"},{"location":"api_reference/enums/#laplace.utils.enums.PriorStructure","title":"PriorStructure","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Valid options for the <code>prior_structure</code> in <code>optimize_prior_precision</code>.</p> <p>Attributes:</p> <ul> <li> <code>SCALAR</code>           \u2013            <p>Scalar prior precision \\( \\tau I, \\tau \\in \\mathbf{R} \\).</p> </li> <li> <code>DIAG</code>           \u2013            <p>Scalar prior precision \\( \\tau \\in \\mathbb{R}^p \\).</p> </li> <li> <code>LAYERWISE</code>           \u2013            <p>Layerwise prior precision, i.e. a single scalar prior precision for each block </p> </li> </ul>"},{"location":"api_reference/enums/#laplace.utils.enums.PriorStructure.SCALAR","title":"SCALAR","text":"<pre><code>SCALAR = 'scalar'\n</code></pre> <p>Scalar prior precision \\( \\tau I, \\tau \\in \\mathbf{R} \\).</p>"},{"location":"api_reference/enums/#laplace.utils.enums.PriorStructure.DIAG","title":"DIAG","text":"<pre><code>DIAG = 'diag'\n</code></pre> <p>Scalar prior precision \\( \\tau \\in \\mathbb{R}^p \\).</p>"},{"location":"api_reference/enums/#laplace.utils.enums.PriorStructure.LAYERWISE","title":"LAYERWISE","text":"<pre><code>LAYERWISE = 'layerwise'\n</code></pre> <p>Layerwise prior precision, i.e. a single scalar prior precision for each block  (corresponding to each the NN's layer) of the diagonal prior-precision matrix..</p>"},{"location":"api_reference/functionallaplace/","title":"Functional Laplace","text":""},{"location":"api_reference/functionallaplace/#laplace.baselaplace","title":"laplace.baselaplace","text":"<p>Classes:</p> <ul> <li> <code>FunctionalLaplace</code>           \u2013            <p>Applying the GGN (Generalized Gauss-Newton) approximation for the Hessian in the Laplace approximation of the posterior</p> </li> </ul>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace","title":"FunctionalLaplace","text":"<pre><code>FunctionalLaplace(model: Module, likelihood: Likelihood | str, n_subset: int, sigma_noise: float | Tensor = 1.0, prior_precision: float | Tensor = 1.0, prior_mean: float | Tensor = 0.0, temperature: float = 1.0, enable_backprop: bool = False, dict_key_x='input_ids', dict_key_y='labels', backend: type[CurvatureInterface] | None = BackPackGGN, backend_kwargs: dict[str, Any] | None = None, independent_outputs: bool = False, seed: int = 0)\n</code></pre> <p>               Bases: <code>BaseLaplace</code></p> <p>Applying the GGN (Generalized Gauss-Newton) approximation for the Hessian in the Laplace approximation of the posterior turns the underlying probabilistic model from a BNN into a GLM (generalized linear model). This GLM (in the weight space) is equivalent to a GP (in the function space), see Approximate Inference Turns Deep Networks into Gaussian Processes (Khan et al., 2019)</p> <p>This class implements the (approximate) GP inference through which we obtain the desired quantities (posterior predictive, marginal log-likelihood). See Improving predictions of Bayesian neural nets via local linearization (Immer et al., 2021) for more details.</p> <p>Note that for <code>likelihood='classification'</code>, we approximate \\( L_{NN} \\) with a diagonal matrix ( \\( L_{NN} \\) is a block-diagonal matrix, where blocks represent Hessians of per-data-point log-likelihood w.r.t. neural network output \\( f \\), See Appendix A.2.1 for exact definition). We resort to such an approximation because of the (possible) errors found in Laplace approximation for multiclass GP classification in Chapter 3.5 of R&amp;W 2006 GP book, see the question here for more details. Alternatively, one could also resort to one-vs-one or one-vs-rest implementations for multiclass classification, however, that is not (yet) supported here.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>fit</code>             \u2013              <p>Fit the Laplace approximation of a GP posterior.</p> </li> <li> <code>__call__</code>             \u2013              <p>Compute the posterior predictive on input data <code>x</code>.</p> </li> <li> <code>functional_samples</code>             \u2013              <p>Sample from the functional posterior on input data <code>x</code>.</p> </li> <li> <code>predictive_samples</code>             \u2013              <p>Sample from the posterior predictive on input data <code>x</code>.</p> </li> <li> <code>functional_variance</code>             \u2013              <p>GP posterior variance:</p> </li> <li> <code>functional_covariance</code>             \u2013              <p>GP posterior covariance:</p> </li> <li> <code>optimize_prior_precision</code>             \u2013              <p><code>optimize_prior_precision_base</code> from <code>BaseLaplace</code> with <code>pred_type='gp'</code></p> </li> <li> <code>log_marginal_likelihood</code>             \u2013              <p>Compute the Laplace approximation to the log marginal likelihood.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>log_likelihood</code>               (<code>Tensor</code>)           \u2013            <p>Compute log likelihood on the training data after <code>.fit()</code> has been called.</p> </li> <li> <code>prior_precision_diag</code>               (<code>Tensor</code>)           \u2013            <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either</p> </li> <li> <code>log_det_ratio</code>               (<code>Tensor</code>)           \u2013            <p>Computes log determinant term in GP marginal likelihood</p> </li> <li> <code>scatter</code>               (<code>Tensor</code>)           \u2013            <p>Compute scatter term in GP log marginal likelihood.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    n_subset: int,\n    sigma_noise: float | torch.Tensor = 1.0,\n    prior_precision: float | torch.Tensor = 1.0,\n    prior_mean: float | torch.Tensor = 0.0,\n    temperature: float = 1.0,\n    enable_backprop: bool = False,\n    dict_key_x=\"input_ids\",\n    dict_key_y=\"labels\",\n    backend: type[CurvatureInterface] | None = BackPackGGN,\n    backend_kwargs: dict[str, Any] | None = None,\n    independent_outputs: bool = False,\n    seed: int = 0,\n):\n    assert backend in [BackPackGGN, AsdlGGN, CurvlinopsGGN]\n    self._check_prior_precision(prior_precision)\n    super().__init__(\n        model,\n        likelihood,\n        sigma_noise,\n        prior_precision,\n        prior_mean,\n        temperature,\n        enable_backprop,\n        dict_key_x,\n        dict_key_y,\n        backend,\n        backend_kwargs,\n    )\n    self.enable_backprop = enable_backprop\n\n    self.n_subset = n_subset\n    self.independent_outputs = independent_outputs\n    self.seed = seed\n\n    self.K_MM = None\n    self.Sigma_inv = None  # (K_{MM} + L_MM_inv)^{-1}\n    self.train_loader = (\n        None  # needed in functional variance and marginal log likelihood\n    )\n    self.batch_size = None\n    self._prior_factor_sod = None\n    self.mu = None  # mean in the scatter term of the log marginal likelihood\n    self.L = None\n\n    # Posterior mean (used in regression marginal likelihood)\n    self.mean = parameters_to_vector(self.model.parameters()).detach()\n\n    self._fitted = False\n    self._recompute_Sigma = True\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace(num_data)","title":"<code>num_data</code>","text":"(<code>int</code>)           \u2013            <p>number of data points for Subset-of-Data (SOD) approximate GP inference.</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace(diagonal_kernel)","title":"<code>diagonal_kernel</code>","text":"(<code>bool</code>)           \u2013            <p>GP kernel here is product of Jacobians, which results in a \\( C \\times C\\) matrix where \\(C\\) is the output dimension. If <code>diagonal_kernel=True</code>, only a diagonal of a GP kernel is used. This is (somewhat) equivalent to assuming independent GPs across output channels.</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace(See)","title":"<code>See</code>","text":"\u2013"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.log_likelihood","title":"log_likelihood","text":"<pre><code>log_likelihood: Tensor\n</code></pre> <p>Compute log likelihood on the training data after <code>.fit()</code> has been called. The log likelihood is computed on-demand based on the loss and, for example, the observation noise which makes it differentiable in the latter for iterative updates.</p> <p>Returns:</p> <ul> <li> <code>log_likelihood</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.prior_precision_diag","title":"prior_precision_diag","text":"<pre><code>prior_precision_diag: Tensor\n</code></pre> <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either a scalar, layer-wise, or diagonal prior precision.</p> <p>Returns:</p> <ul> <li> <code>prior_precision_diag</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.log_det_ratio","title":"log_det_ratio","text":"<pre><code>log_det_ratio: Tensor\n</code></pre> <p>Computes log determinant term in GP marginal likelihood</p> <p>For <code>classification</code> we use eq. (3.44) from Chapter 3.5 from GP book R&amp;W 2006 with (note that we always use diagonal approximation \\(D\\) of the Hessian of log likelihood w.r.t. \\(f\\)):</p> <p>log determinant term := \\( \\log | I + D^{1/2}K D^{1/2} | \\)</p> <p>For <code>regression</code>, we use \"standard\" GP marginal likelihood:</p> <p>log determinant term := \\( \\log | K + \\sigma_2 I | \\)</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.scatter","title":"scatter","text":"<pre><code>scatter: Tensor\n</code></pre> <p>Compute scatter term in GP log marginal likelihood.</p> <p>For <code>classification</code> we use eq. (3.44) from Chapter 3.5 from GP book R&amp;W 2006 with \\(\\hat{f} = f \\):</p> <p>scatter term := \\( f K^{-1} f^{T} \\)</p> <p>For <code>regression</code>, we use \"standard\" GP marginal likelihood:</p> <p>scatter term := \\( (y - m)K^{-1}(y -m )^T \\), where \\( m \\) is the mean of the GP prior, which in our case corresponds to \\( m := f + J (\\theta - \\theta_{MAP}) \\)</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._glm_forward_call","title":"_glm_forward_call","text":"<pre><code>_glm_forward_call(x: Tensor | MutableMapping, likelihood: Likelihood | str, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code> for \"glm\" pred type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_forward_call(\n    self,\n    x: torch.Tensor | MutableMapping,\n    likelihood: Likelihood | str,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x` for \"glm\" pred type.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    likelihood : Likelihood or str in {'classification', 'regression', 'reward_modeling'}\n        determines the log likelihood Hessian approximation.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` and `link_approx='mc'`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    f_mu, f_var = self._glm_predictive_distribution(\n        x, joint=joint and likelihood == Likelihood.REGRESSION\n    )\n\n    if likelihood == Likelihood.REGRESSION:\n        if diagonal_output and not joint:\n            f_var = torch.diagonal(f_var, dim1=-2, dim2=-1)\n        return f_mu, f_var\n\n    if link_approx == LinkApprox.MC:\n        return self._glm_predictive_samples(\n            f_mu,\n            f_var,\n            n_samples=n_samples,\n            diagonal_output=diagonal_output,\n        ).mean(dim=0)\n    elif link_approx == LinkApprox.PROBIT:\n        kappa = 1 / torch.sqrt(1.0 + np.pi / 8 * f_var.diagonal(dim1=1, dim2=2))\n        return torch.softmax(kappa * f_mu, dim=-1)\n    elif \"bridge\" in link_approx:\n        # zero mean correction\n        f_mu -= (\n            f_var.sum(-1)\n            * f_mu.sum(-1).reshape(-1, 1)\n            / f_var.sum(dim=(1, 2)).reshape(-1, 1)\n        )\n        f_var -= torch.einsum(\n            \"bi,bj-&gt;bij\", f_var.sum(-1), f_var.sum(-2)\n        ) / f_var.sum(dim=(1, 2)).reshape(-1, 1, 1)\n\n        # Laplace Bridge\n        _, K = f_mu.size(0), f_mu.size(-1)\n        f_var_diag = torch.diagonal(f_var, dim1=1, dim2=2)\n\n        # optional: variance correction\n        if link_approx == LinkApprox.BRIDGE_NORM:\n            f_var_diag_mean = f_var_diag.mean(dim=1)\n            f_var_diag_mean /= torch.as_tensor(\n                [K / 2], device=self._device, dtype=self._dtype\n            ).sqrt()\n            f_mu /= f_var_diag_mean.sqrt().unsqueeze(-1)\n            f_var_diag /= f_var_diag_mean.unsqueeze(-1)\n\n        sum_exp = torch.exp(-f_mu).sum(dim=1).unsqueeze(-1)\n        alpha = (1 - 2 / K + f_mu.exp() / K**2 * sum_exp) / f_var_diag\n        return torch.nan_to_num(alpha / alpha.sum(dim=1).unsqueeze(-1), nan=1.0)\n    else:\n        raise ValueError(\n            \"Prediction path invalid. Check the likelihood, pred_type, link_approx combination!\"\n        )\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._glm_forward_call(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._glm_forward_call(likelihood)","title":"<code>likelihood</code>","text":"(<code>Likelihood or str in {'classification', 'regression', 'reward_modeling'}</code>)           \u2013            <p>determines the log likelihood Hessian approximation.</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._glm_forward_call(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._glm_forward_call(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._glm_forward_call(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._glm_forward_call(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> and <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._glm_functional_samples","title":"_glm_functional_samples","text":"<pre><code>_glm_functional_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior functional on input data <code>x</code> using \"glm\" prediction type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_functional_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior functional on input data `x` using \"glm\" prediction\n    type.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    assert f_var.shape == torch.Size([f_mu.shape[0], f_mu.shape[1], f_mu.shape[1]])\n\n    if diagonal_output:\n        f_var = torch.diagonal(f_var, dim1=1, dim2=2)\n\n    return normal_samples(f_mu, f_var, n_samples, generator)\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._glm_functional_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._glm_functional_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._glm_functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._glm_functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._glm_functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples","title":"_glm_predictive_samples","text":"<pre><code>_glm_predictive_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code> using \"glm\" prediction type. I.e., the inverse-link function correponding to the likelihood is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_predictive_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x` using \"glm\" prediction\n    type. I.e., the inverse-link function correponding to the likelihood is applied\n    on top of the functional sample.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    f_samples = self._glm_functional_samples(\n        f_mu, f_var, n_samples, diagonal_output, generator\n    )\n\n    if self.likelihood == Likelihood.REGRESSION:\n        return f_samples\n    else:\n        return torch.softmax(f_samples, dim=-1)\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._check_prior_precision","title":"_check_prior_precision","text":"<pre><code>_check_prior_precision(prior_precision: float | Tensor)\n</code></pre> <p>Checks if the given prior precision is suitable for the GP interpretation of LLA. As such, only single value priors, i.e., isotropic priors are suitable.</p> Source code in <code>laplace/baselaplace.py</code> <pre><code>@staticmethod\ndef _check_prior_precision(prior_precision: float | torch.Tensor):\n    \"\"\"Checks if the given prior precision is suitable for the GP interpretation of LLA.\n    As such, only single value priors, i.e., isotropic priors are suitable.\n    \"\"\"\n    if torch.is_tensor(prior_precision):\n        if not (\n            prior_precision.ndim == 0\n            or (prior_precision.ndim == 1 and len(prior_precision) == 1)\n        ):\n            raise ValueError(\"Only isotropic priors supported in FunctionalLaplace\")\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._init_K_MM","title":"_init_K_MM","text":"<pre><code>_init_K_MM()\n</code></pre> <p>Allocates memory for the kernel matrix evaluated at the subset of the training data points. If the subset is of size \\(M\\) and the problem has \\(C\\) outputs, this is a list of C \\((M,M\\)) tensors for diagonal kernel and \\((M x C, M x C)\\) otherwise.</p> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _init_K_MM(self):\n    \"\"\"Allocates memory for the kernel matrix evaluated at the subset of the training\n    data points. If the subset is of size \\\\(M\\\\) and the problem has \\\\(C\\\\) outputs,\n    this is a list of C \\\\((M,M\\\\)) tensors for diagonal kernel and \\\\((M x C, M x C)\\\\)\n    otherwise.\n    \"\"\"\n    if self.independent_outputs:\n        self.K_MM = [\n            torch.empty(\n                size=(self.n_subset, self.n_subset),\n                device=self._device,\n                dtype=self._dtype,\n            )\n            for _ in range(self.n_outputs)\n        ]\n    else:\n        self.K_MM = torch.empty(\n            size=(self.n_subset * self.n_outputs, self.n_subset * self.n_outputs),\n            device=self._device,\n            dtype=self._dtype,\n        )\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._init_Sigma_inv","title":"_init_Sigma_inv","text":"<pre><code>_init_Sigma_inv()\n</code></pre> <p>Allocates memory for the cholesky decomposition of [     K_{MM} + \\Lambda_{MM}^{-1}. ] See See Improving predictions of Bayesian neural nets via local linearization (Immer et al., 2021) Equation 15 for more information.</p> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _init_Sigma_inv(self):\n    \"\"\"Allocates memory for the cholesky decomposition of\n    \\\\[\n        K_{MM} + \\\\Lambda_{MM}^{-1}.\n    \\\\]\n    See See [Improving predictions of Bayesian neural nets via local linearization (Immer et al., 2021)](https://arxiv.org/abs/2008.08400)\n    Equation 15 for more information.\n    \"\"\"\n    if self.independent_outputs:\n        self.Sigma_inv = [\n            torch.empty(\n                size=(self.n_subset, self.n_subset),\n                device=self._device,\n                dtype=self._dtype,\n            )\n            for _ in range(self.n_outputs)\n        ]\n    else:\n        self.Sigma_inv = torch.empty(\n            size=(self.n_subset * self.n_outputs, self.n_subset * self.n_outputs),\n            device=self._device,\n            dtype=self._dtype,\n        )\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._store_K_batch","title":"_store_K_batch","text":"<pre><code>_store_K_batch(K_batch: Tensor, i: int, j: int)\n</code></pre> <p>Given the kernel matrix between the i-th and the j-th batch, stores it in the corresponding position in self.K_MM.</p> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _store_K_batch(self, K_batch: torch.Tensor, i: int, j: int):\n    \"\"\"Given the kernel matrix between the i-th and the j-th batch, stores it in the\n    corresponding position in self.K_MM.\n    \"\"\"\n    if self.independent_outputs:\n        for c in range(self.n_outputs):\n            self.K_MM[c][\n                i * self.batch_size : min((i + 1) * self.batch_size, self.n_subset),\n                j * self.batch_size : min((j + 1) * self.batch_size, self.n_subset),\n            ] = K_batch[:, :, c]\n            if i != j:\n                self.K_MM[c][\n                    j * self.batch_size : min(\n                        (j + 1) * self.batch_size, self.n_subset\n                    ),\n                    i * self.batch_size : min(\n                        (i + 1) * self.batch_size, self.n_subset\n                    ),\n                ] = torch.transpose(K_batch[:, :, c], 0, 1)\n    else:\n        bC = self.batch_size * self.n_outputs\n        MC = self.n_subset * self.n_outputs\n        self.K_MM[\n            i * bC : min((i + 1) * bC, MC), j * bC : min((j + 1) * bC, MC)\n        ] = K_batch\n        if i != j:\n            self.K_MM[\n                j * bC : min((j + 1) * bC, MC), i * bC : min((i + 1) * bC, MC)\n            ] = torch.transpose(K_batch, 0, 1)\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._build_L","title":"_build_L","text":"<pre><code>_build_L(lambdas: list[Tensor])\n</code></pre> <p>Given a list of the Hessians of per-batch log-likelihood w.r.t. neural network output \\( f \\), returns the contatenation of these hessians in a suitable format for the used kernel (diagonal or not).</p> <p>In this function the diagonal approximation is performed. Please refer to the introduction of the class for more details.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>L</code> (              <code>list with length C of tensors with shape M or tensor (MxC)</code> )          \u2013            <p>Contains the given Hessians in a suitable format.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _build_L(self, lambdas: list[torch.Tensor]):\n    \"\"\"Given a list of the Hessians of per-batch log-likelihood w.r.t. neural network output \\\\( f \\\\),\n    returns the contatenation of these hessians in a suitable format for the used kernel\n    (diagonal or not).\n\n    In this function the diagonal approximation is performed. Please refer to the introduction of the\n    class for more details.\n\n    Parameters\n    ----------\n    lambdas : list of torch.Tensor of shape (C, C)\n              Contains per-batch log-likelihood w.r.t. neural network output \\\\( f \\\\).\n\n    Returns\n    -------\n    L : list with length C of tensors with shape M or tensor (MxC)\n        Contains the given Hessians in a suitable format.\n    \"\"\"\n    # Concatenate batch dimension and discard non-diagonal entries.\n    L_diag = torch.diagonal(torch.cat(lambdas, dim=0), dim1=-2, dim2=-1).reshape(-1)\n\n    if self.independent_outputs:\n        return [L_diag[i :: self.n_outputs] for i in range(self.n_outputs)]\n    else:\n        return L_diag\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._build_L(lambdas)","title":"<code>lambdas</code>","text":"(<code>list of torch.Tensor of shape (C, C)</code>)           \u2013            <pre><code>  Contains per-batch log-likelihood w.r.t. neural network output \\( f \\).\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._build_Sigma_inv","title":"_build_Sigma_inv","text":"<pre><code>_build_Sigma_inv()\n</code></pre> <p>Computes the cholesky decomposition of         [             K_{MM} + \\Lambda_{MM}^{-1}.         ]         See See Improving predictions of Bayesian neural nets via local linearization (Immer et al., 2021)         Equation 15 for more information.</p> <p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD         As the diagonal approximation is performed with \\Lambda_{MM} (which is stored in self.L), =======         As the diagonal approximation is performed with \\(\\Lambda_{MM}\\) (which is stored in self.L),</p> <p>main         the code is greatly simplified.</p> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _build_Sigma_inv(self):\n    \"\"\"Computes the cholesky decomposition of\n            \\\\[\n                K_{MM} + \\\\Lambda_{MM}^{-1}.\n            \\\\]\n            See See [Improving predictions of Bayesian neural nets via local linearization (Immer et al., 2021)](https://arxiv.org/abs/2008.08400)\n            Equation 15 for more information.\n\n    &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n            As the diagonal approximation is performed with \\\\Lambda_{MM} (which is stored in self.L),\n    =======\n            As the diagonal approximation is performed with \\\\(\\\\Lambda_{MM}\\\\) (which is stored in self.L),\n    &gt;&gt;&gt;&gt;&gt;&gt;&gt; main\n            the code is greatly simplified.\n    \"\"\"\n    if self.independent_outputs:\n        self.Sigma_inv = [\n            torch.linalg.cholesky(\n                self.gp_kernel_prior_variance * self.K_MM[c]\n                + torch.diag(\n                    torch.nan_to_num(1.0 / (self._H_factor * lambda_c), posinf=10.0)\n                )\n            )\n            for c, lambda_c in enumerate(self.L)\n        ]\n    else:\n        self.Sigma_inv = torch.linalg.cholesky(\n            self.gp_kernel_prior_variance * self.K_MM\n            + torch.diag(\n                torch.nan_to_num(1 / (self._H_factor * self.L), posinf=10.0)\n            )\n        )\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._get_SoD_data_loader","title":"_get_SoD_data_loader","text":"<pre><code>_get_SoD_data_loader(train_loader: DataLoader) -&gt; DataLoader\n</code></pre> <p>Subset-of-Datapoints data loader</p> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _get_SoD_data_loader(self, train_loader: DataLoader) -&gt; DataLoader:\n    \"\"\"Subset-of-Datapoints data loader\"\"\"\n    return DataLoader(\n        dataset=train_loader.dataset,\n        batch_size=train_loader.batch_size,\n        sampler=SoDSampler(\n            N=len(train_loader.dataset), M=self.n_subset, seed=self.seed\n        ),\n        shuffle=False,\n    )\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.fit","title":"fit","text":"<pre><code>fit(train_loader: DataLoader | MutableMapping, progress_bar: bool = False)\n</code></pre> <p>Fit the Laplace approximation of a GP posterior.</p> <p>Parameters:</p> Source code in <code>laplace/baselaplace.py</code> <pre><code>def fit(\n    self, train_loader: DataLoader | MutableMapping, progress_bar: bool = False\n):\n    \"\"\"Fit the Laplace approximation of a GP posterior.\n\n    Parameters\n    ----------\n    train_loader : torch.data.utils.DataLoader\n        `train_loader.dataset` needs to be set to access \\\\(N\\\\), size of the data set\n        `train_loader.batch_size` needs to be set to access \\\\(b\\\\) batch_size\n    progress_bar : bool\n        whether to show a progress bar during the fitting process.\n    \"\"\"\n    # Set model to evaluation mode\n    self.model.eval()\n\n    data = next(iter(train_loader))\n    with torch.no_grad():\n        if isinstance(data, MutableMapping):  # To support Huggingface dataset\n            if \"backpack\" in self._backend_cls.__name__.lower():\n                raise ValueError(\n                    \"Currently BackPACK backend is not supported \"\n                    + \"for custom models with non-tensor inputs \"\n                    + \"(https://github.com/pytorch/functorch/issues/159). Consider \"\n                    + \"using AsdlGGN backend instead.\"\n                )\n\n            out = self.model(data)\n        else:\n            X = data[0]\n            try:\n                out = self.model(X[:1].to(self._device))\n            except (TypeError, AttributeError):\n                out = self.model(X.to(self._device))\n    self.n_outputs = out.shape[-1]\n    setattr(self.model, \"output_size\", self.n_outputs)\n    self.batch_size = train_loader.batch_size\n\n    if (\n        self.likelihood == \"regression\"\n        and self.n_outputs &gt; 1\n        and self.independent_outputs\n    ):\n        warnings.warn(\n            \"Using FunctionalLaplace with the diagonal approximation of a GP kernel is not recommended \"\n            \"in the case of multivariate regression. Predictive variance will likely be overestimated.\"\n        )\n\n    N = len(train_loader.dataset)\n    self.n_data = N\n\n    assert (\n        self.n_subset &lt;= N\n    ), \"`num_data` must be less than or equal to the original number of data points.\"\n\n    train_loader = self._get_SoD_data_loader(train_loader)\n    self.train_loader = train_loader\n    self._prior_factor_sod = self.n_subset / self.n_data\n\n    self._init_K_MM()\n    self._init_Sigma_inv()\n\n    f, lambdas, mu = [], [], []\n\n    if progress_bar:\n        loader = enumerate(tqdm.tqdm(train_loader, desc=\"Fitting\"))\n    else:\n        loader = enumerate(train_loader)\n\n    for i, data in loader:\n        if isinstance(data, MutableMapping):  # To support Huggingface dataset\n            X, y = data, data[self.dict_key_y].to(self._device)\n        else:\n            X, y = data\n            X, y = X.to(self._device), y.to(self._device)\n\n        Js_batch, f_batch = self._jacobians(X, enable_backprop=False)\n\n        if self.likelihood == Likelihood.REGRESSION and y.ndim != out.ndim:\n            raise ValueError(\n                f\"The model's output has {out.ndim} dims but \"\n                f\"the target has {y.ndim} dims.\"\n            )\n\n        with torch.no_grad():\n            loss_batch = self.backend.factor * self.backend.lossfunc(f_batch, y)\n\n        if self.likelihood == Likelihood.REGRESSION:\n            b, C = f_batch.shape\n            lambdas_batch = torch.unsqueeze(\n                torch.eye(C, device=self._device, dtype=self._dtype), 0\n            ).repeat(b, 1, 1)\n        else:\n            # second derivative of log lik is diag(p) - pp^T\n            ps = torch.softmax(f_batch, dim=-1)\n            lambdas_batch = torch.diag_embed(ps) - torch.einsum(\n                \"mk,mc-&gt;mck\", ps, ps\n            )\n\n        self.loss += loss_batch\n        lambdas.append(lambdas_batch)\n        f.append(f_batch)\n        mu.append(\n            self._mean_scatter_term_batch(Js_batch, f_batch, y)\n        )  # needed for marginal likelihood\n        for j, (X2, _) in enumerate(train_loader):\n            if j &gt;= i:\n                X2 = X2.to(self._device)\n                K_batch = self._kernel_batch(Js_batch, X2)\n                self._store_K_batch(K_batch, i, j)\n\n    self.L = self._build_L(lambdas)\n    self.mu = torch.cat(mu, dim=0)\n    self._build_Sigma_inv()\n    self._fitted = True\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.fit(train_loader)","title":"<code>train_loader</code>","text":"(<code>DataLoader</code>)           \u2013            <p><code>train_loader.dataset</code> needs to be set to access \\(N\\), size of the data set <code>train_loader.batch_size</code> needs to be set to access \\(b\\) batch_size</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.fit(progress_bar)","title":"<code>progress_bar</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to show a progress bar during the fitting process.</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.__call__","title":"__call__","text":"<pre><code>__call__(x: Tensor | MutableMapping, pred_type: PredType | str = GP, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None, fitting: bool = False, **model_kwargs: dict[str, Any]) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or Tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def __call__(\n    self,\n    x: torch.Tensor | MutableMapping,\n    pred_type: PredType | str = PredType.GP,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n    fitting: bool = False,\n    **model_kwargs: dict[str, Any],\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x`.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    pred_type : {'gp'}, default='gp'\n        type of posterior predictive, linearized GLM predictive (GP).\n        The GP predictive is consistent with\n        the curvature approximations used here.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `link_approx='mc'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used).\n\n    fitting : bool, default=False\n        whether or not this predictive call is done during fitting. Only useful for\n        reward modeling: the likelihood is set to `\"regression\"` when `False` and\n        `\"classification\"` when `True`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or Tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    if self._fitted is False:\n        raise RuntimeError(\n            \"Functional Laplace has not been fitted to any \"\n            + \"training dataset. Please call .fit method.\"\n        )\n\n    if self._recompute_Sigma is True:\n        warnings.warn(\n            \"The prior precision has been changed since fit. \"\n            + \"Re-compututing its value...\"\n        )\n        self._build_Sigma_inv()\n\n    if pred_type != PredType.GP:\n        raise ValueError(\"Only gp supported as prediction types.\")\n\n    if link_approx not in [la for la in LinkApprox]:\n        raise ValueError(f\"Unsupported link approximation {link_approx}.\")\n\n    if generator is not None:\n        if (\n            not isinstance(generator, torch.Generator)\n            or generator.device != x.device\n        ):\n            raise ValueError(\"Invalid random generator (check type and device).\")\n\n    likelihood = self.likelihood\n    if likelihood == Likelihood.REWARD_MODELING:\n        likelihood = Likelihood.CLASSIFICATION if fitting else Likelihood.REGRESSION\n\n    return self._glm_forward_call(\n        x, likelihood, joint, link_approx, n_samples, diagonal_output\n    )\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.__call__(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.__call__(pred_type)","title":"<code>pred_type</code>","text":"(<code>'gp'</code>, default:                   <code>'gp'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive (GP). The GP predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.__call__(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>.</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.__call__(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.__call__(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.__call__(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.__call__(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used).</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.__call__(fitting)","title":"<code>fitting</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether or not this predictive call is done during fitting. Only useful for reward modeling: the likelihood is set to <code>\"regression\"</code> when <code>False</code> and <code>\"classification\"</code> when <code>True</code>.</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.functional_samples","title":"functional_samples","text":"<pre><code>functional_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the functional posterior on input data <code>x</code>. Can be used, for example, for Thompson sampling.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def functional_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the functional posterior on input data `x`.\n    Can be used, for example, for Thompson sampling.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm'}, default='glm'\n        type of posterior predictive, linearized GLM predictive.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm  supported as prediction type.\")\n\n    f_mu, f_var = self._glm_predictive_distribution(x)\n    return self._glm_functional_samples(\n        f_mu, f_var, n_samples, diagonal_output, generator\n    )\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.functional_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.functional_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>'glm'</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive.</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.predictive_samples","title":"predictive_samples","text":"<pre><code>predictive_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code>. I.e., the corresponding inverse-link function is applied on top of the functional sample. Can be used, for example, for Thompson sampling.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def predictive_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x`.\n    I.e., the corresponding inverse-link function is applied on top of the\n    functional sample. Can be used, for example, for Thompson sampling.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm'}, default='glm'\n        type of posterior predictive, linearized GLM predictive.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm  supported as prediction type.\")\n\n    f_mu, f_var = self._glm_predictive_distribution(x)\n    return self._glm_predictive_samples(\n        f_mu, f_var, n_samples, diagonal_output, generator\n    )\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.predictive_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.predictive_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>'glm'</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive.</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.functional_variance","title":"functional_variance","text":"<pre><code>functional_variance(Js_star: Tensor) -&gt; Tensor\n</code></pre> <p>GP posterior variance:</p> \\[     k_{**} - K_{*M} (K_{MM}+ L_{MM}^{-1})^{-1} K_{M*} \\] <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>f_var</code> (              <code>torch.Tensor of shape (N*,C, C)</code> )          \u2013            <p>Contains the posterior variances of N* testing points.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def functional_variance(self, Js_star: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"GP posterior variance:\n\n    $$\n        k_{**} - K_{*M} (K_{MM}+ L_{MM}^{-1})^{-1} K_{M*}\n    $$\n\n    Parameters\n    ----------\n    Js_star : torch.Tensor of shape (N*, C, P)\n              Jacobians of test data points\n\n    Returns\n    -------\n    f_var : torch.Tensor of shape (N*,C, C)\n            Contains the posterior variances of N* testing points.\n    \"\"\"\n    # Compute K_{**}\n    K_star = self.gp_kernel_prior_variance * self._kernel_star(Js_star)\n\n    # Compute K_{*M}\n    K_M_star = []\n    for X_batch, _ in self.train_loader:\n        K_M_star_batch = self.gp_kernel_prior_variance * self._kernel_batch_star(\n            Js_star, X_batch.to(self._device)\n        )\n        K_M_star.append(K_M_star_batch)\n        del X_batch\n\n    # Build_K_star_M computes K_{*M} (K_{MM}+ L_{MM}^{-1})^{-1} K_{M*}\n    f_var = K_star - self._build_K_star_M(K_M_star)\n\n    # If the considered kernel is diagonal, embed the covariances.\n    # from (N*, C) -&gt; (N*, C, C)\n    if self.independent_outputs:\n        f_var = torch.diag_embed(f_var)\n\n    return f_var\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.functional_variance(Js_star)","title":"<code>Js_star</code>","text":"(<code>torch.Tensor of shape (N*, C, P)</code>)           \u2013            <pre><code>  Jacobians of test data points\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.functional_covariance","title":"functional_covariance","text":"<pre><code>functional_covariance(Js_star: Tensor) -&gt; Tensor\n</code></pre> <p>GP posterior covariance:</p> \\[     k_{**} - K_{*M} (K_{MM}+ L_{MM}^{-1})^{-1} K_{M*} \\] <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>f_var</code> (              <code>torch.Tensor of shape (N*xC, N*xC)</code> )          \u2013            <p>Contains the posterior covariances of N* testing points.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def functional_covariance(self, Js_star: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"GP posterior covariance:\n\n    $$\n        k_{**} - K_{*M} (K_{MM}+ L_{MM}^{-1})^{-1} K_{M*}\n    $$\n\n    Parameters\n    ----------\n    Js_star : torch.Tensor of shape (N*, C, P)\n              Jacobians of test data points\n\n    Returns\n    -------\n    f_var : torch.Tensor of shape (N*xC, N*xC)\n            Contains the posterior covariances of N* testing points.\n    \"\"\"\n    # Compute K_{**}\n    K_star = self.gp_kernel_prior_variance * self._kernel_star(Js_star, joint=True)\n\n    # Compute K_{*M}\n    K_M_star = []\n    for X_batch, _ in self.train_loader:\n        K_M_star_batch = self.gp_kernel_prior_variance * self._kernel_batch_star(\n            Js_star, X_batch.to(self._device)\n        )\n        K_M_star.append(K_M_star_batch)\n        del X_batch\n\n    # Build_K_star_M computes K_{*M} (K_{MM}+ L_{MM}^{-1})^{-1} K_{M*}\n    f_var = K_star - self._build_K_star_M(K_M_star, joint=True)\n\n    # If the considered kernel is diagonal, embed the covariances.\n    # from (N*, N*, C) -&gt; (N*, N*, C, C)\n    if self.independent_outputs:\n        f_var = torch.diag_embed(f_var)\n\n    # Reshape from (N*, N*, C, C) to (N*xC, N*xC)\n    f_var = f_var.permute(0, 2, 1, 3).flatten(0, 1).flatten(1, 2)\n\n    return f_var\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.functional_covariance(Js_star)","title":"<code>Js_star</code>","text":"(<code>torch.Tensor of shape (N*, C, P)</code>)           \u2013            <pre><code>  Jacobians of test data points\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._build_K_star_M","title":"_build_K_star_M","text":"<pre><code>_build_K_star_M(K_M_star: Tensor, joint: bool = False) -&gt; Tensor\n</code></pre> <p>Computes K_{M} (K_{MM}+ L_{MM}^{-1})^{-1} K_{M} given K_{M*}.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>torch.tensor of shape (N_test, N_test, C) for joint diagonal,</code>           \u2013            </li> <li> <code>(N_test, C) for non-joint diagonal, (N_test, N_test, C, C) for</code>           \u2013            </li> <li> <code>joint non-diagonal and (N_test, C, C) for non-joint non-diagonal.</code>           \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _build_K_star_M(\n    self, K_M_star: torch.Tensor, joint: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Computes K_{*M} (K_{MM}+ L_{MM}^{-1})^{-1} K_{M*} given K_{M*}.\n\n    Parameters\n    ----------\n    K_M_star : list of torch.Tensor\n               Contains K_{M*}. Tensors have shape (N_test, C, C)\n               or (N_test, C) for diagonal kernel.\n\n    joint : boolean\n            Wether to compute cross covariances or not.\n\n    Returns\n    -------\n    torch.tensor of shape (N_test, N_test, C) for joint diagonal,\n    (N_test, C) for non-joint diagonal, (N_test, N_test, C, C) for\n    joint non-diagonal and (N_test, C, C) for non-joint non-diagonal.\n    \"\"\"\n    # Shape (N_test, N, C, C) or (N_test, N, C) for diagonal\n    K_M_star = torch.cat(K_M_star, dim=1)\n\n    if self.independent_outputs:\n        prods = []\n        for c in range(self.n_outputs):\n            # Compute K_{*M}L^{-1}\n            v = torch.squeeze(\n                torch.linalg.solve(\n                    self.Sigma_inv[c], K_M_star[:, :, c].unsqueeze(2)\n                ),\n                2,\n            )\n            if joint:\n                prod = torch.einsum(\"bm,am-&gt;ba\", v, v)\n            else:\n                prod = torch.einsum(\"bm,bm-&gt;b\", v, v)\n            prods.append(prod.unsqueeze(1))\n        prods = torch.cat(prods, dim=-1)\n        return prods\n    else:\n        # Reshape to (N_test, NxC, C) or (N_test, N, C)\n        K_M_star = K_M_star.reshape(K_M_star.shape[0], -1, K_M_star.shape[-1])\n        # Compute K_{*M}L^{-1}\n        v = torch.linalg.solve(self.Sigma_inv, K_M_star)\n        if joint:\n            return torch.einsum(\"acm,bcn-&gt;abmn\", v, v)\n        else:\n            return torch.einsum(\"bcm,bcn-&gt;bmn\", v, v)\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._build_K_star_M(K_M_star)","title":"<code>K_M_star</code>","text":"(<code>list of torch.Tensor</code>)           \u2013            <pre><code>   Contains K_{M*}. Tensors have shape (N_test, C, C)\n   or (N_test, C) for diagonal kernel.\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._build_K_star_M(joint)","title":"<code>joint</code>","text":"(<code>boolean</code>, default:                   <code>False</code> )           \u2013            <pre><code>Wether to compute cross covariances or not.\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.optimize_prior_precision","title":"optimize_prior_precision","text":"<pre><code>optimize_prior_precision(pred_type: PredType | str = GP, method: TuningMethod | str = MARGLIK, n_steps: int = 100, lr: float = 0.1, init_prior_prec: float | Tensor = 1.0, prior_structure: PriorStructure | str = SCALAR, val_loader: DataLoader | None = None, loss: Metric | Callable[[Tensor], Tensor | float] | None = None, log_prior_prec_min: float = -4, log_prior_prec_max: float = 4, grid_size: int = 100, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, verbose: bool = False, progress_bar: bool = False) -&gt; None\n</code></pre> <p><code>optimize_prior_precision_base</code> from <code>BaseLaplace</code> with <code>pred_type='gp'</code></p> Source code in <code>laplace/baselaplace.py</code> <pre><code>def optimize_prior_precision(\n    self,\n    pred_type: PredType | str = PredType.GP,\n    method: TuningMethod | str = TuningMethod.MARGLIK,\n    n_steps: int = 100,\n    lr: float = 1e-1,\n    init_prior_prec: float | torch.Tensor = 1.0,\n    prior_structure: PriorStructure | str = PriorStructure.SCALAR,\n    val_loader: DataLoader | None = None,\n    loss: torchmetrics.Metric\n    | Callable[[torch.Tensor], torch.Tensor | float]\n    | None = None,\n    log_prior_prec_min: float = -4,\n    log_prior_prec_max: float = 4,\n    grid_size: int = 100,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    verbose: bool = False,\n    progress_bar: bool = False,\n) -&gt; None:\n    \"\"\"`optimize_prior_precision_base` from `BaseLaplace` with `pred_type='gp'`\"\"\"\n    assert pred_type == PredType.GP  # only gp supported\n    assert prior_structure == \"scalar\"  # only isotropic gaussian prior supported\n    if method == \"marglik\":\n        warnings.warn(\n            \"Use of method='marglik' in case of FunctionalLaplace is discouraged, rather use method='CV'.\"\n        )\n    super().optimize_prior_precision(\n        pred_type,\n        method,\n        n_steps,\n        lr,\n        init_prior_prec,\n        prior_structure,\n        val_loader,\n        loss,\n        log_prior_prec_min,\n        log_prior_prec_max,\n        grid_size,\n        link_approx,\n        n_samples,\n        verbose,\n        progress_bar,\n    )\n    self._build_Sigma_inv()\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._kernel_batch","title":"_kernel_batch","text":"<pre><code>_kernel_batch(jacobians: Tensor, batch: Tensor) -&gt; Tensor\n</code></pre> <p>Compute K_bb, which is part of K_MM kernel matrix.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>kernel</code> (              <code>tensor</code> )          \u2013            <p>K_bb with shape (b * C, b * C)</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _kernel_batch(\n    self, jacobians: torch.Tensor, batch: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Compute K_bb, which is part of K_MM kernel matrix.\n\n    Parameters\n    ----------\n    jacobians : torch.Tensor (b, C, P)\n    batch : torch.Tensor (b, C)\n\n    Returns\n    -------\n    kernel : torch.tensor\n        K_bb with shape (b * C, b * C)\n    \"\"\"\n    jacobians_2, _ = self._jacobians(batch)\n    P = jacobians.shape[-1]  # nr model params\n    if self.independent_outputs:\n        kernel = torch.empty(\n            (jacobians.shape[0], jacobians_2.shape[0], self.n_outputs),\n            device=jacobians.device,\n            dtype=self._dtype,\n        )\n        for c in range(self.n_outputs):\n            kernel[:, :, c] = torch.einsum(\n                \"bp,ep-&gt;be\", jacobians[:, c, :], jacobians_2[:, c, :]\n            )\n    else:\n        kernel = torch.einsum(\n            \"ap,bp-&gt;ab\", jacobians.reshape(-1, P), jacobians_2.reshape(-1, P)\n        )\n    del jacobians_2\n    return kernel\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._kernel_batch(jacobians)","title":"<code>jacobians</code>","text":"(<code>Tensor(b, C, P)</code>)           \u2013"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._kernel_batch(batch)","title":"<code>batch</code>","text":"(<code>Tensor(b, C)</code>)           \u2013"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._kernel_star","title":"_kernel_star","text":"<pre><code>_kernel_star(jacobians: Tensor, joint: bool = False) -&gt; Tensor\n</code></pre> <p>Compute K_star_star kernel matrix.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>kernel</code> (              <code>tensor</code> )          \u2013            <p>K_star with shape (b, C, C)</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _kernel_star(\n    self, jacobians: torch.Tensor, joint: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Compute K_star_star kernel matrix.\n\n    Parameters\n    ----------\n    jacobians : torch.Tensor (b, C, P)\n\n    Returns\n    -------\n    kernel : torch.tensor\n        K_star with shape (b, C, C)\n\n    \"\"\"\n    if joint:\n        if self.independent_outputs:\n            kernel = torch.einsum(\"acp,bcp-&gt;abcc\", jacobians, jacobians)\n        else:\n            kernel = torch.einsum(\"acp,bep-&gt;abce\", jacobians, jacobians)\n\n    else:\n        if self.independent_outputs:\n            kernel = torch.empty(\n                (jacobians.shape[0], self.n_outputs),\n                device=jacobians.device,\n                dtype=self._dtype,\n            )\n            for c in range(self.n_outputs):\n                kernel[:, c] = torch.norm(jacobians[:, c, :], dim=1) ** 2\n        else:\n            kernel = torch.einsum(\"bcp,bep-&gt;bce\", jacobians, jacobians)\n    return kernel\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._kernel_star(jacobians)","title":"<code>jacobians</code>","text":"(<code>Tensor(b, C, P)</code>)           \u2013"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._kernel_batch_star","title":"_kernel_batch_star","text":"<pre><code>_kernel_batch_star(jacobians: Tensor, batch: Tensor) -&gt; Tensor\n</code></pre> <p>Compute K_b_star, which is a part of K_M_star kernel matrix.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>kernel</code> (              <code>tensor</code> )          \u2013            <p>K_batch_star with shape (b1, b2, C, C)</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _kernel_batch_star(\n    self, jacobians: torch.Tensor, batch: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Compute K_b_star, which is a part of K_M_star kernel matrix.\n\n    Parameters\n    ----------\n    jacobians : torch.Tensor (b1, C, P)\n    batch : torch.Tensor (b2, C)\n\n    Returns\n    -------\n    kernel : torch.tensor\n        K_batch_star with shape (b1, b2, C, C)\n    \"\"\"\n    jacobians_2, _ = self._jacobians(batch)\n    if self.independent_outputs:\n        kernel = torch.empty(\n            (jacobians.shape[0], jacobians_2.shape[0], self.n_outputs),\n            device=jacobians.device,\n            dtype=self._dtype,\n        )\n        for c in range(self.n_outputs):\n            kernel[:, :, c] = torch.einsum(\n                \"bp,ep-&gt;be\", jacobians[:, c, :], jacobians_2[:, c, :]\n            )\n    else:\n        kernel = torch.einsum(\"bcp,dep-&gt;bdce\", jacobians, jacobians_2)\n    return kernel\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._kernel_batch_star(jacobians)","title":"<code>jacobians</code>","text":"(<code>Tensor(b1, C, P)</code>)           \u2013"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._kernel_batch_star(batch)","title":"<code>batch</code>","text":"(<code>Tensor(b2, C)</code>)           \u2013"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._jacobians","title":"_jacobians","text":"<pre><code>_jacobians(X: Tensor, enable_backprop: bool = None) -&gt; tuple\n</code></pre> <p>A wrapper function to compute jacobians - this enables reusing same kernel methods (kernel_batch etc.) in FunctionalLaplace and FunctionalLLLaplace by simply overwriting this method instead of all kernel methods.</p> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _jacobians(self, X: torch.Tensor, enable_backprop: bool = None) -&gt; tuple:\n    \"\"\"A wrapper function to compute jacobians - this enables reusing same\n    kernel methods (kernel_batch etc.) in FunctionalLaplace and FunctionalLLLaplace\n    by simply overwriting this method instead of all kernel methods.\n    \"\"\"\n    if enable_backprop is None:\n        enable_backprop = self.enable_backprop\n    return self.backend.jacobians(X, enable_backprop=enable_backprop)\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._mean_scatter_term_batch","title":"_mean_scatter_term_batch","text":"<pre><code>_mean_scatter_term_batch(Js: Tensor, f: Tensor, y: Tensor)\n</code></pre> <p>Compute mean vector in the scatter term in the log marginal likelihood</p> <p>See <code>scatter_lml</code> property above for the exact equations of mean vectors in scatter terms for both types of likelihood (regression, classification).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>mu</code> (              <code>tensor</code> )          \u2013            <p>K_batch_star with shape (batch, output_shape)</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _mean_scatter_term_batch(\n    self, Js: torch.Tensor, f: torch.Tensor, y: torch.Tensor\n):\n    \"\"\"Compute mean vector in the scatter term in the log marginal likelihood\n\n    See `scatter_lml` property above for the exact equations of mean vectors in scatter terms for\n    both types of likelihood (regression, classification).\n\n    Parameters\n    ----------\n    Js : torch.tensor\n          Jacobians (batch, output_shape, parameters)\n    f : torch.tensor\n          NN output (batch, output_shape)\n    y: torch.tensor\n          data labels (batch, output_shape)\n\n    Returns\n    -------\n    mu : torch.tensor\n        K_batch_star with shape (batch, output_shape)\n    \"\"\"\n    if self.likelihood == Likelihood.REGRESSION:\n        return y - (f + torch.einsum(\"bcp,p-&gt;bc\", Js, self.prior_mean - self.mean))\n    elif self.likelihood == Likelihood.CLASSIFICATION:\n        return -torch.einsum(\"bcp,p-&gt;bc\", Js, self.prior_mean - self.mean)\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._mean_scatter_term_batch(Js)","title":"<code>Js</code>","text":"(<code>tensor</code>)           \u2013            <p>Jacobians (batch, output_shape, parameters)</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._mean_scatter_term_batch(f)","title":"<code>f</code>","text":"(<code>tensor</code>)           \u2013            <p>NN output (batch, output_shape)</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace._mean_scatter_term_batch(y)","title":"<code>y</code>","text":"(<code>Tensor</code>)           \u2013            <p>data labels (batch, output_shape)</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.log_marginal_likelihood","title":"log_marginal_likelihood","text":"<pre><code>log_marginal_likelihood(prior_precision: Tensor | None = None, sigma_noise: Tensor | None = None) -&gt; Tensor\n</code></pre> <p>Compute the Laplace approximation to the log marginal likelihood. Requires that the Laplace approximation has been fit before. The resulting torch.Tensor is differentiable in <code>prior_precision</code> and <code>sigma_noise</code> if these have gradients enabled. By passing <code>prior_precision</code> or <code>sigma_noise</code>, the current value is overwritten. This is useful for iterating on the log marginal likelihood.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>log_marglik</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def log_marginal_likelihood(\n    self,\n    prior_precision: torch.Tensor | None = None,\n    sigma_noise: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the Laplace approximation to the log marginal likelihood.\n    Requires that the Laplace approximation has been fit before.\n    The resulting torch.Tensor is differentiable in `prior_precision` and\n    `sigma_noise` if these have gradients enabled.\n    By passing `prior_precision` or `sigma_noise`, the current value is\n    overwritten. This is useful for iterating on the log marginal likelihood.\n\n    Parameters\n    ----------\n    prior_precision : torch.Tensor, optional\n        prior precision if should be changed from current `prior_precision` value\n    sigma_noise : torch.Tensor, optional\n        observation noise standard deviation if should be changed\n\n    Returns\n    -------\n    log_marglik : torch.Tensor\n    \"\"\"\n    # update prior precision (useful when iterating on marglik)\n    if prior_precision is not None:\n        self.prior_precision = prior_precision\n\n    # update sigma_noise (useful when iterating on marglik)\n    if sigma_noise is not None:\n        if self.likelihood != Likelihood.REGRESSION:\n            raise ValueError(\"Can only change sigma_noise for regression.\")\n        self.sigma_noise = sigma_noise\n\n    return self.log_likelihood - 0.5 * (self.log_det_ratio + self.scatter)\n</code></pre>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.log_marginal_likelihood(prior_precision)","title":"<code>prior_precision</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>prior precision if should be changed from current <code>prior_precision</code> value</p>"},{"location":"api_reference/functionallaplace/#laplace.baselaplace.FunctionalLaplace.log_marginal_likelihood(sigma_noise)","title":"<code>sigma_noise</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>observation noise standard deviation if should be changed</p>"},{"location":"api_reference/laplace/","title":"Laplace Frontend","text":""},{"location":"api_reference/laplace/#laplace.laplace","title":"laplace.laplace","text":"<p>Functions:</p> <ul> <li> <code>Laplace</code>             \u2013              <p>Simplified Laplace access using strings instead of different classes.</p> </li> </ul>"},{"location":"api_reference/laplace/#laplace.laplace.Laplace","title":"Laplace","text":"<pre><code>Laplace(model: Module, likelihood: Likelihood | str, subset_of_weights: SubsetOfWeights | str = LAST_LAYER, hessian_structure: HessianStructure | str = KRON, *args, **kwargs) -&gt; BaseLaplace\n</code></pre> <p>Simplified Laplace access using strings instead of different classes.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>laplace</code> (              <code>BaseLaplace</code> )          \u2013            <p>chosen subclass of BaseLaplace instantiated with additional arguments</p> </li> </ul> Source code in <code>laplace/laplace.py</code> <pre><code>def Laplace(\n    model: torch.nn.Module,\n    likelihood: Likelihood | str,\n    subset_of_weights: SubsetOfWeights | str = SubsetOfWeights.LAST_LAYER,\n    hessian_structure: HessianStructure | str = HessianStructure.KRON,\n    *args,\n    **kwargs,\n) -&gt; BaseLaplace:\n    \"\"\"Simplified Laplace access using strings instead of different classes.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n    likelihood : Likelihood or str in {'classification', 'regression'}\n    subset_of_weights : SubsetofWeights or {'last_layer', 'subnetwork', 'all'}, default=SubsetOfWeights.LAST_LAYER\n        subset of weights to consider for inference\n    hessian_structure : HessianStructure or str in {'diag', 'kron', 'full', 'lowrank', 'gp'}, default=HessianStructure.KRON\n        structure of the Hessian approximation (note that in case of 'gp',\n        we are not actually doing any Hessian approximation, the inference is instead done in the functional space)\n    Returns\n    -------\n    laplace : BaseLaplace\n        chosen subclass of BaseLaplace instantiated with additional arguments\n    \"\"\"\n    if subset_of_weights == \"subnetwork\" and hessian_structure not in [\"full\", \"diag\"]:\n        raise ValueError(\n            \"Subnetwork Laplace requires a full or diagonal Hessian approximation!\"\n        )\n    laplace_map = {\n        subclass._key: subclass\n        for subclass in _all_subclasses(BaseLaplace)\n        if hasattr(subclass, \"_key\")\n    }\n    laplace_class = laplace_map[(subset_of_weights, hessian_structure)]\n    return laplace_class(model, likelihood, *args, **kwargs)\n</code></pre>"},{"location":"api_reference/laplace/#laplace.laplace.Laplace(model)","title":"<code>model</code>","text":"(<code>Module</code>)           \u2013"},{"location":"api_reference/laplace/#laplace.laplace.Laplace(likelihood)","title":"<code>likelihood</code>","text":"(<code>Likelihood or str in {'classification', 'regression'}</code>)           \u2013"},{"location":"api_reference/laplace/#laplace.laplace.Laplace(subset_of_weights)","title":"<code>subset_of_weights</code>","text":"(<code>SubsetofWeights or {'last_layer', 'subnetwork', 'all'}</code>, default:                   <code>SubsetOfWeights.LAST_LAYER</code> )           \u2013            <p>subset of weights to consider for inference</p>"},{"location":"api_reference/laplace/#laplace.laplace.Laplace(hessian_structure)","title":"<code>hessian_structure</code>","text":"(<code>HessianStructure or str in {'diag', 'kron', 'full', 'lowrank', 'gp'}</code>, default:                   <code>HessianStructure.KRON</code> )           \u2013            <p>structure of the Hessian approximation (note that in case of 'gp', we are not actually doing any Hessian approximation, the inference is instead done in the functional space)</p>"},{"location":"api_reference/lllaplace/","title":"Last-Layer Laplace","text":""},{"location":"api_reference/lllaplace/#laplace.lllaplace","title":"laplace.lllaplace","text":"<p>Classes:</p> <ul> <li> <code>LLLaplace</code>           \u2013            <p>Baseclass for all last-layer Laplace approximations in this library.</p> </li> <li> <code>DiagLLLaplace</code>           \u2013            <p>Last-layer Laplace approximation with diagonal log likelihood Hessian approximation</p> </li> <li> <code>KronLLLaplace</code>           \u2013            <p>Last-layer Laplace approximation with Kronecker factored log likelihood Hessian approximation</p> </li> <li> <code>FullLLLaplace</code>           \u2013            <p>Last-layer Laplace approximation with full, i.e., dense, log likelihood Hessian approximation</p> </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace","title":"LLLaplace","text":"<pre><code>LLLaplace(model: Module, likelihood: Likelihood | str, sigma_noise: float | Tensor = 1.0, prior_precision: float | Tensor = 1.0, prior_mean: float | Tensor = 0.0, temperature: float = 1.0, enable_backprop: bool = False, feature_reduction: FeatureReduction | str | None = None, dict_key_x: str = 'input_ids', dict_key_y: str = 'labels', backend: type[CurvatureInterface] | None = None, last_layer_name: str | None = None, backend_kwargs: dict[str, Any] | None = None, asdl_fisher_kwargs: dict[str, Any] | None = None)\n</code></pre> <p>               Bases: <code>ParametricLaplace</code></p> <p>Baseclass for all last-layer Laplace approximations in this library. Subclasses specify the structure of the Hessian approximation. See <code>BaseLaplace</code> for the full interface.</p> <p>A Laplace approximation is represented by a MAP which is given by the <code>model</code> parameter and a posterior precision or covariance specifying a Gaussian distribution \\(\\mathcal{N}(\\theta_{MAP}, P^{-1})\\). Here, only the parameters of the last layer of the neural network are treated probabilistically. The goal of this class is to compute the posterior precision \\(P\\) which sums as</p> \\[     P = \\sum_{n=1}^N \\nabla^2_\\theta \\log p(\\mathcal{D}_n \\mid \\theta)     \\vert_{\\theta_{MAP}} + \\nabla^2_\\theta \\log p(\\theta) \\vert_{\\theta_{MAP}}. \\] <p>Every subclass implements different approximations to the log likelihood Hessians, for example, a diagonal one. The prior is assumed to be Gaussian and therefore we have a simple form for \\(\\nabla^2_\\theta \\log p(\\theta) \\vert_{\\theta_{MAP}} = P_0 \\). In particular, we assume a scalar or diagonal prior precision so that in all cases \\(P_0 = \\textrm{diag}(p_0)\\) and the structure of \\(p_0\\) can be varied.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>log_marginal_likelihood</code>             \u2013              <p>Compute the Laplace approximation to the log marginal likelihood subject</p> </li> <li> <code>__call__</code>             \u2013              <p>Compute the posterior predictive on input data <code>x</code>.</p> </li> <li> <code>square_norm</code>             \u2013              <p>Compute the square norm under post. Precision with <code>value-self.mean</code> as \ud835\udee5:</p> </li> <li> <code>log_prob</code>             \u2013              <p>Compute the log probability under the (current) Laplace approximation.</p> </li> <li> <code>functional_samples</code>             \u2013              <p>Sample from the function-space posterior on input data <code>x</code>.</p> </li> <li> <code>predictive_samples</code>             \u2013              <p>Sample from the posterior predictive on input data <code>x</code>. I.e., the respective</p> </li> <li> <code>functional_variance</code>             \u2013              <p>Compute functional variance for the <code>'glm'</code> predictive:</p> </li> <li> <code>functional_covariance</code>             \u2013              <p>Compute functional covariance for the <code>'glm'</code> predictive:</p> </li> <li> <code>sample</code>             \u2013              <p>Sample from the Laplace posterior approximation, i.e.,</p> </li> <li> <code>fit</code>             \u2013              <p>Fit the local Laplace approximation at the parameters of the model.</p> </li> <li> <code>functional_variance_fast</code>             \u2013              <p>Should be overriden if there exists a trick to make this fast!</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>log_likelihood</code>               (<code>Tensor</code>)           \u2013            <p>Compute log likelihood on the training data after <code>.fit()</code> has been called.</p> </li> <li> <code>scatter</code>               (<code>Tensor</code>)           \u2013            <p>Computes the scatter, a term of the log marginal likelihood that</p> </li> <li> <code>log_det_prior_precision</code>               (<code>Tensor</code>)           \u2013            <p>Compute log determinant of the prior precision</p> </li> <li> <code>log_det_posterior_precision</code>               (<code>Tensor</code>)           \u2013            <p>Compute log determinant of the posterior precision</p> </li> <li> <code>log_det_ratio</code>               (<code>Tensor</code>)           \u2013            <p>Compute the log determinant ratio, a part of the log marginal likelihood.</p> </li> <li> <code>posterior_precision</code>               (<code>Tensor</code>)           \u2013            <p>Compute or return the posterior precision \\(P\\).</p> </li> <li> <code>prior_precision_diag</code>               (<code>Tensor</code>)           \u2013            <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either</p> </li> </ul> Source code in <code>laplace/lllaplace.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    sigma_noise: float | torch.Tensor = 1.0,\n    prior_precision: float | torch.Tensor = 1.0,\n    prior_mean: float | torch.Tensor = 0.0,\n    temperature: float = 1.0,\n    enable_backprop: bool = False,\n    feature_reduction: FeatureReduction | str | None = None,\n    dict_key_x: str = \"input_ids\",\n    dict_key_y: str = \"labels\",\n    backend: type[CurvatureInterface] | None = None,\n    last_layer_name: str | None = None,\n    backend_kwargs: dict[str, Any] | None = None,\n    asdl_fisher_kwargs: dict[str, Any] | None = None,\n):\n    if asdl_fisher_kwargs is not None:\n        raise ValueError(\"Last-layer Laplace does not support asdl_fisher_kwargs.\")\n\n    self.H = None\n    super().__init__(\n        model,\n        likelihood,\n        sigma_noise=sigma_noise,\n        prior_precision=1.0,\n        prior_mean=0.0,\n        temperature=temperature,\n        enable_backprop=enable_backprop,\n        dict_key_x=dict_key_x,\n        dict_key_y=dict_key_y,\n        backend=backend,\n        backend_kwargs=backend_kwargs,\n    )\n    self.model = FeatureExtractor(\n        deepcopy(model),\n        last_layer_name=last_layer_name,\n        enable_backprop=enable_backprop,\n        feature_reduction=feature_reduction,\n    )\n\n    if self.model.last_layer is None:\n        self.mean: torch.Tensor | None = None\n        self.n_params: int | None = None\n        self.n_layers: int | None = None\n        # ignore checks of prior mean setter temporarily, check on .fit()\n        self._prior_precision: float | torch.Tensor = prior_precision\n        self._prior_mean: float | torch.Tensor = prior_mean\n    else:\n        self.n_params: int = len(\n            parameters_to_vector(self.model.last_layer.parameters())\n        )\n        self.n_layers: int | None = len(list(self.model.last_layer.parameters()))\n        self.prior_precision: float | torch.Tensor = prior_precision\n        self.prior_mean: float | torch.Tensor = prior_mean\n        self.mean: float | torch.Tensor = self.prior_mean\n        self._init_H()\n\n    self._backend_kwargs[\"last_layer\"] = True\n    self._last_layer_name: str | None = last_layer_name\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace(model)","title":"<code>model</code>","text":"(<code>torch.nn.Module or `laplace.utils.feature_extractor.FeatureExtractor`</code>)           \u2013"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace(likelihood)","title":"<code>likelihood</code>","text":"(<code>Likelihood or {'classification', 'regression'}</code>)           \u2013            <p>determines the log likelihood Hessian approximation</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace(sigma_noise)","title":"<code>sigma_noise</code>","text":"(<code>Tensor or float</code>, default:                   <code>1</code> )           \u2013            <p>observation noise for the regression setting; must be 1 for classification</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace(prior_precision)","title":"<code>prior_precision</code>","text":"(<code>Tensor or float</code>, default:                   <code>1</code> )           \u2013            <p>prior precision of a Gaussian prior (= weight decay); can be scalar, per-layer, or diagonal in the most general case</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace(prior_mean)","title":"<code>prior_mean</code>","text":"(<code>Tensor or float</code>, default:                   <code>0</code> )           \u2013            <p>prior mean of a Gaussian prior, useful for continual learning</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace(temperature)","title":"<code>temperature</code>","text":"(<code>float</code>, default:                   <code>1</code> )           \u2013            <p>temperature of the likelihood; lower temperature leads to more concentrated posterior and vice versa.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to enable backprop to the input <code>x</code> through the Laplace predictive. Useful for e.g. Bayesian optimization.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace(feature_reduction)","title":"<code>feature_reduction</code>","text":"(<code>FeatureReduction | str | None</code>, default:                   <code>None</code> )           \u2013            <p>when the last-layer <code>features</code> is a tensor of dim &gt;= 3, this tells how to reduce it into a dim-2 tensor. E.g. in LLMs for non-language modeling problems, the penultultimate output is a tensor of shape <code>(batch_size, seq_len, embd_dim)</code>. But the last layer maps <code>(batch_size, embd_dim)</code> to <code>(batch_size, n_classes)</code>. Note: Make sure that this option faithfully reflects the reduction in the model definition. When inputting a string, available options are <code>{'pick_first', 'pick_last', 'average'}</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace(dict_key_x)","title":"<code>dict_key_x</code>","text":"(<code>str</code>, default:                   <code>'input_ids'</code> )           \u2013            <p>The dictionary key under which the input tensor <code>x</code> is stored. Only has effect when the model takes a <code>MutableMapping</code> as the input. Useful for Huggingface LLM models.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace(dict_key_y)","title":"<code>dict_key_y</code>","text":"(<code>str</code>, default:                   <code>'labels'</code> )           \u2013            <p>The dictionary key under which the target tensor <code>y</code> is stored. Only has effect when the model takes a <code>MutableMapping</code> as the input. Useful for Huggingface LLM models.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace(backend)","title":"<code>backend</code>","text":"(<code>subclasses of `laplace.curvature.CurvatureInterface`</code>, default:                   <code>None</code> )           \u2013            <p>backend for access to curvature/Hessian approximations</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace(last_layer_name)","title":"<code>last_layer_name</code>","text":"(<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>name of the model's last layer, if None it will be determined automatically</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace(backend_kwargs)","title":"<code>backend_kwargs</code>","text":"(<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>arguments passed to the backend on initialization, for example to set the number of MC samples for stochastic approximations.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.log_likelihood","title":"log_likelihood","text":"<pre><code>log_likelihood: Tensor\n</code></pre> <p>Compute log likelihood on the training data after <code>.fit()</code> has been called. The log likelihood is computed on-demand based on the loss and, for example, the observation noise which makes it differentiable in the latter for iterative updates.</p> <p>Returns:</p> <ul> <li> <code>log_likelihood</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.scatter","title":"scatter","text":"<pre><code>scatter: Tensor\n</code></pre> <p>Computes the scatter, a term of the log marginal likelihood that corresponds to L-2 regularization: <code>scatter</code> = \\((\\theta_{MAP} - \\mu_0)^{T} P_0 (\\theta_{MAP} - \\mu_0) \\).</p> <p>Returns:</p> <ul> <li> <code>scatter</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.log_det_prior_precision","title":"log_det_prior_precision","text":"<pre><code>log_det_prior_precision: Tensor\n</code></pre> <p>Compute log determinant of the prior precision \\(\\log \\det P_0\\)</p> <p>Returns:</p> <ul> <li> <code>log_det</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.log_det_posterior_precision","title":"log_det_posterior_precision","text":"<pre><code>log_det_posterior_precision: Tensor\n</code></pre> <p>Compute log determinant of the posterior precision \\(\\log \\det P\\) which depends on the subclasses structure used for the Hessian approximation.</p> <p>Returns:</p> <ul> <li> <code>log_det</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.log_det_ratio","title":"log_det_ratio","text":"<pre><code>log_det_ratio: Tensor\n</code></pre> <p>Compute the log determinant ratio, a part of the log marginal likelihood.</p> \\[     \\log \\frac{\\det P}{\\det P_0} = \\log \\det P - \\log \\det P_0 \\] <p>Returns:</p> <ul> <li> <code>log_det_ratio</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.posterior_precision","title":"posterior_precision","text":"<pre><code>posterior_precision: Tensor\n</code></pre> <p>Compute or return the posterior precision \\(P\\).</p> <p>Returns:</p> <ul> <li> <code>posterior_prec</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.prior_precision_diag","title":"prior_precision_diag","text":"<pre><code>prior_precision_diag: Tensor\n</code></pre> <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either a scalar or diagonal prior precision.</p> <p>Returns:</p> <ul> <li> <code>prior_precision_diag</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.log_marginal_likelihood","title":"log_marginal_likelihood","text":"<pre><code>log_marginal_likelihood(prior_precision: Tensor | None = None, sigma_noise: Tensor | None = None) -&gt; Tensor\n</code></pre> <p>Compute the Laplace approximation to the log marginal likelihood subject to specific Hessian approximations that subclasses implement. Requires that the Laplace approximation has been fit before. The resulting torch.Tensor is differentiable in <code>prior_precision</code> and <code>sigma_noise</code> if these have gradients enabled. By passing <code>prior_precision</code> or <code>sigma_noise</code>, the current value is overwritten. This is useful for iterating on the log marginal likelihood.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>log_marglik</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def log_marginal_likelihood(\n    self,\n    prior_precision: torch.Tensor | None = None,\n    sigma_noise: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the Laplace approximation to the log marginal likelihood subject\n    to specific Hessian approximations that subclasses implement.\n    Requires that the Laplace approximation has been fit before.\n    The resulting torch.Tensor is differentiable in `prior_precision` and\n    `sigma_noise` if these have gradients enabled.\n    By passing `prior_precision` or `sigma_noise`, the current value is\n    overwritten. This is useful for iterating on the log marginal likelihood.\n\n    Parameters\n    ----------\n    prior_precision : torch.Tensor, optional\n        prior precision if should be changed from current `prior_precision` value\n    sigma_noise : torch.Tensor, optional\n        observation noise standard deviation if should be changed\n\n    Returns\n    -------\n    log_marglik : torch.Tensor\n    \"\"\"\n    # update prior precision (useful when iterating on marglik)\n    if prior_precision is not None:\n        self.prior_precision = prior_precision\n\n    # update sigma_noise (useful when iterating on marglik)\n    if sigma_noise is not None:\n        if self.likelihood != Likelihood.REGRESSION:\n            raise ValueError(\"Can only change sigma_noise for regression.\")\n\n        self.sigma_noise = sigma_noise\n\n    return self.log_likelihood - 0.5 * (self.log_det_ratio + self.scatter)\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.log_marginal_likelihood(prior_precision)","title":"<code>prior_precision</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>prior precision if should be changed from current <code>prior_precision</code> value</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.log_marginal_likelihood(sigma_noise)","title":"<code>sigma_noise</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>observation noise standard deviation if should be changed</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.__call__","title":"__call__","text":"<pre><code>__call__(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None, fitting: bool = False, **model_kwargs: dict[str, Any]) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def __call__(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n    fitting: bool = False,\n    **model_kwargs: dict[str, Any],\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x`.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here. When Laplace is done only\n        on subset of parameters (i.e. some grad are disabled),\n        only `nn` predictive is supported.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` when `joint=False` in regression.\n        In the case of last-layer Laplace with a diagonal or Kron Hessian,\n        setting this to `True` makes computation much(!) faster for large\n        number of outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used).\n\n    fitting : bool, default=False\n        whether or not this predictive call is done during fitting. Only useful for\n        reward modeling: the likelihood is set to `\"regression\"` when `False` and\n        `\"classification\"` when `True`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    if pred_type not in [pred for pred in PredType]:\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if link_approx not in [la for la in LinkApprox]:\n        raise ValueError(f\"Unsupported link approximation {link_approx}.\")\n\n    if pred_type == PredType.NN and link_approx != LinkApprox.MC:\n        raise ValueError(\n            \"Only mc link approximation is supported for nn prediction type.\"\n        )\n\n    if generator is not None:\n        if (\n            not isinstance(generator, torch.Generator)\n            or generator.device != self._device\n        ):\n            raise ValueError(\"Invalid random generator (check type and device).\")\n\n    likelihood = self.likelihood\n    if likelihood == Likelihood.REWARD_MODELING:\n        likelihood = Likelihood.CLASSIFICATION if fitting else Likelihood.REGRESSION\n\n    if pred_type == PredType.GLM:\n        return self._glm_forward_call(\n            x, likelihood, joint, link_approx, n_samples, diagonal_output\n        )\n    else:\n        if likelihood == Likelihood.REGRESSION:\n            samples = self._nn_predictive_samples(x, n_samples, **model_kwargs)\n            return samples.mean(dim=0), samples.var(dim=0)\n        else:  # classification; the average is computed online\n            return self._nn_predictive_classification(x, n_samples, **model_kwargs)\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.__call__(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.__call__(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here. When Laplace is done only on subset of parameters (i.e. some grad are disabled), only <code>nn</code> predictive is supported.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.__call__(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.__call__(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.__call__(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.__call__(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> when <code>joint=False</code> in regression. In the case of last-layer Laplace with a diagonal or Kron Hessian, setting this to <code>True</code> makes computation much(!) faster for large number of outputs.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.__call__(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used).</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.__call__(fitting)","title":"<code>fitting</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether or not this predictive call is done during fitting. Only useful for reward modeling: the likelihood is set to <code>\"regression\"</code> when <code>False</code> and <code>\"classification\"</code> when <code>True</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace._glm_forward_call","title":"_glm_forward_call","text":"<pre><code>_glm_forward_call(x: Tensor | MutableMapping, likelihood: Likelihood | str, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code> for \"glm\" pred type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_forward_call(\n    self,\n    x: torch.Tensor | MutableMapping,\n    likelihood: Likelihood | str,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x` for \"glm\" pred type.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    likelihood : Likelihood or str in {'classification', 'regression', 'reward_modeling'}\n        determines the log likelihood Hessian approximation.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` and `link_approx='mc'`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    f_mu, f_var = self._glm_predictive_distribution(\n        x, joint=joint and likelihood == Likelihood.REGRESSION\n    )\n\n    if likelihood == Likelihood.REGRESSION:\n        if diagonal_output and not joint:\n            f_var = torch.diagonal(f_var, dim1=-2, dim2=-1)\n        return f_mu, f_var\n\n    if link_approx == LinkApprox.MC:\n        return self._glm_predictive_samples(\n            f_mu,\n            f_var,\n            n_samples=n_samples,\n            diagonal_output=diagonal_output,\n        ).mean(dim=0)\n    elif link_approx == LinkApprox.PROBIT:\n        kappa = 1 / torch.sqrt(1.0 + np.pi / 8 * f_var.diagonal(dim1=1, dim2=2))\n        return torch.softmax(kappa * f_mu, dim=-1)\n    elif \"bridge\" in link_approx:\n        # zero mean correction\n        f_mu -= (\n            f_var.sum(-1)\n            * f_mu.sum(-1).reshape(-1, 1)\n            / f_var.sum(dim=(1, 2)).reshape(-1, 1)\n        )\n        f_var -= torch.einsum(\n            \"bi,bj-&gt;bij\", f_var.sum(-1), f_var.sum(-2)\n        ) / f_var.sum(dim=(1, 2)).reshape(-1, 1, 1)\n\n        # Laplace Bridge\n        _, K = f_mu.size(0), f_mu.size(-1)\n        f_var_diag = torch.diagonal(f_var, dim1=1, dim2=2)\n\n        # optional: variance correction\n        if link_approx == LinkApprox.BRIDGE_NORM:\n            f_var_diag_mean = f_var_diag.mean(dim=1)\n            f_var_diag_mean /= torch.as_tensor(\n                [K / 2], device=self._device, dtype=self._dtype\n            ).sqrt()\n            f_mu /= f_var_diag_mean.sqrt().unsqueeze(-1)\n            f_var_diag /= f_var_diag_mean.unsqueeze(-1)\n\n        sum_exp = torch.exp(-f_mu).sum(dim=1).unsqueeze(-1)\n        alpha = (1 - 2 / K + f_mu.exp() / K**2 * sum_exp) / f_var_diag\n        return torch.nan_to_num(alpha / alpha.sum(dim=1).unsqueeze(-1), nan=1.0)\n    else:\n        raise ValueError(\n            \"Prediction path invalid. Check the likelihood, pred_type, link_approx combination!\"\n        )\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace._glm_forward_call(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace._glm_forward_call(likelihood)","title":"<code>likelihood</code>","text":"(<code>Likelihood or str in {'classification', 'regression', 'reward_modeling'}</code>)           \u2013            <p>determines the log likelihood Hessian approximation.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace._glm_forward_call(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace._glm_forward_call(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace._glm_forward_call(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace._glm_forward_call(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> and <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace._glm_functional_samples","title":"_glm_functional_samples","text":"<pre><code>_glm_functional_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior functional on input data <code>x</code> using \"glm\" prediction type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_functional_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior functional on input data `x` using \"glm\" prediction\n    type.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    assert f_var.shape == torch.Size([f_mu.shape[0], f_mu.shape[1], f_mu.shape[1]])\n\n    if diagonal_output:\n        f_var = torch.diagonal(f_var, dim1=1, dim2=2)\n\n    return normal_samples(f_mu, f_var, n_samples, generator)\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace._glm_functional_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace._glm_functional_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace._glm_functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace._glm_functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace._glm_functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace._glm_predictive_samples","title":"_glm_predictive_samples","text":"<pre><code>_glm_predictive_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code> using \"glm\" prediction type. I.e., the inverse-link function correponding to the likelihood is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_predictive_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x` using \"glm\" prediction\n    type. I.e., the inverse-link function correponding to the likelihood is applied\n    on top of the functional sample.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    f_samples = self._glm_functional_samples(\n        f_mu, f_var, n_samples, diagonal_output, generator\n    )\n\n    if self.likelihood == Likelihood.REGRESSION:\n        return f_samples\n    else:\n        return torch.softmax(f_samples, dim=-1)\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace._glm_predictive_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace._glm_predictive_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace._glm_predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace._glm_predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace._glm_predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.square_norm","title":"square_norm","text":"<pre><code>square_norm(value) -&gt; Tensor\n</code></pre> <p>Compute the square norm under post. Precision with <code>value-self.mean</code> as \ud835\udee5:</p> \\[     \\Delta^     op P \\Delta \\] <p>Returns:</p> <ul> <li> <code>square_form</code>           \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def square_norm(self, value) -&gt; torch.Tensor:\n    \"\"\"Compute the square norm under post. Precision with `value-self.mean` as \ud835\udee5:\n\n    $$\n        \\\\Delta^\\top P \\\\Delta\n    $$\n\n    Returns\n    -------\n    square_form\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.log_prob","title":"log_prob","text":"<pre><code>log_prob(value: Tensor, normalized: bool = True) -&gt; Tensor\n</code></pre> <p>Compute the log probability under the (current) Laplace approximation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>log_prob</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def log_prob(self, value: torch.Tensor, normalized: bool = True) -&gt; torch.Tensor:\n    \"\"\"Compute the log probability under the (current) Laplace approximation.\n\n    Parameters\n    ----------\n    value: torch.Tensor\n    normalized : bool, default=True\n        whether to return log of a properly normalized Gaussian or just the\n        terms that depend on `value`.\n\n    Returns\n    -------\n    log_prob : torch.Tensor\n    \"\"\"\n    if not normalized:\n        return -self.square_norm(value) / 2\n    log_prob = (\n        -self.n_params / 2 * log(2 * pi) + self.log_det_posterior_precision / 2\n    )\n    log_prob -= self.square_norm(value) / 2\n    return log_prob\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.log_prob(value)","title":"<code>value</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.log_prob(normalized)","title":"<code>normalized</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to return log of a properly normalized Gaussian or just the terms that depend on <code>value</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.functional_samples","title":"functional_samples","text":"<pre><code>functional_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the function-space posterior on input data <code>x</code>. Can be used, for example, for Thompson sampling or to compute an arbitrary expectation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def functional_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the function-space posterior on input data `x`.\n    Can be used, for example, for Thompson sampling or to compute an arbitrary\n    expectation.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if pred_type == PredType.GLM:\n        f_mu, f_var = self._glm_predictive_distribution(x)\n        return self._glm_functional_samples(\n            f_mu, f_var, n_samples, diagonal_output, generator\n        )\n    else:  # 'nn'\n        return self._nn_functional_samples(x, n_samples, generator)\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.functional_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.functional_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.predictive_samples","title":"predictive_samples","text":"<pre><code>predictive_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code>. I.e., the respective inverse-link function (e.g. softmax) is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def predictive_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x`. I.e., the respective\n    inverse-link function (e.g. softmax) is applied on top of the functional\n    sample.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if pred_type == PredType.GLM:\n        f_mu, f_var = self._glm_predictive_distribution(x)\n        return self._glm_predictive_samples(\n            f_mu, f_var, n_samples, diagonal_output, generator\n        )\n    else:  # 'nn'\n        return self._nn_predictive_samples(x, n_samples, generator)\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.predictive_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.predictive_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.functional_variance","title":"functional_variance","text":"<pre><code>functional_variance(Js: Tensor) -&gt; Tensor\n</code></pre> <p>Compute functional variance for the <code>'glm'</code> predictive: <code>f_var[i] = Js[i] @ P.inv() @ Js[i].T</code>, which is a output x output predictive covariance matrix. Mathematically, we have for a single Jacobian \\(\\mathcal{J} = \\nabla_\\theta f(x;\\theta)\\vert_{\\theta_{MAP}}\\) the output covariance matrix \\( \\mathcal{J} P^{-1} \\mathcal{J}^T \\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>f_var</code> (              <code>Tensor</code> )          \u2013            <p>output covariance <code>(batch, outputs, outputs)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def functional_variance(self, Js: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute functional variance for the `'glm'` predictive:\n    `f_var[i] = Js[i] @ P.inv() @ Js[i].T`, which is a output x output\n    predictive covariance matrix.\n    Mathematically, we have for a single Jacobian\n    \\\\(\\\\mathcal{J} = \\\\nabla_\\\\theta f(x;\\\\theta)\\\\vert_{\\\\theta_{MAP}}\\\\)\n    the output covariance matrix\n    \\\\( \\\\mathcal{J} P^{-1} \\\\mathcal{J}^T \\\\).\n\n    Parameters\n    ----------\n    Js : torch.Tensor\n        Jacobians of model output wrt parameters\n        `(batch, outputs, parameters)`\n\n    Returns\n    -------\n    f_var : torch.Tensor\n        output covariance `(batch, outputs, outputs)`\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.functional_variance(Js)","title":"<code>Js</code>","text":"(<code>Tensor</code>)           \u2013            <p>Jacobians of model output wrt parameters <code>(batch, outputs, parameters)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.functional_covariance","title":"functional_covariance","text":"<pre><code>functional_covariance(Js: Tensor) -&gt; Tensor\n</code></pre> <p>Compute functional covariance for the <code>'glm'</code> predictive: <code>f_cov = Js @ P.inv() @ Js.T</code>, which is a batchoutput x batchoutput predictive covariance matrix.</p> <p>This emulates the GP posterior covariance N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). Useful for joint predictions, such as in batched Bayesian optimization.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>f_cov</code> (              <code>Tensor</code> )          \u2013            <p>output covariance <code>(batch*outputs, batch*outputs)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def functional_covariance(self, Js: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute functional covariance for the `'glm'` predictive:\n    `f_cov = Js @ P.inv() @ Js.T`, which is a batch*output x batch*output\n    predictive covariance matrix.\n\n    This emulates the GP posterior covariance N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n    Useful for joint predictions, such as in batched Bayesian optimization.\n\n    Parameters\n    ----------\n    Js : torch.Tensor\n        Jacobians of model output wrt parameters\n        `(batch*outputs, parameters)`\n\n    Returns\n    -------\n    f_cov : torch.Tensor\n        output covariance `(batch*outputs, batch*outputs)`\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.functional_covariance(Js)","title":"<code>Js</code>","text":"(<code>Tensor</code>)           \u2013            <p>Jacobians of model output wrt parameters <code>(batch*outputs, parameters)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.sample","title":"sample","text":"<pre><code>sample(n_samples: int = 100, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the Laplace posterior approximation, i.e., \\( \\theta \\sim \\mathcal{N}(\\theta_{MAP}, P^{-1})\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def sample(\n    self, n_samples: int = 100, generator: torch.Generator | None = None\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the Laplace posterior approximation, i.e.,\n    \\\\( \\\\theta \\\\sim \\\\mathcal{N}(\\\\theta_{MAP}, P^{-1})\\\\).\n\n    Parameters\n    ----------\n    n_samples : int, default=100\n        number of samples\n\n    generator : torch.Generator, optional\n        random number generator to control the samples\n\n    Returns\n    -------\n    samples: torch.Tensor\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.sample(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.sample(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.fit","title":"fit","text":"<pre><code>fit(train_loader: DataLoader, override: bool = True, progress_bar: bool = False) -&gt; None\n</code></pre> <p>Fit the local Laplace approximation at the parameters of the model.</p> <p>Parameters:</p> Source code in <code>laplace/lllaplace.py</code> <pre><code>def fit(\n    self,\n    train_loader: DataLoader,\n    override: bool = True,\n    progress_bar: bool = False,\n) -&gt; None:\n    \"\"\"Fit the local Laplace approximation at the parameters of the model.\n\n    Parameters\n    ----------\n    train_loader : torch.data.utils.DataLoader\n        each iterate is a training batch, either `(X, y)` tensors or a dict-like\n        object containing keys as expressed by `self.dict_key_x` and\n        `self.dict_key_y`. `train_loader.dataset` needs to be set to access\n        \\\\(N\\\\), size of the data set.\n    override : bool, default=True\n        whether to initialize H, loss, and n_data again; setting to False is useful for\n        online learning settings to accumulate a sequential posterior approximation.\n    progress_bar: bool, default=False\n    \"\"\"\n    if not override:\n        raise ValueError(\n            \"Last-layer Laplace approximations do not support `override=False`.\"\n        )\n\n    self.model.eval()\n\n    if self.model.last_layer is None:\n        self.data: tuple[torch.Tensor, torch.Tensor] | MutableMapping = next(\n            iter(train_loader)\n        )\n        self._find_last_layer(self.data)\n        params: torch.Tensor = parameters_to_vector(\n            self.model.last_layer.parameters()\n        ).detach()\n        self.n_params: int = len(params)\n        self.n_layers: int = len(list(self.model.last_layer.parameters()))\n        # here, check the already set prior precision again\n        self.prior_precision: float | torch.Tensor = self._prior_precision\n        self.prior_mean: float | torch.Tensor = self._prior_mean\n        self._init_H()\n\n    super().fit(train_loader, override=override)\n    self.mean: torch.Tensor = parameters_to_vector(\n        self.model.last_layer.parameters()\n    )\n\n    if not self.enable_backprop:\n        self.mean = self.mean.detach()\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.fit(train_loader)","title":"<code>train_loader</code>","text":"(<code>DataLoader</code>)           \u2013            <p>each iterate is a training batch, either <code>(X, y)</code> tensors or a dict-like object containing keys as expressed by <code>self.dict_key_x</code> and <code>self.dict_key_y</code>. <code>train_loader.dataset</code> needs to be set to access \\(N\\), size of the data set.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.fit(override)","title":"<code>override</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to initialize H, loss, and n_data again; setting to False is useful for online learning settings to accumulate a sequential posterior approximation.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.fit(progress_bar)","title":"<code>progress_bar</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.functional_variance_fast","title":"functional_variance_fast","text":"<pre><code>functional_variance_fast(X)\n</code></pre> <p>Should be overriden if there exists a trick to make this fast!</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>f_var_diag</code> (              <code>torch.Tensor of shape (batch_size, num_outputs)</code> )          \u2013            <p>Corresponding to the diagonal of the covariance matrix of the outputs</p> </li> </ul> Source code in <code>laplace/lllaplace.py</code> <pre><code>def functional_variance_fast(self, X):\n    \"\"\"\n    Should be overriden if there exists a trick to make this fast!\n\n    Parameters\n    ----------\n    X: torch.Tensor of shape (batch_size, input_dim)\n\n    Returns\n    -------\n    f_var_diag: torch.Tensor of shape (batch_size, num_outputs)\n        Corresponding to the diagonal of the covariance matrix of the outputs\n    \"\"\"\n    Js, f_mu = self.backend.last_layer_jacobians(X, self.enable_backprop)\n    f_cov = self.functional_variance(Js)  # No trick possible for Full Laplace\n    f_var = torch.diagonal(f_cov, dim1=-2, dim2=-1)\n    return f_mu, f_var\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.LLLaplace.functional_variance_fast(X)","title":"<code>X</code>","text":"\u2013"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace","title":"DiagLLLaplace","text":"<pre><code>DiagLLLaplace(model: Module, likelihood: Likelihood | str, sigma_noise: float | Tensor = 1.0, prior_precision: float | Tensor = 1.0, prior_mean: float | Tensor = 0.0, temperature: float = 1.0, enable_backprop: bool = False, feature_reduction: FeatureReduction | str | None = None, dict_key_x: str = 'input_ids', dict_key_y: str = 'labels', backend: type[CurvatureInterface] | None = None, last_layer_name: str | None = None, backend_kwargs: dict[str, Any] | None = None, asdl_fisher_kwargs: dict[str, Any] | None = None)\n</code></pre> <p>               Bases: <code>LLLaplace</code>, <code>DiagLaplace</code></p> <p>Last-layer Laplace approximation with diagonal log likelihood Hessian approximation and hence posterior precision. Mathematically, we have \\(P \\approx \\textrm{diag}(P)\\). See <code>DiagLaplace</code>, <code>LLLaplace</code>, and <code>BaseLaplace</code> for the full interface.</p> <p>Methods:</p> <ul> <li> <code>fit</code>             \u2013              <p>Fit the local Laplace approximation at the parameters of the model.</p> </li> <li> <code>log_marginal_likelihood</code>             \u2013              <p>Compute the Laplace approximation to the log marginal likelihood subject</p> </li> <li> <code>__call__</code>             \u2013              <p>Compute the posterior predictive on input data <code>x</code>.</p> </li> <li> <code>log_prob</code>             \u2013              <p>Compute the log probability under the (current) Laplace approximation.</p> </li> <li> <code>functional_samples</code>             \u2013              <p>Sample from the function-space posterior on input data <code>x</code>.</p> </li> <li> <code>predictive_samples</code>             \u2013              <p>Sample from the posterior predictive on input data <code>x</code>. I.e., the respective</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>log_likelihood</code>               (<code>Tensor</code>)           \u2013            <p>Compute log likelihood on the training data after <code>.fit()</code> has been called.</p> </li> <li> <code>prior_precision_diag</code>               (<code>Tensor</code>)           \u2013            <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either</p> </li> <li> <code>scatter</code>               (<code>Tensor</code>)           \u2013            <p>Computes the scatter, a term of the log marginal likelihood that</p> </li> <li> <code>log_det_prior_precision</code>               (<code>Tensor</code>)           \u2013            <p>Compute log determinant of the prior precision</p> </li> <li> <code>log_det_ratio</code>               (<code>Tensor</code>)           \u2013            <p>Compute the log determinant ratio, a part of the log marginal likelihood.</p> </li> <li> <code>posterior_precision</code>               (<code>Tensor</code>)           \u2013            <p>Diagonal posterior precision \\(p\\).</p> </li> <li> <code>posterior_scale</code>               (<code>Tensor</code>)           \u2013            <p>Diagonal posterior scale \\(\\sqrt{p^{-1}}\\).</p> </li> <li> <code>posterior_variance</code>               (<code>Tensor</code>)           \u2013            <p>Diagonal posterior variance \\(p^{-1}\\).</p> </li> </ul> Source code in <code>laplace/lllaplace.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    sigma_noise: float | torch.Tensor = 1.0,\n    prior_precision: float | torch.Tensor = 1.0,\n    prior_mean: float | torch.Tensor = 0.0,\n    temperature: float = 1.0,\n    enable_backprop: bool = False,\n    feature_reduction: FeatureReduction | str | None = None,\n    dict_key_x: str = \"input_ids\",\n    dict_key_y: str = \"labels\",\n    backend: type[CurvatureInterface] | None = None,\n    last_layer_name: str | None = None,\n    backend_kwargs: dict[str, Any] | None = None,\n    asdl_fisher_kwargs: dict[str, Any] | None = None,\n):\n    if asdl_fisher_kwargs is not None:\n        raise ValueError(\"Last-layer Laplace does not support asdl_fisher_kwargs.\")\n\n    self.H = None\n    super().__init__(\n        model,\n        likelihood,\n        sigma_noise=sigma_noise,\n        prior_precision=1.0,\n        prior_mean=0.0,\n        temperature=temperature,\n        enable_backprop=enable_backprop,\n        dict_key_x=dict_key_x,\n        dict_key_y=dict_key_y,\n        backend=backend,\n        backend_kwargs=backend_kwargs,\n    )\n    self.model = FeatureExtractor(\n        deepcopy(model),\n        last_layer_name=last_layer_name,\n        enable_backprop=enable_backprop,\n        feature_reduction=feature_reduction,\n    )\n\n    if self.model.last_layer is None:\n        self.mean: torch.Tensor | None = None\n        self.n_params: int | None = None\n        self.n_layers: int | None = None\n        # ignore checks of prior mean setter temporarily, check on .fit()\n        self._prior_precision: float | torch.Tensor = prior_precision\n        self._prior_mean: float | torch.Tensor = prior_mean\n    else:\n        self.n_params: int = len(\n            parameters_to_vector(self.model.last_layer.parameters())\n        )\n        self.n_layers: int | None = len(list(self.model.last_layer.parameters()))\n        self.prior_precision: float | torch.Tensor = prior_precision\n        self.prior_mean: float | torch.Tensor = prior_mean\n        self.mean: float | torch.Tensor = self.prior_mean\n        self._init_H()\n\n    self._backend_kwargs[\"last_layer\"] = True\n    self._last_layer_name: str | None = last_layer_name\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.log_likelihood","title":"log_likelihood","text":"<pre><code>log_likelihood: Tensor\n</code></pre> <p>Compute log likelihood on the training data after <code>.fit()</code> has been called. The log likelihood is computed on-demand based on the loss and, for example, the observation noise which makes it differentiable in the latter for iterative updates.</p> <p>Returns:</p> <ul> <li> <code>log_likelihood</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.prior_precision_diag","title":"prior_precision_diag","text":"<pre><code>prior_precision_diag: Tensor\n</code></pre> <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either a scalar or diagonal prior precision.</p> <p>Returns:</p> <ul> <li> <code>prior_precision_diag</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.scatter","title":"scatter","text":"<pre><code>scatter: Tensor\n</code></pre> <p>Computes the scatter, a term of the log marginal likelihood that corresponds to L-2 regularization: <code>scatter</code> = \\((\\theta_{MAP} - \\mu_0)^{T} P_0 (\\theta_{MAP} - \\mu_0) \\).</p> <p>Returns:</p> <ul> <li> <code>scatter</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.log_det_prior_precision","title":"log_det_prior_precision","text":"<pre><code>log_det_prior_precision: Tensor\n</code></pre> <p>Compute log determinant of the prior precision \\(\\log \\det P_0\\)</p> <p>Returns:</p> <ul> <li> <code>log_det</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.log_det_ratio","title":"log_det_ratio","text":"<pre><code>log_det_ratio: Tensor\n</code></pre> <p>Compute the log determinant ratio, a part of the log marginal likelihood.</p> \\[     \\log \\frac{\\det P}{\\det P_0} = \\log \\det P - \\log \\det P_0 \\] <p>Returns:</p> <ul> <li> <code>log_det_ratio</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.posterior_precision","title":"posterior_precision","text":"<pre><code>posterior_precision: Tensor\n</code></pre> <p>Diagonal posterior precision \\(p\\).</p> <p>Returns:</p> <ul> <li> <code>precision</code> (              <code>tensor</code> )          \u2013            <p><code>(parameters)</code></p> </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.posterior_scale","title":"posterior_scale","text":"<pre><code>posterior_scale: Tensor\n</code></pre> <p>Diagonal posterior scale \\(\\sqrt{p^{-1}}\\).</p> <p>Returns:</p> <ul> <li> <code>precision</code> (              <code>tensor</code> )          \u2013            <p><code>(parameters)</code></p> </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.posterior_variance","title":"posterior_variance","text":"<pre><code>posterior_variance: Tensor\n</code></pre> <p>Diagonal posterior variance \\(p^{-1}\\).</p> <p>Returns:</p> <ul> <li> <code>precision</code> (              <code>tensor</code> )          \u2013            <p><code>(parameters)</code></p> </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.fit","title":"fit","text":"<pre><code>fit(train_loader: DataLoader, override: bool = True, progress_bar: bool = False) -&gt; None\n</code></pre> <p>Fit the local Laplace approximation at the parameters of the model.</p> <p>Parameters:</p> Source code in <code>laplace/lllaplace.py</code> <pre><code>def fit(\n    self,\n    train_loader: DataLoader,\n    override: bool = True,\n    progress_bar: bool = False,\n) -&gt; None:\n    \"\"\"Fit the local Laplace approximation at the parameters of the model.\n\n    Parameters\n    ----------\n    train_loader : torch.data.utils.DataLoader\n        each iterate is a training batch, either `(X, y)` tensors or a dict-like\n        object containing keys as expressed by `self.dict_key_x` and\n        `self.dict_key_y`. `train_loader.dataset` needs to be set to access\n        \\\\(N\\\\), size of the data set.\n    override : bool, default=True\n        whether to initialize H, loss, and n_data again; setting to False is useful for\n        online learning settings to accumulate a sequential posterior approximation.\n    progress_bar: bool, default=False\n    \"\"\"\n    if not override:\n        raise ValueError(\n            \"Last-layer Laplace approximations do not support `override=False`.\"\n        )\n\n    self.model.eval()\n\n    if self.model.last_layer is None:\n        self.data: tuple[torch.Tensor, torch.Tensor] | MutableMapping = next(\n            iter(train_loader)\n        )\n        self._find_last_layer(self.data)\n        params: torch.Tensor = parameters_to_vector(\n            self.model.last_layer.parameters()\n        ).detach()\n        self.n_params: int = len(params)\n        self.n_layers: int = len(list(self.model.last_layer.parameters()))\n        # here, check the already set prior precision again\n        self.prior_precision: float | torch.Tensor = self._prior_precision\n        self.prior_mean: float | torch.Tensor = self._prior_mean\n        self._init_H()\n\n    super().fit(train_loader, override=override)\n    self.mean: torch.Tensor = parameters_to_vector(\n        self.model.last_layer.parameters()\n    )\n\n    if not self.enable_backprop:\n        self.mean = self.mean.detach()\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.fit(train_loader)","title":"<code>train_loader</code>","text":"(<code>DataLoader</code>)           \u2013            <p>each iterate is a training batch, either <code>(X, y)</code> tensors or a dict-like object containing keys as expressed by <code>self.dict_key_x</code> and <code>self.dict_key_y</code>. <code>train_loader.dataset</code> needs to be set to access \\(N\\), size of the data set.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.fit(override)","title":"<code>override</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to initialize H, loss, and n_data again; setting to False is useful for online learning settings to accumulate a sequential posterior approximation.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.fit(progress_bar)","title":"<code>progress_bar</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.log_marginal_likelihood","title":"log_marginal_likelihood","text":"<pre><code>log_marginal_likelihood(prior_precision: Tensor | None = None, sigma_noise: Tensor | None = None) -&gt; Tensor\n</code></pre> <p>Compute the Laplace approximation to the log marginal likelihood subject to specific Hessian approximations that subclasses implement. Requires that the Laplace approximation has been fit before. The resulting torch.Tensor is differentiable in <code>prior_precision</code> and <code>sigma_noise</code> if these have gradients enabled. By passing <code>prior_precision</code> or <code>sigma_noise</code>, the current value is overwritten. This is useful for iterating on the log marginal likelihood.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>log_marglik</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def log_marginal_likelihood(\n    self,\n    prior_precision: torch.Tensor | None = None,\n    sigma_noise: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the Laplace approximation to the log marginal likelihood subject\n    to specific Hessian approximations that subclasses implement.\n    Requires that the Laplace approximation has been fit before.\n    The resulting torch.Tensor is differentiable in `prior_precision` and\n    `sigma_noise` if these have gradients enabled.\n    By passing `prior_precision` or `sigma_noise`, the current value is\n    overwritten. This is useful for iterating on the log marginal likelihood.\n\n    Parameters\n    ----------\n    prior_precision : torch.Tensor, optional\n        prior precision if should be changed from current `prior_precision` value\n    sigma_noise : torch.Tensor, optional\n        observation noise standard deviation if should be changed\n\n    Returns\n    -------\n    log_marglik : torch.Tensor\n    \"\"\"\n    # update prior precision (useful when iterating on marglik)\n    if prior_precision is not None:\n        self.prior_precision = prior_precision\n\n    # update sigma_noise (useful when iterating on marglik)\n    if sigma_noise is not None:\n        if self.likelihood != Likelihood.REGRESSION:\n            raise ValueError(\"Can only change sigma_noise for regression.\")\n\n        self.sigma_noise = sigma_noise\n\n    return self.log_likelihood - 0.5 * (self.log_det_ratio + self.scatter)\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.log_marginal_likelihood(prior_precision)","title":"<code>prior_precision</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>prior precision if should be changed from current <code>prior_precision</code> value</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.log_marginal_likelihood(sigma_noise)","title":"<code>sigma_noise</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>observation noise standard deviation if should be changed</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.__call__","title":"__call__","text":"<pre><code>__call__(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None, fitting: bool = False, **model_kwargs: dict[str, Any]) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def __call__(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n    fitting: bool = False,\n    **model_kwargs: dict[str, Any],\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x`.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here. When Laplace is done only\n        on subset of parameters (i.e. some grad are disabled),\n        only `nn` predictive is supported.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` when `joint=False` in regression.\n        In the case of last-layer Laplace with a diagonal or Kron Hessian,\n        setting this to `True` makes computation much(!) faster for large\n        number of outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used).\n\n    fitting : bool, default=False\n        whether or not this predictive call is done during fitting. Only useful for\n        reward modeling: the likelihood is set to `\"regression\"` when `False` and\n        `\"classification\"` when `True`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    if pred_type not in [pred for pred in PredType]:\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if link_approx not in [la for la in LinkApprox]:\n        raise ValueError(f\"Unsupported link approximation {link_approx}.\")\n\n    if pred_type == PredType.NN and link_approx != LinkApprox.MC:\n        raise ValueError(\n            \"Only mc link approximation is supported for nn prediction type.\"\n        )\n\n    if generator is not None:\n        if (\n            not isinstance(generator, torch.Generator)\n            or generator.device != self._device\n        ):\n            raise ValueError(\"Invalid random generator (check type and device).\")\n\n    likelihood = self.likelihood\n    if likelihood == Likelihood.REWARD_MODELING:\n        likelihood = Likelihood.CLASSIFICATION if fitting else Likelihood.REGRESSION\n\n    if pred_type == PredType.GLM:\n        return self._glm_forward_call(\n            x, likelihood, joint, link_approx, n_samples, diagonal_output\n        )\n    else:\n        if likelihood == Likelihood.REGRESSION:\n            samples = self._nn_predictive_samples(x, n_samples, **model_kwargs)\n            return samples.mean(dim=0), samples.var(dim=0)\n        else:  # classification; the average is computed online\n            return self._nn_predictive_classification(x, n_samples, **model_kwargs)\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.__call__(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.__call__(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here. When Laplace is done only on subset of parameters (i.e. some grad are disabled), only <code>nn</code> predictive is supported.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.__call__(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.__call__(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.__call__(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.__call__(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> when <code>joint=False</code> in regression. In the case of last-layer Laplace with a diagonal or Kron Hessian, setting this to <code>True</code> makes computation much(!) faster for large number of outputs.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.__call__(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used).</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.__call__(fitting)","title":"<code>fitting</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether or not this predictive call is done during fitting. Only useful for reward modeling: the likelihood is set to <code>\"regression\"</code> when <code>False</code> and <code>\"classification\"</code> when <code>True</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace._glm_forward_call","title":"_glm_forward_call","text":"<pre><code>_glm_forward_call(x: Tensor | MutableMapping, likelihood: Likelihood | str, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code> for \"glm\" pred type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_forward_call(\n    self,\n    x: torch.Tensor | MutableMapping,\n    likelihood: Likelihood | str,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x` for \"glm\" pred type.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    likelihood : Likelihood or str in {'classification', 'regression', 'reward_modeling'}\n        determines the log likelihood Hessian approximation.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` and `link_approx='mc'`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    f_mu, f_var = self._glm_predictive_distribution(\n        x, joint=joint and likelihood == Likelihood.REGRESSION\n    )\n\n    if likelihood == Likelihood.REGRESSION:\n        if diagonal_output and not joint:\n            f_var = torch.diagonal(f_var, dim1=-2, dim2=-1)\n        return f_mu, f_var\n\n    if link_approx == LinkApprox.MC:\n        return self._glm_predictive_samples(\n            f_mu,\n            f_var,\n            n_samples=n_samples,\n            diagonal_output=diagonal_output,\n        ).mean(dim=0)\n    elif link_approx == LinkApprox.PROBIT:\n        kappa = 1 / torch.sqrt(1.0 + np.pi / 8 * f_var.diagonal(dim1=1, dim2=2))\n        return torch.softmax(kappa * f_mu, dim=-1)\n    elif \"bridge\" in link_approx:\n        # zero mean correction\n        f_mu -= (\n            f_var.sum(-1)\n            * f_mu.sum(-1).reshape(-1, 1)\n            / f_var.sum(dim=(1, 2)).reshape(-1, 1)\n        )\n        f_var -= torch.einsum(\n            \"bi,bj-&gt;bij\", f_var.sum(-1), f_var.sum(-2)\n        ) / f_var.sum(dim=(1, 2)).reshape(-1, 1, 1)\n\n        # Laplace Bridge\n        _, K = f_mu.size(0), f_mu.size(-1)\n        f_var_diag = torch.diagonal(f_var, dim1=1, dim2=2)\n\n        # optional: variance correction\n        if link_approx == LinkApprox.BRIDGE_NORM:\n            f_var_diag_mean = f_var_diag.mean(dim=1)\n            f_var_diag_mean /= torch.as_tensor(\n                [K / 2], device=self._device, dtype=self._dtype\n            ).sqrt()\n            f_mu /= f_var_diag_mean.sqrt().unsqueeze(-1)\n            f_var_diag /= f_var_diag_mean.unsqueeze(-1)\n\n        sum_exp = torch.exp(-f_mu).sum(dim=1).unsqueeze(-1)\n        alpha = (1 - 2 / K + f_mu.exp() / K**2 * sum_exp) / f_var_diag\n        return torch.nan_to_num(alpha / alpha.sum(dim=1).unsqueeze(-1), nan=1.0)\n    else:\n        raise ValueError(\n            \"Prediction path invalid. Check the likelihood, pred_type, link_approx combination!\"\n        )\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace._glm_forward_call(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace._glm_forward_call(likelihood)","title":"<code>likelihood</code>","text":"(<code>Likelihood or str in {'classification', 'regression', 'reward_modeling'}</code>)           \u2013            <p>determines the log likelihood Hessian approximation.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace._glm_forward_call(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace._glm_forward_call(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace._glm_forward_call(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace._glm_forward_call(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> and <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace._glm_functional_samples","title":"_glm_functional_samples","text":"<pre><code>_glm_functional_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior functional on input data <code>x</code> using \"glm\" prediction type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_functional_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior functional on input data `x` using \"glm\" prediction\n    type.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    assert f_var.shape == torch.Size([f_mu.shape[0], f_mu.shape[1], f_mu.shape[1]])\n\n    if diagonal_output:\n        f_var = torch.diagonal(f_var, dim1=1, dim2=2)\n\n    return normal_samples(f_mu, f_var, n_samples, generator)\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace._glm_functional_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace._glm_functional_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace._glm_functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace._glm_functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace._glm_functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace._glm_predictive_samples","title":"_glm_predictive_samples","text":"<pre><code>_glm_predictive_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code> using \"glm\" prediction type. I.e., the inverse-link function correponding to the likelihood is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_predictive_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x` using \"glm\" prediction\n    type. I.e., the inverse-link function correponding to the likelihood is applied\n    on top of the functional sample.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    f_samples = self._glm_functional_samples(\n        f_mu, f_var, n_samples, diagonal_output, generator\n    )\n\n    if self.likelihood == Likelihood.REGRESSION:\n        return f_samples\n    else:\n        return torch.softmax(f_samples, dim=-1)\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace._glm_predictive_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace._glm_predictive_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace._glm_predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace._glm_predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace._glm_predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.log_prob","title":"log_prob","text":"<pre><code>log_prob(value: Tensor, normalized: bool = True) -&gt; Tensor\n</code></pre> <p>Compute the log probability under the (current) Laplace approximation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>log_prob</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def log_prob(self, value: torch.Tensor, normalized: bool = True) -&gt; torch.Tensor:\n    \"\"\"Compute the log probability under the (current) Laplace approximation.\n\n    Parameters\n    ----------\n    value: torch.Tensor\n    normalized : bool, default=True\n        whether to return log of a properly normalized Gaussian or just the\n        terms that depend on `value`.\n\n    Returns\n    -------\n    log_prob : torch.Tensor\n    \"\"\"\n    if not normalized:\n        return -self.square_norm(value) / 2\n    log_prob = (\n        -self.n_params / 2 * log(2 * pi) + self.log_det_posterior_precision / 2\n    )\n    log_prob -= self.square_norm(value) / 2\n    return log_prob\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.log_prob(value)","title":"<code>value</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.log_prob(normalized)","title":"<code>normalized</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to return log of a properly normalized Gaussian or just the terms that depend on <code>value</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.functional_samples","title":"functional_samples","text":"<pre><code>functional_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the function-space posterior on input data <code>x</code>. Can be used, for example, for Thompson sampling or to compute an arbitrary expectation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def functional_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the function-space posterior on input data `x`.\n    Can be used, for example, for Thompson sampling or to compute an arbitrary\n    expectation.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if pred_type == PredType.GLM:\n        f_mu, f_var = self._glm_predictive_distribution(x)\n        return self._glm_functional_samples(\n            f_mu, f_var, n_samples, diagonal_output, generator\n        )\n    else:  # 'nn'\n        return self._nn_functional_samples(x, n_samples, generator)\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.functional_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.functional_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.predictive_samples","title":"predictive_samples","text":"<pre><code>predictive_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code>. I.e., the respective inverse-link function (e.g. softmax) is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def predictive_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x`. I.e., the respective\n    inverse-link function (e.g. softmax) is applied on top of the functional\n    sample.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if pred_type == PredType.GLM:\n        f_mu, f_var = self._glm_predictive_distribution(x)\n        return self._glm_predictive_samples(\n            f_mu, f_var, n_samples, diagonal_output, generator\n        )\n    else:  # 'nn'\n        return self._nn_predictive_samples(x, n_samples, generator)\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.predictive_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.predictive_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.DiagLLLaplace.predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace","title":"KronLLLaplace","text":"<pre><code>KronLLLaplace(model: Module, likelihood: Likelihood | str, sigma_noise: float | Tensor = 1.0, prior_precision: float | Tensor = 1.0, prior_mean: float | Tensor = 0.0, temperature: float = 1.0, enable_backprop: bool = False, feature_reduction: FeatureReduction | str | None = None, dict_key_x: str = 'input_ids', dict_key_y: str = 'labels', backend: type[CurvatureInterface] | None = None, last_layer_name: str | None = None, damping: bool = False, backend_kwargs: dict[str, Any] | None = None, asdl_fisher_kwargs: dict[str, Any] | None = None)\n</code></pre> <p>               Bases: <code>LLLaplace</code>, <code>KronLaplace</code></p> <p>Last-layer Laplace approximation with Kronecker factored log likelihood Hessian approximation and hence posterior precision. Mathematically, we have for the last parameter group, i.e., torch.nn.Linear, that \\P\\approx Q \\otimes H. See <code>KronLaplace</code>, <code>LLLaplace</code>, and <code>BaseLaplace</code> for the full interface and see <code>laplace.utils.matrix.Kron</code> and <code>laplace.utils.matrix.KronDecomposed</code> for the structure of the Kronecker factors. <code>Kron</code> is used to aggregate factors by summing up and <code>KronDecomposed</code> is used to add the prior, a Hessian factor (e.g. temperature), and computing posterior covariances, marginal likelihood, etc. Use of <code>damping</code> is possible by initializing or setting <code>damping=True</code>.</p> <p>Methods:</p> <ul> <li> <code>fit</code>             \u2013              <p>Fit the local Laplace approximation at the parameters of the model.</p> </li> <li> <code>log_marginal_likelihood</code>             \u2013              <p>Compute the Laplace approximation to the log marginal likelihood subject</p> </li> <li> <code>__call__</code>             \u2013              <p>Compute the posterior predictive on input data <code>x</code>.</p> </li> <li> <code>log_prob</code>             \u2013              <p>Compute the log probability under the (current) Laplace approximation.</p> </li> <li> <code>functional_samples</code>             \u2013              <p>Sample from the function-space posterior on input data <code>x</code>.</p> </li> <li> <code>predictive_samples</code>             \u2013              <p>Sample from the posterior predictive on input data <code>x</code>. I.e., the respective</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>log_likelihood</code>               (<code>Tensor</code>)           \u2013            <p>Compute log likelihood on the training data after <code>.fit()</code> has been called.</p> </li> <li> <code>prior_precision_diag</code>               (<code>Tensor</code>)           \u2013            <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either</p> </li> <li> <code>scatter</code>               (<code>Tensor</code>)           \u2013            <p>Computes the scatter, a term of the log marginal likelihood that</p> </li> <li> <code>log_det_prior_precision</code>               (<code>Tensor</code>)           \u2013            <p>Compute log determinant of the prior precision</p> </li> <li> <code>log_det_ratio</code>               (<code>Tensor</code>)           \u2013            <p>Compute the log determinant ratio, a part of the log marginal likelihood.</p> </li> <li> <code>posterior_precision</code>               (<code>KronDecomposed</code>)           \u2013            <p>Kronecker factored Posterior precision \\(P\\).</p> </li> </ul> Source code in <code>laplace/lllaplace.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    sigma_noise: float | torch.Tensor = 1.0,\n    prior_precision: float | torch.Tensor = 1.0,\n    prior_mean: float | torch.Tensor = 0.0,\n    temperature: float = 1.0,\n    enable_backprop: bool = False,\n    feature_reduction: FeatureReduction | str | None = None,\n    dict_key_x: str = \"input_ids\",\n    dict_key_y: str = \"labels\",\n    backend: type[CurvatureInterface] | None = None,\n    last_layer_name: str | None = None,\n    damping: bool = False,\n    backend_kwargs: dict[str, Any] | None = None,\n    asdl_fisher_kwargs: dict[str, Any] | None = None,\n):\n    self.damping = damping\n    super().__init__(\n        model,\n        likelihood,\n        sigma_noise,\n        prior_precision,\n        prior_mean,\n        temperature,\n        enable_backprop,\n        feature_reduction,\n        dict_key_x,\n        dict_key_y,\n        backend,\n        last_layer_name,\n        backend_kwargs,\n        asdl_fisher_kwargs,\n    )\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.log_likelihood","title":"log_likelihood","text":"<pre><code>log_likelihood: Tensor\n</code></pre> <p>Compute log likelihood on the training data after <code>.fit()</code> has been called. The log likelihood is computed on-demand based on the loss and, for example, the observation noise which makes it differentiable in the latter for iterative updates.</p> <p>Returns:</p> <ul> <li> <code>log_likelihood</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.prior_precision_diag","title":"prior_precision_diag","text":"<pre><code>prior_precision_diag: Tensor\n</code></pre> <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either a scalar or diagonal prior precision.</p> <p>Returns:</p> <ul> <li> <code>prior_precision_diag</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.scatter","title":"scatter","text":"<pre><code>scatter: Tensor\n</code></pre> <p>Computes the scatter, a term of the log marginal likelihood that corresponds to L-2 regularization: <code>scatter</code> = \\((\\theta_{MAP} - \\mu_0)^{T} P_0 (\\theta_{MAP} - \\mu_0) \\).</p> <p>Returns:</p> <ul> <li> <code>scatter</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.log_det_prior_precision","title":"log_det_prior_precision","text":"<pre><code>log_det_prior_precision: Tensor\n</code></pre> <p>Compute log determinant of the prior precision \\(\\log \\det P_0\\)</p> <p>Returns:</p> <ul> <li> <code>log_det</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.log_det_ratio","title":"log_det_ratio","text":"<pre><code>log_det_ratio: Tensor\n</code></pre> <p>Compute the log determinant ratio, a part of the log marginal likelihood.</p> \\[     \\log \\frac{\\det P}{\\det P_0} = \\log \\det P - \\log \\det P_0 \\] <p>Returns:</p> <ul> <li> <code>log_det_ratio</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.posterior_precision","title":"posterior_precision","text":"<pre><code>posterior_precision: KronDecomposed\n</code></pre> <p>Kronecker factored Posterior precision \\(P\\).</p> <p>Returns:</p> <ul> <li> <code>precision</code> (              <code>`laplace.utils.matrix.KronDecomposed`</code> )          \u2013            </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.fit","title":"fit","text":"<pre><code>fit(train_loader: DataLoader, override: bool = True, progress_bar: bool = False) -&gt; None\n</code></pre> <p>Fit the local Laplace approximation at the parameters of the model.</p> <p>Parameters:</p> Source code in <code>laplace/lllaplace.py</code> <pre><code>def fit(\n    self,\n    train_loader: DataLoader,\n    override: bool = True,\n    progress_bar: bool = False,\n) -&gt; None:\n    \"\"\"Fit the local Laplace approximation at the parameters of the model.\n\n    Parameters\n    ----------\n    train_loader : torch.data.utils.DataLoader\n        each iterate is a training batch, either `(X, y)` tensors or a dict-like\n        object containing keys as expressed by `self.dict_key_x` and\n        `self.dict_key_y`. `train_loader.dataset` needs to be set to access\n        \\\\(N\\\\), size of the data set.\n    override : bool, default=True\n        whether to initialize H, loss, and n_data again; setting to False is useful for\n        online learning settings to accumulate a sequential posterior approximation.\n    progress_bar: bool, default=False\n    \"\"\"\n    if not override:\n        raise ValueError(\n            \"Last-layer Laplace approximations do not support `override=False`.\"\n        )\n\n    self.model.eval()\n\n    if self.model.last_layer is None:\n        self.data: tuple[torch.Tensor, torch.Tensor] | MutableMapping = next(\n            iter(train_loader)\n        )\n        self._find_last_layer(self.data)\n        params: torch.Tensor = parameters_to_vector(\n            self.model.last_layer.parameters()\n        ).detach()\n        self.n_params: int = len(params)\n        self.n_layers: int = len(list(self.model.last_layer.parameters()))\n        # here, check the already set prior precision again\n        self.prior_precision: float | torch.Tensor = self._prior_precision\n        self.prior_mean: float | torch.Tensor = self._prior_mean\n        self._init_H()\n\n    super().fit(train_loader, override=override)\n    self.mean: torch.Tensor = parameters_to_vector(\n        self.model.last_layer.parameters()\n    )\n\n    if not self.enable_backprop:\n        self.mean = self.mean.detach()\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.fit(train_loader)","title":"<code>train_loader</code>","text":"(<code>DataLoader</code>)           \u2013            <p>each iterate is a training batch, either <code>(X, y)</code> tensors or a dict-like object containing keys as expressed by <code>self.dict_key_x</code> and <code>self.dict_key_y</code>. <code>train_loader.dataset</code> needs to be set to access \\(N\\), size of the data set.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.fit(override)","title":"<code>override</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to initialize H, loss, and n_data again; setting to False is useful for online learning settings to accumulate a sequential posterior approximation.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.fit(progress_bar)","title":"<code>progress_bar</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.log_marginal_likelihood","title":"log_marginal_likelihood","text":"<pre><code>log_marginal_likelihood(prior_precision: Tensor | None = None, sigma_noise: Tensor | None = None) -&gt; Tensor\n</code></pre> <p>Compute the Laplace approximation to the log marginal likelihood subject to specific Hessian approximations that subclasses implement. Requires that the Laplace approximation has been fit before. The resulting torch.Tensor is differentiable in <code>prior_precision</code> and <code>sigma_noise</code> if these have gradients enabled. By passing <code>prior_precision</code> or <code>sigma_noise</code>, the current value is overwritten. This is useful for iterating on the log marginal likelihood.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>log_marglik</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def log_marginal_likelihood(\n    self,\n    prior_precision: torch.Tensor | None = None,\n    sigma_noise: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the Laplace approximation to the log marginal likelihood subject\n    to specific Hessian approximations that subclasses implement.\n    Requires that the Laplace approximation has been fit before.\n    The resulting torch.Tensor is differentiable in `prior_precision` and\n    `sigma_noise` if these have gradients enabled.\n    By passing `prior_precision` or `sigma_noise`, the current value is\n    overwritten. This is useful for iterating on the log marginal likelihood.\n\n    Parameters\n    ----------\n    prior_precision : torch.Tensor, optional\n        prior precision if should be changed from current `prior_precision` value\n    sigma_noise : torch.Tensor, optional\n        observation noise standard deviation if should be changed\n\n    Returns\n    -------\n    log_marglik : torch.Tensor\n    \"\"\"\n    # update prior precision (useful when iterating on marglik)\n    if prior_precision is not None:\n        self.prior_precision = prior_precision\n\n    # update sigma_noise (useful when iterating on marglik)\n    if sigma_noise is not None:\n        if self.likelihood != Likelihood.REGRESSION:\n            raise ValueError(\"Can only change sigma_noise for regression.\")\n\n        self.sigma_noise = sigma_noise\n\n    return self.log_likelihood - 0.5 * (self.log_det_ratio + self.scatter)\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.log_marginal_likelihood(prior_precision)","title":"<code>prior_precision</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>prior precision if should be changed from current <code>prior_precision</code> value</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.log_marginal_likelihood(sigma_noise)","title":"<code>sigma_noise</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>observation noise standard deviation if should be changed</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.__call__","title":"__call__","text":"<pre><code>__call__(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None, fitting: bool = False, **model_kwargs: dict[str, Any]) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def __call__(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n    fitting: bool = False,\n    **model_kwargs: dict[str, Any],\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x`.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here. When Laplace is done only\n        on subset of parameters (i.e. some grad are disabled),\n        only `nn` predictive is supported.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` when `joint=False` in regression.\n        In the case of last-layer Laplace with a diagonal or Kron Hessian,\n        setting this to `True` makes computation much(!) faster for large\n        number of outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used).\n\n    fitting : bool, default=False\n        whether or not this predictive call is done during fitting. Only useful for\n        reward modeling: the likelihood is set to `\"regression\"` when `False` and\n        `\"classification\"` when `True`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    if pred_type not in [pred for pred in PredType]:\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if link_approx not in [la for la in LinkApprox]:\n        raise ValueError(f\"Unsupported link approximation {link_approx}.\")\n\n    if pred_type == PredType.NN and link_approx != LinkApprox.MC:\n        raise ValueError(\n            \"Only mc link approximation is supported for nn prediction type.\"\n        )\n\n    if generator is not None:\n        if (\n            not isinstance(generator, torch.Generator)\n            or generator.device != self._device\n        ):\n            raise ValueError(\"Invalid random generator (check type and device).\")\n\n    likelihood = self.likelihood\n    if likelihood == Likelihood.REWARD_MODELING:\n        likelihood = Likelihood.CLASSIFICATION if fitting else Likelihood.REGRESSION\n\n    if pred_type == PredType.GLM:\n        return self._glm_forward_call(\n            x, likelihood, joint, link_approx, n_samples, diagonal_output\n        )\n    else:\n        if likelihood == Likelihood.REGRESSION:\n            samples = self._nn_predictive_samples(x, n_samples, **model_kwargs)\n            return samples.mean(dim=0), samples.var(dim=0)\n        else:  # classification; the average is computed online\n            return self._nn_predictive_classification(x, n_samples, **model_kwargs)\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.__call__(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.__call__(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here. When Laplace is done only on subset of parameters (i.e. some grad are disabled), only <code>nn</code> predictive is supported.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.__call__(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.__call__(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.__call__(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.__call__(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> when <code>joint=False</code> in regression. In the case of last-layer Laplace with a diagonal or Kron Hessian, setting this to <code>True</code> makes computation much(!) faster for large number of outputs.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.__call__(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used).</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.__call__(fitting)","title":"<code>fitting</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether or not this predictive call is done during fitting. Only useful for reward modeling: the likelihood is set to <code>\"regression\"</code> when <code>False</code> and <code>\"classification\"</code> when <code>True</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace._glm_forward_call","title":"_glm_forward_call","text":"<pre><code>_glm_forward_call(x: Tensor | MutableMapping, likelihood: Likelihood | str, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code> for \"glm\" pred type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_forward_call(\n    self,\n    x: torch.Tensor | MutableMapping,\n    likelihood: Likelihood | str,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x` for \"glm\" pred type.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    likelihood : Likelihood or str in {'classification', 'regression', 'reward_modeling'}\n        determines the log likelihood Hessian approximation.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` and `link_approx='mc'`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    f_mu, f_var = self._glm_predictive_distribution(\n        x, joint=joint and likelihood == Likelihood.REGRESSION\n    )\n\n    if likelihood == Likelihood.REGRESSION:\n        if diagonal_output and not joint:\n            f_var = torch.diagonal(f_var, dim1=-2, dim2=-1)\n        return f_mu, f_var\n\n    if link_approx == LinkApprox.MC:\n        return self._glm_predictive_samples(\n            f_mu,\n            f_var,\n            n_samples=n_samples,\n            diagonal_output=diagonal_output,\n        ).mean(dim=0)\n    elif link_approx == LinkApprox.PROBIT:\n        kappa = 1 / torch.sqrt(1.0 + np.pi / 8 * f_var.diagonal(dim1=1, dim2=2))\n        return torch.softmax(kappa * f_mu, dim=-1)\n    elif \"bridge\" in link_approx:\n        # zero mean correction\n        f_mu -= (\n            f_var.sum(-1)\n            * f_mu.sum(-1).reshape(-1, 1)\n            / f_var.sum(dim=(1, 2)).reshape(-1, 1)\n        )\n        f_var -= torch.einsum(\n            \"bi,bj-&gt;bij\", f_var.sum(-1), f_var.sum(-2)\n        ) / f_var.sum(dim=(1, 2)).reshape(-1, 1, 1)\n\n        # Laplace Bridge\n        _, K = f_mu.size(0), f_mu.size(-1)\n        f_var_diag = torch.diagonal(f_var, dim1=1, dim2=2)\n\n        # optional: variance correction\n        if link_approx == LinkApprox.BRIDGE_NORM:\n            f_var_diag_mean = f_var_diag.mean(dim=1)\n            f_var_diag_mean /= torch.as_tensor(\n                [K / 2], device=self._device, dtype=self._dtype\n            ).sqrt()\n            f_mu /= f_var_diag_mean.sqrt().unsqueeze(-1)\n            f_var_diag /= f_var_diag_mean.unsqueeze(-1)\n\n        sum_exp = torch.exp(-f_mu).sum(dim=1).unsqueeze(-1)\n        alpha = (1 - 2 / K + f_mu.exp() / K**2 * sum_exp) / f_var_diag\n        return torch.nan_to_num(alpha / alpha.sum(dim=1).unsqueeze(-1), nan=1.0)\n    else:\n        raise ValueError(\n            \"Prediction path invalid. Check the likelihood, pred_type, link_approx combination!\"\n        )\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace._glm_forward_call(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace._glm_forward_call(likelihood)","title":"<code>likelihood</code>","text":"(<code>Likelihood or str in {'classification', 'regression', 'reward_modeling'}</code>)           \u2013            <p>determines the log likelihood Hessian approximation.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace._glm_forward_call(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace._glm_forward_call(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace._glm_forward_call(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace._glm_forward_call(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> and <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace._glm_functional_samples","title":"_glm_functional_samples","text":"<pre><code>_glm_functional_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior functional on input data <code>x</code> using \"glm\" prediction type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_functional_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior functional on input data `x` using \"glm\" prediction\n    type.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    assert f_var.shape == torch.Size([f_mu.shape[0], f_mu.shape[1], f_mu.shape[1]])\n\n    if diagonal_output:\n        f_var = torch.diagonal(f_var, dim1=1, dim2=2)\n\n    return normal_samples(f_mu, f_var, n_samples, generator)\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace._glm_functional_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace._glm_functional_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace._glm_functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace._glm_functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace._glm_functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace._glm_predictive_samples","title":"_glm_predictive_samples","text":"<pre><code>_glm_predictive_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code> using \"glm\" prediction type. I.e., the inverse-link function correponding to the likelihood is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_predictive_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x` using \"glm\" prediction\n    type. I.e., the inverse-link function correponding to the likelihood is applied\n    on top of the functional sample.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    f_samples = self._glm_functional_samples(\n        f_mu, f_var, n_samples, diagonal_output, generator\n    )\n\n    if self.likelihood == Likelihood.REGRESSION:\n        return f_samples\n    else:\n        return torch.softmax(f_samples, dim=-1)\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace._glm_predictive_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace._glm_predictive_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace._glm_predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace._glm_predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace._glm_predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.log_prob","title":"log_prob","text":"<pre><code>log_prob(value: Tensor, normalized: bool = True) -&gt; Tensor\n</code></pre> <p>Compute the log probability under the (current) Laplace approximation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>log_prob</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def log_prob(self, value: torch.Tensor, normalized: bool = True) -&gt; torch.Tensor:\n    \"\"\"Compute the log probability under the (current) Laplace approximation.\n\n    Parameters\n    ----------\n    value: torch.Tensor\n    normalized : bool, default=True\n        whether to return log of a properly normalized Gaussian or just the\n        terms that depend on `value`.\n\n    Returns\n    -------\n    log_prob : torch.Tensor\n    \"\"\"\n    if not normalized:\n        return -self.square_norm(value) / 2\n    log_prob = (\n        -self.n_params / 2 * log(2 * pi) + self.log_det_posterior_precision / 2\n    )\n    log_prob -= self.square_norm(value) / 2\n    return log_prob\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.log_prob(value)","title":"<code>value</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.log_prob(normalized)","title":"<code>normalized</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to return log of a properly normalized Gaussian or just the terms that depend on <code>value</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.functional_samples","title":"functional_samples","text":"<pre><code>functional_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the function-space posterior on input data <code>x</code>. Can be used, for example, for Thompson sampling or to compute an arbitrary expectation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def functional_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the function-space posterior on input data `x`.\n    Can be used, for example, for Thompson sampling or to compute an arbitrary\n    expectation.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if pred_type == PredType.GLM:\n        f_mu, f_var = self._glm_predictive_distribution(x)\n        return self._glm_functional_samples(\n            f_mu, f_var, n_samples, diagonal_output, generator\n        )\n    else:  # 'nn'\n        return self._nn_functional_samples(x, n_samples, generator)\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.functional_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.functional_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.predictive_samples","title":"predictive_samples","text":"<pre><code>predictive_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code>. I.e., the respective inverse-link function (e.g. softmax) is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def predictive_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x`. I.e., the respective\n    inverse-link function (e.g. softmax) is applied on top of the functional\n    sample.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if pred_type == PredType.GLM:\n        f_mu, f_var = self._glm_predictive_distribution(x)\n        return self._glm_predictive_samples(\n            f_mu, f_var, n_samples, diagonal_output, generator\n        )\n    else:  # 'nn'\n        return self._nn_predictive_samples(x, n_samples, generator)\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.predictive_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.predictive_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.KronLLLaplace.predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace","title":"FullLLLaplace","text":"<pre><code>FullLLLaplace(model: Module, likelihood: Likelihood | str, sigma_noise: float | Tensor = 1.0, prior_precision: float | Tensor = 1.0, prior_mean: float | Tensor = 0.0, temperature: float = 1.0, enable_backprop: bool = False, feature_reduction: FeatureReduction | str | None = None, dict_key_x: str = 'input_ids', dict_key_y: str = 'labels', backend: type[CurvatureInterface] | None = None, last_layer_name: str | None = None, backend_kwargs: dict[str, Any] | None = None, asdl_fisher_kwargs: dict[str, Any] | None = None)\n</code></pre> <p>               Bases: <code>LLLaplace</code>, <code>FullLaplace</code></p> <p>Last-layer Laplace approximation with full, i.e., dense, log likelihood Hessian approximation and hence posterior precision. Based on the chosen <code>backend</code> parameter, the full approximation can be, for example, a generalized Gauss-Newton matrix. Mathematically, we have \\(P \\in \\mathbb{R}^{P \\times P}\\). See <code>FullLaplace</code>, <code>LLLaplace</code>, and <code>BaseLaplace</code> for the full interface.</p> <p>Methods:</p> <ul> <li> <code>fit</code>             \u2013              <p>Fit the local Laplace approximation at the parameters of the model.</p> </li> <li> <code>log_marginal_likelihood</code>             \u2013              <p>Compute the Laplace approximation to the log marginal likelihood subject</p> </li> <li> <code>__call__</code>             \u2013              <p>Compute the posterior predictive on input data <code>x</code>.</p> </li> <li> <code>log_prob</code>             \u2013              <p>Compute the log probability under the (current) Laplace approximation.</p> </li> <li> <code>functional_samples</code>             \u2013              <p>Sample from the function-space posterior on input data <code>x</code>.</p> </li> <li> <code>predictive_samples</code>             \u2013              <p>Sample from the posterior predictive on input data <code>x</code>. I.e., the respective</p> </li> <li> <code>functional_variance_fast</code>             \u2013              <p>Should be overriden if there exists a trick to make this fast!</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>log_likelihood</code>               (<code>Tensor</code>)           \u2013            <p>Compute log likelihood on the training data after <code>.fit()</code> has been called.</p> </li> <li> <code>prior_precision_diag</code>               (<code>Tensor</code>)           \u2013            <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either</p> </li> <li> <code>scatter</code>               (<code>Tensor</code>)           \u2013            <p>Computes the scatter, a term of the log marginal likelihood that</p> </li> <li> <code>log_det_prior_precision</code>               (<code>Tensor</code>)           \u2013            <p>Compute log determinant of the prior precision</p> </li> <li> <code>log_det_ratio</code>               (<code>Tensor</code>)           \u2013            <p>Compute the log determinant ratio, a part of the log marginal likelihood.</p> </li> <li> <code>posterior_precision</code>               (<code>Tensor</code>)           \u2013            <p>Posterior precision \\(P\\).</p> </li> <li> <code>posterior_scale</code>               (<code>Tensor</code>)           \u2013            <p>Posterior scale (square root of the covariance), i.e.,</p> </li> <li> <code>posterior_covariance</code>               (<code>Tensor</code>)           \u2013            <p>Posterior covariance, i.e., \\(P^{-1}\\).</p> </li> </ul> Source code in <code>laplace/lllaplace.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    sigma_noise: float | torch.Tensor = 1.0,\n    prior_precision: float | torch.Tensor = 1.0,\n    prior_mean: float | torch.Tensor = 0.0,\n    temperature: float = 1.0,\n    enable_backprop: bool = False,\n    feature_reduction: FeatureReduction | str | None = None,\n    dict_key_x: str = \"input_ids\",\n    dict_key_y: str = \"labels\",\n    backend: type[CurvatureInterface] | None = None,\n    last_layer_name: str | None = None,\n    backend_kwargs: dict[str, Any] | None = None,\n    asdl_fisher_kwargs: dict[str, Any] | None = None,\n):\n    if asdl_fisher_kwargs is not None:\n        raise ValueError(\"Last-layer Laplace does not support asdl_fisher_kwargs.\")\n\n    self.H = None\n    super().__init__(\n        model,\n        likelihood,\n        sigma_noise=sigma_noise,\n        prior_precision=1.0,\n        prior_mean=0.0,\n        temperature=temperature,\n        enable_backprop=enable_backprop,\n        dict_key_x=dict_key_x,\n        dict_key_y=dict_key_y,\n        backend=backend,\n        backend_kwargs=backend_kwargs,\n    )\n    self.model = FeatureExtractor(\n        deepcopy(model),\n        last_layer_name=last_layer_name,\n        enable_backprop=enable_backprop,\n        feature_reduction=feature_reduction,\n    )\n\n    if self.model.last_layer is None:\n        self.mean: torch.Tensor | None = None\n        self.n_params: int | None = None\n        self.n_layers: int | None = None\n        # ignore checks of prior mean setter temporarily, check on .fit()\n        self._prior_precision: float | torch.Tensor = prior_precision\n        self._prior_mean: float | torch.Tensor = prior_mean\n    else:\n        self.n_params: int = len(\n            parameters_to_vector(self.model.last_layer.parameters())\n        )\n        self.n_layers: int | None = len(list(self.model.last_layer.parameters()))\n        self.prior_precision: float | torch.Tensor = prior_precision\n        self.prior_mean: float | torch.Tensor = prior_mean\n        self.mean: float | torch.Tensor = self.prior_mean\n        self._init_H()\n\n    self._backend_kwargs[\"last_layer\"] = True\n    self._last_layer_name: str | None = last_layer_name\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.log_likelihood","title":"log_likelihood","text":"<pre><code>log_likelihood: Tensor\n</code></pre> <p>Compute log likelihood on the training data after <code>.fit()</code> has been called. The log likelihood is computed on-demand based on the loss and, for example, the observation noise which makes it differentiable in the latter for iterative updates.</p> <p>Returns:</p> <ul> <li> <code>log_likelihood</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.prior_precision_diag","title":"prior_precision_diag","text":"<pre><code>prior_precision_diag: Tensor\n</code></pre> <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either a scalar or diagonal prior precision.</p> <p>Returns:</p> <ul> <li> <code>prior_precision_diag</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.scatter","title":"scatter","text":"<pre><code>scatter: Tensor\n</code></pre> <p>Computes the scatter, a term of the log marginal likelihood that corresponds to L-2 regularization: <code>scatter</code> = \\((\\theta_{MAP} - \\mu_0)^{T} P_0 (\\theta_{MAP} - \\mu_0) \\).</p> <p>Returns:</p> <ul> <li> <code>scatter</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.log_det_prior_precision","title":"log_det_prior_precision","text":"<pre><code>log_det_prior_precision: Tensor\n</code></pre> <p>Compute log determinant of the prior precision \\(\\log \\det P_0\\)</p> <p>Returns:</p> <ul> <li> <code>log_det</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.log_det_ratio","title":"log_det_ratio","text":"<pre><code>log_det_ratio: Tensor\n</code></pre> <p>Compute the log determinant ratio, a part of the log marginal likelihood.</p> \\[     \\log \\frac{\\det P}{\\det P_0} = \\log \\det P - \\log \\det P_0 \\] <p>Returns:</p> <ul> <li> <code>log_det_ratio</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.posterior_precision","title":"posterior_precision","text":"<pre><code>posterior_precision: Tensor\n</code></pre> <p>Posterior precision \\(P\\).</p> <p>Returns:</p> <ul> <li> <code>precision</code> (              <code>tensor</code> )          \u2013            <p><code>(parameters, parameters)</code></p> </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.posterior_scale","title":"posterior_scale","text":"<pre><code>posterior_scale: Tensor\n</code></pre> <p>Posterior scale (square root of the covariance), i.e., \\(P^{-\\frac{1}{2}}\\).</p> <p>Returns:</p> <ul> <li> <code>scale</code> (              <code>tensor</code> )          \u2013            <p><code>(parameters, parameters)</code></p> </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.posterior_covariance","title":"posterior_covariance","text":"<pre><code>posterior_covariance: Tensor\n</code></pre> <p>Posterior covariance, i.e., \\(P^{-1}\\).</p> <p>Returns:</p> <ul> <li> <code>covariance</code> (              <code>tensor</code> )          \u2013            <p><code>(parameters, parameters)</code></p> </li> </ul>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.fit","title":"fit","text":"<pre><code>fit(train_loader: DataLoader, override: bool = True, progress_bar: bool = False) -&gt; None\n</code></pre> <p>Fit the local Laplace approximation at the parameters of the model.</p> <p>Parameters:</p> Source code in <code>laplace/lllaplace.py</code> <pre><code>def fit(\n    self,\n    train_loader: DataLoader,\n    override: bool = True,\n    progress_bar: bool = False,\n) -&gt; None:\n    \"\"\"Fit the local Laplace approximation at the parameters of the model.\n\n    Parameters\n    ----------\n    train_loader : torch.data.utils.DataLoader\n        each iterate is a training batch, either `(X, y)` tensors or a dict-like\n        object containing keys as expressed by `self.dict_key_x` and\n        `self.dict_key_y`. `train_loader.dataset` needs to be set to access\n        \\\\(N\\\\), size of the data set.\n    override : bool, default=True\n        whether to initialize H, loss, and n_data again; setting to False is useful for\n        online learning settings to accumulate a sequential posterior approximation.\n    progress_bar: bool, default=False\n    \"\"\"\n    if not override:\n        raise ValueError(\n            \"Last-layer Laplace approximations do not support `override=False`.\"\n        )\n\n    self.model.eval()\n\n    if self.model.last_layer is None:\n        self.data: tuple[torch.Tensor, torch.Tensor] | MutableMapping = next(\n            iter(train_loader)\n        )\n        self._find_last_layer(self.data)\n        params: torch.Tensor = parameters_to_vector(\n            self.model.last_layer.parameters()\n        ).detach()\n        self.n_params: int = len(params)\n        self.n_layers: int = len(list(self.model.last_layer.parameters()))\n        # here, check the already set prior precision again\n        self.prior_precision: float | torch.Tensor = self._prior_precision\n        self.prior_mean: float | torch.Tensor = self._prior_mean\n        self._init_H()\n\n    super().fit(train_loader, override=override)\n    self.mean: torch.Tensor = parameters_to_vector(\n        self.model.last_layer.parameters()\n    )\n\n    if not self.enable_backprop:\n        self.mean = self.mean.detach()\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.fit(train_loader)","title":"<code>train_loader</code>","text":"(<code>DataLoader</code>)           \u2013            <p>each iterate is a training batch, either <code>(X, y)</code> tensors or a dict-like object containing keys as expressed by <code>self.dict_key_x</code> and <code>self.dict_key_y</code>. <code>train_loader.dataset</code> needs to be set to access \\(N\\), size of the data set.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.fit(override)","title":"<code>override</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to initialize H, loss, and n_data again; setting to False is useful for online learning settings to accumulate a sequential posterior approximation.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.fit(progress_bar)","title":"<code>progress_bar</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.log_marginal_likelihood","title":"log_marginal_likelihood","text":"<pre><code>log_marginal_likelihood(prior_precision: Tensor | None = None, sigma_noise: Tensor | None = None) -&gt; Tensor\n</code></pre> <p>Compute the Laplace approximation to the log marginal likelihood subject to specific Hessian approximations that subclasses implement. Requires that the Laplace approximation has been fit before. The resulting torch.Tensor is differentiable in <code>prior_precision</code> and <code>sigma_noise</code> if these have gradients enabled. By passing <code>prior_precision</code> or <code>sigma_noise</code>, the current value is overwritten. This is useful for iterating on the log marginal likelihood.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>log_marglik</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def log_marginal_likelihood(\n    self,\n    prior_precision: torch.Tensor | None = None,\n    sigma_noise: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the Laplace approximation to the log marginal likelihood subject\n    to specific Hessian approximations that subclasses implement.\n    Requires that the Laplace approximation has been fit before.\n    The resulting torch.Tensor is differentiable in `prior_precision` and\n    `sigma_noise` if these have gradients enabled.\n    By passing `prior_precision` or `sigma_noise`, the current value is\n    overwritten. This is useful for iterating on the log marginal likelihood.\n\n    Parameters\n    ----------\n    prior_precision : torch.Tensor, optional\n        prior precision if should be changed from current `prior_precision` value\n    sigma_noise : torch.Tensor, optional\n        observation noise standard deviation if should be changed\n\n    Returns\n    -------\n    log_marglik : torch.Tensor\n    \"\"\"\n    # update prior precision (useful when iterating on marglik)\n    if prior_precision is not None:\n        self.prior_precision = prior_precision\n\n    # update sigma_noise (useful when iterating on marglik)\n    if sigma_noise is not None:\n        if self.likelihood != Likelihood.REGRESSION:\n            raise ValueError(\"Can only change sigma_noise for regression.\")\n\n        self.sigma_noise = sigma_noise\n\n    return self.log_likelihood - 0.5 * (self.log_det_ratio + self.scatter)\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.log_marginal_likelihood(prior_precision)","title":"<code>prior_precision</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>prior precision if should be changed from current <code>prior_precision</code> value</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.log_marginal_likelihood(sigma_noise)","title":"<code>sigma_noise</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>observation noise standard deviation if should be changed</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.__call__","title":"__call__","text":"<pre><code>__call__(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None, fitting: bool = False, **model_kwargs: dict[str, Any]) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def __call__(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n    fitting: bool = False,\n    **model_kwargs: dict[str, Any],\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x`.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here. When Laplace is done only\n        on subset of parameters (i.e. some grad are disabled),\n        only `nn` predictive is supported.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` when `joint=False` in regression.\n        In the case of last-layer Laplace with a diagonal or Kron Hessian,\n        setting this to `True` makes computation much(!) faster for large\n        number of outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used).\n\n    fitting : bool, default=False\n        whether or not this predictive call is done during fitting. Only useful for\n        reward modeling: the likelihood is set to `\"regression\"` when `False` and\n        `\"classification\"` when `True`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    if pred_type not in [pred for pred in PredType]:\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if link_approx not in [la for la in LinkApprox]:\n        raise ValueError(f\"Unsupported link approximation {link_approx}.\")\n\n    if pred_type == PredType.NN and link_approx != LinkApprox.MC:\n        raise ValueError(\n            \"Only mc link approximation is supported for nn prediction type.\"\n        )\n\n    if generator is not None:\n        if (\n            not isinstance(generator, torch.Generator)\n            or generator.device != self._device\n        ):\n            raise ValueError(\"Invalid random generator (check type and device).\")\n\n    likelihood = self.likelihood\n    if likelihood == Likelihood.REWARD_MODELING:\n        likelihood = Likelihood.CLASSIFICATION if fitting else Likelihood.REGRESSION\n\n    if pred_type == PredType.GLM:\n        return self._glm_forward_call(\n            x, likelihood, joint, link_approx, n_samples, diagonal_output\n        )\n    else:\n        if likelihood == Likelihood.REGRESSION:\n            samples = self._nn_predictive_samples(x, n_samples, **model_kwargs)\n            return samples.mean(dim=0), samples.var(dim=0)\n        else:  # classification; the average is computed online\n            return self._nn_predictive_classification(x, n_samples, **model_kwargs)\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.__call__(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.__call__(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here. When Laplace is done only on subset of parameters (i.e. some grad are disabled), only <code>nn</code> predictive is supported.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.__call__(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.__call__(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.__call__(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.__call__(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> when <code>joint=False</code> in regression. In the case of last-layer Laplace with a diagonal or Kron Hessian, setting this to <code>True</code> makes computation much(!) faster for large number of outputs.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.__call__(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used).</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.__call__(fitting)","title":"<code>fitting</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether or not this predictive call is done during fitting. Only useful for reward modeling: the likelihood is set to <code>\"regression\"</code> when <code>False</code> and <code>\"classification\"</code> when <code>True</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace._glm_forward_call","title":"_glm_forward_call","text":"<pre><code>_glm_forward_call(x: Tensor | MutableMapping, likelihood: Likelihood | str, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code> for \"glm\" pred type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_forward_call(\n    self,\n    x: torch.Tensor | MutableMapping,\n    likelihood: Likelihood | str,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x` for \"glm\" pred type.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    likelihood : Likelihood or str in {'classification', 'regression', 'reward_modeling'}\n        determines the log likelihood Hessian approximation.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` and `link_approx='mc'`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    f_mu, f_var = self._glm_predictive_distribution(\n        x, joint=joint and likelihood == Likelihood.REGRESSION\n    )\n\n    if likelihood == Likelihood.REGRESSION:\n        if diagonal_output and not joint:\n            f_var = torch.diagonal(f_var, dim1=-2, dim2=-1)\n        return f_mu, f_var\n\n    if link_approx == LinkApprox.MC:\n        return self._glm_predictive_samples(\n            f_mu,\n            f_var,\n            n_samples=n_samples,\n            diagonal_output=diagonal_output,\n        ).mean(dim=0)\n    elif link_approx == LinkApprox.PROBIT:\n        kappa = 1 / torch.sqrt(1.0 + np.pi / 8 * f_var.diagonal(dim1=1, dim2=2))\n        return torch.softmax(kappa * f_mu, dim=-1)\n    elif \"bridge\" in link_approx:\n        # zero mean correction\n        f_mu -= (\n            f_var.sum(-1)\n            * f_mu.sum(-1).reshape(-1, 1)\n            / f_var.sum(dim=(1, 2)).reshape(-1, 1)\n        )\n        f_var -= torch.einsum(\n            \"bi,bj-&gt;bij\", f_var.sum(-1), f_var.sum(-2)\n        ) / f_var.sum(dim=(1, 2)).reshape(-1, 1, 1)\n\n        # Laplace Bridge\n        _, K = f_mu.size(0), f_mu.size(-1)\n        f_var_diag = torch.diagonal(f_var, dim1=1, dim2=2)\n\n        # optional: variance correction\n        if link_approx == LinkApprox.BRIDGE_NORM:\n            f_var_diag_mean = f_var_diag.mean(dim=1)\n            f_var_diag_mean /= torch.as_tensor(\n                [K / 2], device=self._device, dtype=self._dtype\n            ).sqrt()\n            f_mu /= f_var_diag_mean.sqrt().unsqueeze(-1)\n            f_var_diag /= f_var_diag_mean.unsqueeze(-1)\n\n        sum_exp = torch.exp(-f_mu).sum(dim=1).unsqueeze(-1)\n        alpha = (1 - 2 / K + f_mu.exp() / K**2 * sum_exp) / f_var_diag\n        return torch.nan_to_num(alpha / alpha.sum(dim=1).unsqueeze(-1), nan=1.0)\n    else:\n        raise ValueError(\n            \"Prediction path invalid. Check the likelihood, pred_type, link_approx combination!\"\n        )\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace._glm_forward_call(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace._glm_forward_call(likelihood)","title":"<code>likelihood</code>","text":"(<code>Likelihood or str in {'classification', 'regression', 'reward_modeling'}</code>)           \u2013            <p>determines the log likelihood Hessian approximation.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace._glm_forward_call(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace._glm_forward_call(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace._glm_forward_call(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace._glm_forward_call(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> and <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace._glm_functional_samples","title":"_glm_functional_samples","text":"<pre><code>_glm_functional_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior functional on input data <code>x</code> using \"glm\" prediction type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_functional_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior functional on input data `x` using \"glm\" prediction\n    type.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    assert f_var.shape == torch.Size([f_mu.shape[0], f_mu.shape[1], f_mu.shape[1]])\n\n    if diagonal_output:\n        f_var = torch.diagonal(f_var, dim1=1, dim2=2)\n\n    return normal_samples(f_mu, f_var, n_samples, generator)\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace._glm_functional_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace._glm_functional_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace._glm_functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace._glm_functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace._glm_functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace._glm_predictive_samples","title":"_glm_predictive_samples","text":"<pre><code>_glm_predictive_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code> using \"glm\" prediction type. I.e., the inverse-link function correponding to the likelihood is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_predictive_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x` using \"glm\" prediction\n    type. I.e., the inverse-link function correponding to the likelihood is applied\n    on top of the functional sample.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    f_samples = self._glm_functional_samples(\n        f_mu, f_var, n_samples, diagonal_output, generator\n    )\n\n    if self.likelihood == Likelihood.REGRESSION:\n        return f_samples\n    else:\n        return torch.softmax(f_samples, dim=-1)\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace._glm_predictive_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace._glm_predictive_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace._glm_predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace._glm_predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace._glm_predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.log_prob","title":"log_prob","text":"<pre><code>log_prob(value: Tensor, normalized: bool = True) -&gt; Tensor\n</code></pre> <p>Compute the log probability under the (current) Laplace approximation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>log_prob</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def log_prob(self, value: torch.Tensor, normalized: bool = True) -&gt; torch.Tensor:\n    \"\"\"Compute the log probability under the (current) Laplace approximation.\n\n    Parameters\n    ----------\n    value: torch.Tensor\n    normalized : bool, default=True\n        whether to return log of a properly normalized Gaussian or just the\n        terms that depend on `value`.\n\n    Returns\n    -------\n    log_prob : torch.Tensor\n    \"\"\"\n    if not normalized:\n        return -self.square_norm(value) / 2\n    log_prob = (\n        -self.n_params / 2 * log(2 * pi) + self.log_det_posterior_precision / 2\n    )\n    log_prob -= self.square_norm(value) / 2\n    return log_prob\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.log_prob(value)","title":"<code>value</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.log_prob(normalized)","title":"<code>normalized</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to return log of a properly normalized Gaussian or just the terms that depend on <code>value</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.functional_samples","title":"functional_samples","text":"<pre><code>functional_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the function-space posterior on input data <code>x</code>. Can be used, for example, for Thompson sampling or to compute an arbitrary expectation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def functional_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the function-space posterior on input data `x`.\n    Can be used, for example, for Thompson sampling or to compute an arbitrary\n    expectation.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if pred_type == PredType.GLM:\n        f_mu, f_var = self._glm_predictive_distribution(x)\n        return self._glm_functional_samples(\n            f_mu, f_var, n_samples, diagonal_output, generator\n        )\n    else:  # 'nn'\n        return self._nn_functional_samples(x, n_samples, generator)\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.functional_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.functional_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.predictive_samples","title":"predictive_samples","text":"<pre><code>predictive_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code>. I.e., the respective inverse-link function (e.g. softmax) is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def predictive_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x`. I.e., the respective\n    inverse-link function (e.g. softmax) is applied on top of the functional\n    sample.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if pred_type == PredType.GLM:\n        f_mu, f_var = self._glm_predictive_distribution(x)\n        return self._glm_predictive_samples(\n            f_mu, f_var, n_samples, diagonal_output, generator\n        )\n    else:  # 'nn'\n        return self._nn_predictive_samples(x, n_samples, generator)\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.predictive_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.predictive_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.functional_variance_fast","title":"functional_variance_fast","text":"<pre><code>functional_variance_fast(X)\n</code></pre> <p>Should be overriden if there exists a trick to make this fast!</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>f_var_diag</code> (              <code>torch.Tensor of shape (batch_size, num_outputs)</code> )          \u2013            <p>Corresponding to the diagonal of the covariance matrix of the outputs</p> </li> </ul> Source code in <code>laplace/lllaplace.py</code> <pre><code>def functional_variance_fast(self, X):\n    \"\"\"\n    Should be overriden if there exists a trick to make this fast!\n\n    Parameters\n    ----------\n    X: torch.Tensor of shape (batch_size, input_dim)\n\n    Returns\n    -------\n    f_var_diag: torch.Tensor of shape (batch_size, num_outputs)\n        Corresponding to the diagonal of the covariance matrix of the outputs\n    \"\"\"\n    Js, f_mu = self.backend.last_layer_jacobians(X, self.enable_backprop)\n    f_cov = self.functional_variance(Js)  # No trick possible for Full Laplace\n    f_var = torch.diagonal(f_cov, dim1=-2, dim2=-1)\n    return f_mu, f_var\n</code></pre>"},{"location":"api_reference/lllaplace/#laplace.lllaplace.FullLLLaplace.functional_variance_fast(X)","title":"<code>X</code>","text":"\u2013"},{"location":"api_reference/marglik_training/","title":"Marglik Training Utils","text":""},{"location":"api_reference/marglik_training/#laplace.marglik_training","title":"laplace.marglik_training","text":"<p>Functions:</p> <ul> <li> <code>marglik_training</code>             \u2013              <p>Marginal-likelihood based training (Algorithm 1 in [1]).</p> </li> </ul>"},{"location":"api_reference/marglik_training/#laplace.marglik_training.marglik_training","title":"marglik_training","text":"<pre><code>marglik_training(model: Module, train_loader: DataLoader, likelihood: Likelihood | str = CLASSIFICATION, hessian_structure: HessianStructure | str = KRON, backend: Type[CurvatureInterface] = AsdlGGN, optimizer_cls: Type[Optimizer] = Adam, optimizer_kwargs: dict | None = None, scheduler_cls: Type[LRScheduler] | None = None, scheduler_kwargs: dict | None = None, n_epochs: int = 300, lr_hyp: float = 0.1, prior_structure: PriorStructure | str = LAYERWISE, n_epochs_burnin: int = 0, n_hypersteps: int = 10, marglik_frequency: int = 1, prior_prec_init: float = 1.0, sigma_noise_init: float = 1.0, temperature: float = 1.0, fix_sigma_noise: bool = False, progress_bar: bool = False, enable_backprop: bool = False, dict_key_x: str = 'input_ids', dict_key_y: str = 'labels') -&gt; tuple[BaseLaplace, Module, list[Number], list[Number]]\n</code></pre> <p>Marginal-likelihood based training (Algorithm 1 in [1]). Optimize model parameters and hyperparameters jointly. Model parameters are optimized to minimize negative log joint (train loss) while hyperparameters minimize negative log marginal likelihood.</p> <p>This method replaces standard neural network training and adds hyperparameter optimization to the procedure.</p> <p>The settings of standard training can be controlled by passing <code>train_loader</code>, <code>optimizer_cls</code>, <code>optimizer_kwargs</code>, <code>scheduler_cls</code>, <code>scheduler_kwargs</code>, and <code>n_epochs</code>. The <code>model</code> should return logits, i.e., no softmax should be applied. With <code>likelihood=Likelihood.CLASSIFICATION</code> or <code>Likelihood.REGRESSION</code>, one can choose between categorical likelihood (CrossEntropyLoss) and Gaussian likelihood (MSELoss).</p> <p>As in [1], we optimize prior precision and, for regression, observation noise using the marginal likelihood. The prior precision structure can be chosen as <code>'scalar'</code>, <code>'layerwise'</code>, or <code>'diagonal'</code>. <code>'layerwise'</code> is a good default and available to all Laplace approximations. <code>lr_hyp</code> is the step size of the Adam hyperparameter optimizer, <code>n_hypersteps</code> controls the number of steps for each estimated marginal likelihood, <code>n_epochs_burnin</code> controls how many epochs to skip marginal likelihood estimation, <code>marglik_frequency</code> controls how often to estimate the marginal likelihood (default of 1 re-estimates after every epoch, 5 would estimate every 5-th epoch).</p> References <p>[1] Immer, A., Bauer, M., Fortuin, V., R\u00e4tsch, G., Khan, EM. Scalable Marginal Likelihood Estimation for Model Selection in Deep Learning. ICML 2021.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>lap</code> (              <code>laplace</code> )          \u2013            <p>fit Laplace approximation with the best obtained marginal likelihood during training</p> </li> <li> <code>model</code> (              <code>Module</code> )          \u2013            <p>corresponding model with the MAP parameters</p> </li> <li> <code>margliks</code> (              <code>list</code> )          \u2013            <p>list of marginal likelihoods obtained during training (to monitor convergence)</p> </li> <li> <code>losses</code> (              <code>list</code> )          \u2013            <p>list of losses (log joints) obtained during training (to monitor convergence)</p> </li> </ul> Source code in <code>laplace/marglik_training.py</code> <pre><code>def marglik_training(\n    model: torch.nn.Module,\n    train_loader: DataLoader,\n    likelihood: Likelihood | str = Likelihood.CLASSIFICATION,\n    hessian_structure: HessianStructure | str = HessianStructure.KRON,\n    backend: Type[CurvatureInterface] = AsdlGGN,\n    optimizer_cls: Type[Optimizer] = Adam,\n    optimizer_kwargs: dict | None = None,\n    scheduler_cls: Type[LRScheduler] | None = None,\n    scheduler_kwargs: dict | None = None,\n    n_epochs: int = 300,\n    lr_hyp: float = 1e-1,\n    prior_structure: PriorStructure | str = PriorStructure.LAYERWISE,\n    n_epochs_burnin: int = 0,\n    n_hypersteps: int = 10,\n    marglik_frequency: int = 1,\n    prior_prec_init: float = 1.0,\n    sigma_noise_init: float = 1.0,\n    temperature: float = 1.0,\n    fix_sigma_noise: bool = False,\n    progress_bar: bool = False,\n    enable_backprop: bool = False,\n    dict_key_x: str = \"input_ids\",\n    dict_key_y: str = \"labels\",\n) -&gt; tuple[BaseLaplace, nn.Module, list[Number], list[Number]]:\n    \"\"\"Marginal-likelihood based training (Algorithm 1 in [1]).\n    Optimize model parameters and hyperparameters jointly.\n    Model parameters are optimized to minimize negative log joint (train loss)\n    while hyperparameters minimize negative log marginal likelihood.\n\n    This method replaces standard neural network training and adds hyperparameter\n    optimization to the procedure.\n\n    The settings of standard training can be controlled by passing `train_loader`,\n    `optimizer_cls`, `optimizer_kwargs`, `scheduler_cls`, `scheduler_kwargs`, and `n_epochs`.\n    The `model` should return logits, i.e., no softmax should be applied.\n    With `likelihood=Likelihood.CLASSIFICATION` or `Likelihood.REGRESSION`, one can choose between\n    categorical likelihood (CrossEntropyLoss) and Gaussian likelihood (MSELoss).\n\n    As in [1], we optimize prior precision and, for regression, observation noise\n    using the marginal likelihood. The prior precision structure can be chosen\n    as `'scalar'`, `'layerwise'`, or `'diagonal'`. `'layerwise'` is a good default\n    and available to all Laplace approximations. `lr_hyp` is the step size of the\n    Adam hyperparameter optimizer, `n_hypersteps` controls the number of steps\n    for each estimated marginal likelihood, `n_epochs_burnin` controls how many\n    epochs to skip marginal likelihood estimation, `marglik_frequency` controls\n    how often to estimate the marginal likelihood (default of 1 re-estimates\n    after every epoch, 5 would estimate every 5-th epoch).\n\n    References\n    ----------\n    [1] Immer, A., Bauer, M., Fortuin, V., R\u00e4tsch, G., Khan, EM.\n    [*Scalable Marginal Likelihood Estimation for Model Selection in Deep Learning*](https://arxiv.org/abs/2104.04975).\n    ICML 2021.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        torch neural network model (needs to comply with Backend choice)\n    train_loader : DataLoader\n        pytorch dataloader that implements `len(train_loader.dataset)` to obtain number of data points\n    likelihood : str, default=Likelihood.CLASSIFICATION\n        Likelihood.CLASSIFICATION or Likelihood.REGRESSION\n    hessian_structure : {'diag', 'kron', 'full'}, default='kron'\n        structure of the Hessian approximation\n    backend : Backend, default=AsdlGGN\n        Curvature subclass, e.g. AsdlGGN/AsdlEF or BackPackGGN/BackPackEF\n    optimizer_cls : torch.optim.Optimizer, default=Adam\n        optimizer to use for optimizing the neural network parameters togeth with `train_loader`\n    optimizer_kwargs : dict, default=None\n        keyword arguments for `optimizer_cls`, for example to change learning rate or momentum\n    scheduler_cls : torch.optim.lr_scheduler._LRScheduler, default=None\n        optionally, a scheduler to use on the learning rate of the optimizer.\n        `scheduler.step()` is called after every batch of the standard training.\n    scheduler_kwargs : dict, default=None\n        keyword arguments for `scheduler_cls`, e.g. `lr_min` for CosineAnnealingLR\n    n_epochs : int, default=300\n        number of epochs to train for\n    lr_hyp : float, default=0.1\n        Adam learning rate for hyperparameters\n    prior_structure : str, default='layerwise'\n        structure of the prior. one of `['scalar', 'layerwise', 'diag']`\n    n_epochs_burnin : int default=0\n        how many epochs to train without estimating and differentiating marglik\n    n_hypersteps : int, default=10\n        how many steps to take on the hyperparameters when marglik is estimated\n    marglik_frequency : int\n        how often to estimate (and differentiate) the marginal likelihood\n        `marglik_frequency=1` would be every epoch,\n        `marglik_frequency=5` would be every 5 epochs.\n    prior_prec_init : float, default=1.0\n        initial prior precision\n    sigma_noise_init : float, default=1.0\n        initial observation noise (for regression only)\n    temperature : float, default=1.0\n        factor for the likelihood for 'overcounting' data. Might be required for data augmentation.\n    fix_sigma_noise: bool, default=False\n        if False, optimize observation noise via marglik otherwise use `sigma_noise_init` throughout.\n        Only works for regression.\n    progress_bar: bool, default=False\n        whether to show a progress bar (updated per epoch) or not\n    enable_backprop : bool, default=False\n        make the returned Laplace instance backpropable---useful for e.g. Bayesian optimization.\n    dict_key_x: str, default='input_ids'\n        The dictionary key under which the input tensor `x` is stored. Only has effect\n        when the model takes a `MutableMapping` as the input. Useful for Huggingface\n        LLM models.\n    dict_key_y: str, default='labels'\n        The dictionary key under which the target tensor `y` is stored. Only has effect\n        when the model takes a `MutableMapping` as the input. Useful for Huggingface\n        LLM models.\n\n    Returns\n    -------\n    lap : laplace\n        fit Laplace approximation with the best obtained marginal likelihood during training\n    model : torch.nn.Module\n        corresponding model with the MAP parameters\n    margliks : list\n        list of marginal likelihoods obtained during training (to monitor convergence)\n    losses : list\n        list of losses (log joints) obtained during training (to monitor convergence)\n    \"\"\"\n    if optimizer_kwargs is not None and \"weight_decay\" in optimizer_kwargs:\n        warnings.warn(\"Weight decay is handled and optimized. Will be set to 0.\")\n        optimizer_kwargs[\"weight_decay\"] = 0.0\n\n    # get device, data set size N, number of layers H, number of parameters P\n    p = next(model.parameters())\n    device, dtype = p.device, p.dtype\n    N = len(train_loader.dataset)\n    trainable_params = [p for p in model.parameters() if p.requires_grad]\n    H = len(trainable_params)\n    P = len(parameters_to_vector(trainable_params))\n\n    # differentiable hyperparameters\n    hyperparameters = list()\n    # prior precision\n    log_prior_prec_init = np.log(temperature * prior_prec_init)\n    log_prior_prec = fix_prior_prec_structure(\n        log_prior_prec_init, prior_structure, H, P, device, dtype\n    )\n    log_prior_prec.requires_grad = True\n    hyperparameters.append(log_prior_prec)\n\n    # set up loss (and observation noise hyperparam)\n    if likelihood == Likelihood.CLASSIFICATION:\n        criterion = CrossEntropyLoss(reduction=\"mean\")\n        sigma_noise = 1.0\n    elif likelihood == Likelihood.REGRESSION:\n        criterion = MSELoss(reduction=\"mean\")\n        log_sigma_noise_init = np.log(sigma_noise_init)\n        log_sigma_noise = log_sigma_noise_init * torch.ones(\n            1, device=device, dtype=dtype\n        )\n        log_sigma_noise.requires_grad = True\n        hyperparameters.append(log_sigma_noise)\n\n    # set up model optimizer\n    if optimizer_kwargs is None:\n        optimizer_kwargs = dict()\n    optimizer = optimizer_cls(model.parameters(), **optimizer_kwargs)\n\n    # set up learning rate scheduler\n    scheduler = None\n    if scheduler_cls is not None:\n        if scheduler_kwargs is None:\n            scheduler_kwargs = dict()\n        scheduler = scheduler_cls(optimizer, **scheduler_kwargs)\n\n    # set up hyperparameter optimizer\n    hyper_optimizer = Adam(hyperparameters, lr=lr_hyp)\n\n    best_marglik = np.inf\n    best_model_dict = None\n    best_precision = None\n    losses = list()\n    margliks = list()\n\n    pbar = tqdm.trange(\n        1,\n        n_epochs + 1,\n        disable=not progress_bar,\n        position=1,\n        leave=False,\n        desc=\"[Training]\",\n        colour=\"blue\",\n    )\n\n    for epoch in pbar:\n        epoch_loss = 0\n        epoch_perf = 0\n\n        # standard NN training per batch\n        for data in train_loader:\n            if isinstance(data, MutableMapping):\n                X, y = data, data[dict_key_y]\n                y = y.to(device, non_blocking=True)\n            else:\n                X, y = data\n                X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n\n            optimizer.zero_grad()\n\n            if likelihood == Likelihood.REGRESSION:\n                sigma_noise = (\n                    torch.exp(log_sigma_noise).detach()\n                    if not fix_sigma_noise\n                    else sigma_noise_init\n                )\n                crit_factor = temperature / (2 * sigma_noise**2)\n            else:\n                crit_factor = temperature\n\n            prior_prec = torch.exp(log_prior_prec).detach()\n            theta = parameters_to_vector(\n                [p for p in model.parameters() if p.requires_grad]\n            )\n            delta = expand_prior_precision(prior_prec, model)\n\n            f = model(X)\n            loss = criterion(f, y) + (0.5 * (delta * theta) @ theta) / N / crit_factor\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.cpu().item() * len(y)\n\n            if likelihood == Likelihood.REGRESSION:\n                epoch_perf += (f.detach() - y).square().sum()\n            else:\n                epoch_perf += torch.sum(torch.argmax(f.detach(), dim=-1) == y).item()\n\n            if scheduler is not None:\n                scheduler.step()\n\n        losses.append(epoch_loss / N)\n\n        # compute validation error to report during training\n        logging.info(\n            f\"MARGLIK[epoch={epoch}]: network training. Loss={losses[-1]:.3f}.\"\n            + f\"Perf={epoch_perf/N:.3f}\"\n        )\n\n        # only update hyperparameters every marglik_frequency steps after burnin\n        if (epoch % marglik_frequency) != 0 or epoch &lt; n_epochs_burnin:\n            continue\n\n        # optimizer hyperparameters by differentiating marglik\n        # 1. fit laplace approximation\n        if likelihood == Likelihood.CLASSIFICATION:\n            sigma_noise = 1\n        else:\n            sigma_noise = (\n                torch.exp(log_sigma_noise) if not fix_sigma_noise else sigma_noise_init\n            )\n        prior_prec = torch.exp(log_prior_prec)\n        lap = Laplace(\n            model,\n            likelihood,\n            hessian_structure=hessian_structure,\n            sigma_noise=sigma_noise,\n            prior_precision=prior_prec,\n            temperature=temperature,\n            backend=backend,\n            subset_of_weights=\"all\",\n            dict_key_x=dict_key_x,\n            dict_key_y=dict_key_y,\n        )\n        lap.fit(train_loader)\n\n        # 2. differentiate wrt. hyperparameters for n_hypersteps\n        for _ in range(n_hypersteps):\n            hyper_optimizer.zero_grad()\n            if likelihood == Likelihood.CLASSIFICATION or fix_sigma_noise:\n                sigma_noise = None\n            else:\n                sigma_noise = torch.exp(log_sigma_noise)\n            prior_prec = torch.exp(log_prior_prec)\n            marglik = -lap.log_marginal_likelihood(prior_prec, sigma_noise)\n            marglik.backward()\n            hyper_optimizer.step()\n            margliks.append(marglik.item())\n\n        # early stopping on marginal likelihood\n        if margliks[-1] &lt; best_marglik:\n            best_model_dict = deepcopy(model.state_dict())\n            best_precision = deepcopy(prior_prec.detach())\n            if likelihood == Likelihood.CLASSIFICATION:\n                best_sigma = 1\n            else:\n                best_sigma = (\n                    deepcopy(sigma_noise.detach())\n                    if not fix_sigma_noise\n                    else sigma_noise_init\n                )\n            best_marglik = margliks[-1]\n            logging.info(\n                f\"MARGLIK[epoch={epoch}]: marglik optimization. MargLik={best_marglik:.2f}. \"\n                + \"Saving new best model.\"\n            )\n        else:\n            logging.info(\n                f\"MARGLIK[epoch={epoch}]: marglik optimization. MargLik={margliks[-1]:.2f}.\"\n                + f\"No improvement over {best_marglik:.2f}\"\n            )\n\n    logging.info(\"MARGLIK: finished training. Recover best model and fit Laplace.\")\n\n    if best_model_dict is not None:\n        model.load_state_dict(best_model_dict)\n        sigma_noise = best_sigma\n        prior_prec = best_precision\n    logging.info(f\"best params: {sigma_noise}, {prior_prec}\")\n\n    lap = Laplace(\n        model,\n        likelihood,\n        hessian_structure=hessian_structure,\n        sigma_noise=sigma_noise,\n        prior_precision=prior_prec,\n        temperature=temperature,\n        backend=backend,\n        subset_of_weights=SubsetOfWeights.ALL,\n        enable_backprop=enable_backprop,\n        dict_key_x=dict_key_x,\n        dict_key_y=dict_key_y,\n    )\n    lap.fit(train_loader)\n    return lap, model, margliks, losses\n</code></pre>"},{"location":"api_reference/marglik_training/#laplace.marglik_training.marglik_training(model)","title":"<code>model</code>","text":"(<code>Module</code>)           \u2013            <p>torch neural network model (needs to comply with Backend choice)</p>"},{"location":"api_reference/marglik_training/#laplace.marglik_training.marglik_training(train_loader)","title":"<code>train_loader</code>","text":"(<code>DataLoader</code>)           \u2013            <p>pytorch dataloader that implements <code>len(train_loader.dataset)</code> to obtain number of data points</p>"},{"location":"api_reference/marglik_training/#laplace.marglik_training.marglik_training(likelihood)","title":"<code>likelihood</code>","text":"(<code>str</code>, default:                   <code>Likelihood.CLASSIFICATION</code> )           \u2013            <p>Likelihood.CLASSIFICATION or Likelihood.REGRESSION</p>"},{"location":"api_reference/marglik_training/#laplace.marglik_training.marglik_training(hessian_structure)","title":"<code>hessian_structure</code>","text":"(<code>('diag', 'kron', 'full')</code>, default:                   <code>'diag'</code> )           \u2013            <p>structure of the Hessian approximation</p>"},{"location":"api_reference/marglik_training/#laplace.marglik_training.marglik_training(backend)","title":"<code>backend</code>","text":"(<code>Backend</code>, default:                   <code>AsdlGGN</code> )           \u2013            <p>Curvature subclass, e.g. AsdlGGN/AsdlEF or BackPackGGN/BackPackEF</p>"},{"location":"api_reference/marglik_training/#laplace.marglik_training.marglik_training(optimizer_cls)","title":"<code>optimizer_cls</code>","text":"(<code>Optimizer</code>, default:                   <code>Adam</code> )           \u2013            <p>optimizer to use for optimizing the neural network parameters togeth with <code>train_loader</code></p>"},{"location":"api_reference/marglik_training/#laplace.marglik_training.marglik_training(optimizer_kwargs)","title":"<code>optimizer_kwargs</code>","text":"(<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>keyword arguments for <code>optimizer_cls</code>, for example to change learning rate or momentum</p>"},{"location":"api_reference/marglik_training/#laplace.marglik_training.marglik_training(scheduler_cls)","title":"<code>scheduler_cls</code>","text":"(<code>_LRScheduler</code>, default:                   <code>None</code> )           \u2013            <p>optionally, a scheduler to use on the learning rate of the optimizer. <code>scheduler.step()</code> is called after every batch of the standard training.</p>"},{"location":"api_reference/marglik_training/#laplace.marglik_training.marglik_training(scheduler_kwargs)","title":"<code>scheduler_kwargs</code>","text":"(<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>keyword arguments for <code>scheduler_cls</code>, e.g. <code>lr_min</code> for CosineAnnealingLR</p>"},{"location":"api_reference/marglik_training/#laplace.marglik_training.marglik_training(n_epochs)","title":"<code>n_epochs</code>","text":"(<code>int</code>, default:                   <code>300</code> )           \u2013            <p>number of epochs to train for</p>"},{"location":"api_reference/marglik_training/#laplace.marglik_training.marglik_training(lr_hyp)","title":"<code>lr_hyp</code>","text":"(<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>Adam learning rate for hyperparameters</p>"},{"location":"api_reference/marglik_training/#laplace.marglik_training.marglik_training(prior_structure)","title":"<code>prior_structure</code>","text":"(<code>str</code>, default:                   <code>'layerwise'</code> )           \u2013            <p>structure of the prior. one of <code>['scalar', 'layerwise', 'diag']</code></p>"},{"location":"api_reference/marglik_training/#laplace.marglik_training.marglik_training(n_epochs_burnin)","title":"<code>n_epochs_burnin</code>","text":"(<code>int default=0</code>, default:                   <code>0</code> )           \u2013            <p>how many epochs to train without estimating and differentiating marglik</p>"},{"location":"api_reference/marglik_training/#laplace.marglik_training.marglik_training(n_hypersteps)","title":"<code>n_hypersteps</code>","text":"(<code>int</code>, default:                   <code>10</code> )           \u2013            <p>how many steps to take on the hyperparameters when marglik is estimated</p>"},{"location":"api_reference/marglik_training/#laplace.marglik_training.marglik_training(marglik_frequency)","title":"<code>marglik_frequency</code>","text":"(<code>int</code>, default:                   <code>1</code> )           \u2013            <p>how often to estimate (and differentiate) the marginal likelihood <code>marglik_frequency=1</code> would be every epoch, <code>marglik_frequency=5</code> would be every 5 epochs.</p>"},{"location":"api_reference/marglik_training/#laplace.marglik_training.marglik_training(prior_prec_init)","title":"<code>prior_prec_init</code>","text":"(<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>initial prior precision</p>"},{"location":"api_reference/marglik_training/#laplace.marglik_training.marglik_training(sigma_noise_init)","title":"<code>sigma_noise_init</code>","text":"(<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>initial observation noise (for regression only)</p>"},{"location":"api_reference/marglik_training/#laplace.marglik_training.marglik_training(temperature)","title":"<code>temperature</code>","text":"(<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>factor for the likelihood for 'overcounting' data. Might be required for data augmentation.</p>"},{"location":"api_reference/marglik_training/#laplace.marglik_training.marglik_training(fix_sigma_noise)","title":"<code>fix_sigma_noise</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>if False, optimize observation noise via marglik otherwise use <code>sigma_noise_init</code> throughout. Only works for regression.</p>"},{"location":"api_reference/marglik_training/#laplace.marglik_training.marglik_training(progress_bar)","title":"<code>progress_bar</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to show a progress bar (updated per epoch) or not</p>"},{"location":"api_reference/marglik_training/#laplace.marglik_training.marglik_training(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>make the returned Laplace instance backpropable---useful for e.g. Bayesian optimization.</p>"},{"location":"api_reference/marglik_training/#laplace.marglik_training.marglik_training(dict_key_x)","title":"<code>dict_key_x</code>","text":"(<code>str</code>, default:                   <code>'input_ids'</code> )           \u2013            <p>The dictionary key under which the input tensor <code>x</code> is stored. Only has effect when the model takes a <code>MutableMapping</code> as the input. Useful for Huggingface LLM models.</p>"},{"location":"api_reference/marglik_training/#laplace.marglik_training.marglik_training(dict_key_y)","title":"<code>dict_key_y</code>","text":"(<code>str</code>, default:                   <code>'labels'</code> )           \u2013            <p>The dictionary key under which the target tensor <code>y</code> is stored. Only has effect when the model takes a <code>MutableMapping</code> as the input. Useful for Huggingface LLM models.</p>"},{"location":"api_reference/parametriclaplace/","title":"Parametric Laplace","text":""},{"location":"api_reference/parametriclaplace/#laplace.baselaplace","title":"laplace.baselaplace","text":"<p>Classes:</p> <ul> <li> <code>ParametricLaplace</code>           \u2013            <p>Parametric Laplace class.</p> </li> <li> <code>DiagLaplace</code>           \u2013            <p>Laplace approximation with diagonal log likelihood Hessian approximation</p> </li> <li> <code>KronLaplace</code>           \u2013            <p>Laplace approximation with Kronecker factored log likelihood Hessian approximation</p> </li> <li> <code>LowRankLaplace</code>           \u2013            <p>Laplace approximation with low-rank log likelihood Hessian (approximation).</p> </li> <li> <code>FullLaplace</code>           \u2013            <p>Laplace approximation with full, i.e., dense, log likelihood Hessian approximation</p> </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace","title":"ParametricLaplace","text":"<pre><code>ParametricLaplace(model: Module, likelihood: Likelihood | str, sigma_noise: float | Tensor = 1.0, prior_precision: float | Tensor = 1.0, prior_mean: float | Tensor = 0.0, temperature: float = 1.0, enable_backprop: bool = False, dict_key_x: str = 'input_ids', dict_key_y: str = 'labels', backend: type[CurvatureInterface] | None = None, backend_kwargs: dict[str, Any] | None = None, asdl_fisher_kwargs: dict[str, Any] | None = None)\n</code></pre> <p>               Bases: <code>BaseLaplace</code></p> <p>Parametric Laplace class.</p> <p>Subclasses need to specify how the Hessian approximation is initialized, how to add up curvature over training data, how to sample from the Laplace approximation, and how to compute the functional variance.</p> <p>A Laplace approximation is represented by a MAP which is given by the <code>model</code> parameter and a posterior precision or covariance specifying a Gaussian distribution \\(\\mathcal{N}(\\theta_{MAP}, P^{-1})\\). The goal of this class is to compute the posterior precision \\(P\\) which sums as</p> \\[     P = \\sum_{n=1}^N \\nabla^2_\\theta \\log p(\\mathcal{D}_n \\mid \\theta)     \\vert_{\\theta_{MAP}} + \\nabla^2_\\theta \\log p(\\theta) \\vert_{\\theta_{MAP}}. \\] <p>Every subclass implements different approximations to the log likelihood Hessians, for example, a diagonal one. The prior is assumed to be Gaussian and therefore we have a simple form for \\(\\nabla^2_\\theta \\log p(\\theta) \\vert_{\\theta_{MAP}} = P_0 \\). In particular, we assume a scalar, layer-wise, or diagonal prior precision so that in all cases \\(P_0 = \\textrm{diag}(p_0)\\) and the structure of \\(p_0\\) can be varied.</p> <p>Methods:</p> <ul> <li> <code>fit</code>             \u2013              <p>Fit the local Laplace approximation at the parameters of the model.</p> </li> <li> <code>square_norm</code>             \u2013              <p>Compute the square norm under post. Precision with <code>value-self.mean</code> as \ud835\udee5:</p> </li> <li> <code>log_prob</code>             \u2013              <p>Compute the log probability under the (current) Laplace approximation.</p> </li> <li> <code>log_marginal_likelihood</code>             \u2013              <p>Compute the Laplace approximation to the log marginal likelihood subject</p> </li> <li> <code>__call__</code>             \u2013              <p>Compute the posterior predictive on input data <code>x</code>.</p> </li> <li> <code>functional_samples</code>             \u2013              <p>Sample from the function-space posterior on input data <code>x</code>.</p> </li> <li> <code>predictive_samples</code>             \u2013              <p>Sample from the posterior predictive on input data <code>x</code>. I.e., the respective</p> </li> <li> <code>functional_variance</code>             \u2013              <p>Compute functional variance for the <code>'glm'</code> predictive:</p> </li> <li> <code>functional_covariance</code>             \u2013              <p>Compute functional covariance for the <code>'glm'</code> predictive:</p> </li> <li> <code>sample</code>             \u2013              <p>Sample from the Laplace posterior approximation, i.e.,</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>log_likelihood</code>               (<code>Tensor</code>)           \u2013            <p>Compute log likelihood on the training data after <code>.fit()</code> has been called.</p> </li> <li> <code>prior_precision_diag</code>               (<code>Tensor</code>)           \u2013            <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either</p> </li> <li> <code>scatter</code>               (<code>Tensor</code>)           \u2013            <p>Computes the scatter, a term of the log marginal likelihood that</p> </li> <li> <code>log_det_prior_precision</code>               (<code>Tensor</code>)           \u2013            <p>Compute log determinant of the prior precision</p> </li> <li> <code>log_det_posterior_precision</code>               (<code>Tensor</code>)           \u2013            <p>Compute log determinant of the posterior precision</p> </li> <li> <code>log_det_ratio</code>               (<code>Tensor</code>)           \u2013            <p>Compute the log determinant ratio, a part of the log marginal likelihood.</p> </li> <li> <code>posterior_precision</code>               (<code>Tensor</code>)           \u2013            <p>Compute or return the posterior precision \\(P\\).</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    sigma_noise: float | torch.Tensor = 1.0,\n    prior_precision: float | torch.Tensor = 1.0,\n    prior_mean: float | torch.Tensor = 0.0,\n    temperature: float = 1.0,\n    enable_backprop: bool = False,\n    dict_key_x: str = \"input_ids\",\n    dict_key_y: str = \"labels\",\n    backend: type[CurvatureInterface] | None = None,\n    backend_kwargs: dict[str, Any] | None = None,\n    asdl_fisher_kwargs: dict[str, Any] | None = None,\n):\n    super().__init__(\n        model,\n        likelihood,\n        sigma_noise,\n        prior_precision,\n        prior_mean,\n        temperature,\n        enable_backprop,\n        dict_key_x,\n        dict_key_y,\n        backend,\n        backend_kwargs,\n        asdl_fisher_kwargs,\n    )\n    if not hasattr(self, \"H\"):\n        self._init_H()\n        # posterior mean/mode\n        self.mean: float | torch.Tensor = self.prior_mean\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.log_likelihood","title":"log_likelihood","text":"<pre><code>log_likelihood: Tensor\n</code></pre> <p>Compute log likelihood on the training data after <code>.fit()</code> has been called. The log likelihood is computed on-demand based on the loss and, for example, the observation noise which makes it differentiable in the latter for iterative updates.</p> <p>Returns:</p> <ul> <li> <code>log_likelihood</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.prior_precision_diag","title":"prior_precision_diag","text":"<pre><code>prior_precision_diag: Tensor\n</code></pre> <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either a scalar, layer-wise, or diagonal prior precision.</p> <p>Returns:</p> <ul> <li> <code>prior_precision_diag</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.scatter","title":"scatter","text":"<pre><code>scatter: Tensor\n</code></pre> <p>Computes the scatter, a term of the log marginal likelihood that corresponds to L-2 regularization: <code>scatter</code> = \\((\\theta_{MAP} - \\mu_0)^{T} P_0 (\\theta_{MAP} - \\mu_0) \\).</p> <p>Returns:</p> <ul> <li> <code>scatter</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.log_det_prior_precision","title":"log_det_prior_precision","text":"<pre><code>log_det_prior_precision: Tensor\n</code></pre> <p>Compute log determinant of the prior precision \\(\\log \\det P_0\\)</p> <p>Returns:</p> <ul> <li> <code>log_det</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.log_det_posterior_precision","title":"log_det_posterior_precision","text":"<pre><code>log_det_posterior_precision: Tensor\n</code></pre> <p>Compute log determinant of the posterior precision \\(\\log \\det P\\) which depends on the subclasses structure used for the Hessian approximation.</p> <p>Returns:</p> <ul> <li> <code>log_det</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.log_det_ratio","title":"log_det_ratio","text":"<pre><code>log_det_ratio: Tensor\n</code></pre> <p>Compute the log determinant ratio, a part of the log marginal likelihood.</p> \\[     \\log \\frac{\\det P}{\\det P_0} = \\log \\det P - \\log \\det P_0 \\] <p>Returns:</p> <ul> <li> <code>log_det_ratio</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.posterior_precision","title":"posterior_precision","text":"<pre><code>posterior_precision: Tensor\n</code></pre> <p>Compute or return the posterior precision \\(P\\).</p> <p>Returns:</p> <ul> <li> <code>posterior_prec</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace._glm_forward_call","title":"_glm_forward_call","text":"<pre><code>_glm_forward_call(x: Tensor | MutableMapping, likelihood: Likelihood | str, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code> for \"glm\" pred type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_forward_call(\n    self,\n    x: torch.Tensor | MutableMapping,\n    likelihood: Likelihood | str,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x` for \"glm\" pred type.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    likelihood : Likelihood or str in {'classification', 'regression', 'reward_modeling'}\n        determines the log likelihood Hessian approximation.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` and `link_approx='mc'`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    f_mu, f_var = self._glm_predictive_distribution(\n        x, joint=joint and likelihood == Likelihood.REGRESSION\n    )\n\n    if likelihood == Likelihood.REGRESSION:\n        if diagonal_output and not joint:\n            f_var = torch.diagonal(f_var, dim1=-2, dim2=-1)\n        return f_mu, f_var\n\n    if link_approx == LinkApprox.MC:\n        return self._glm_predictive_samples(\n            f_mu,\n            f_var,\n            n_samples=n_samples,\n            diagonal_output=diagonal_output,\n        ).mean(dim=0)\n    elif link_approx == LinkApprox.PROBIT:\n        kappa = 1 / torch.sqrt(1.0 + np.pi / 8 * f_var.diagonal(dim1=1, dim2=2))\n        return torch.softmax(kappa * f_mu, dim=-1)\n    elif \"bridge\" in link_approx:\n        # zero mean correction\n        f_mu -= (\n            f_var.sum(-1)\n            * f_mu.sum(-1).reshape(-1, 1)\n            / f_var.sum(dim=(1, 2)).reshape(-1, 1)\n        )\n        f_var -= torch.einsum(\n            \"bi,bj-&gt;bij\", f_var.sum(-1), f_var.sum(-2)\n        ) / f_var.sum(dim=(1, 2)).reshape(-1, 1, 1)\n\n        # Laplace Bridge\n        _, K = f_mu.size(0), f_mu.size(-1)\n        f_var_diag = torch.diagonal(f_var, dim1=1, dim2=2)\n\n        # optional: variance correction\n        if link_approx == LinkApprox.BRIDGE_NORM:\n            f_var_diag_mean = f_var_diag.mean(dim=1)\n            f_var_diag_mean /= torch.as_tensor(\n                [K / 2], device=self._device, dtype=self._dtype\n            ).sqrt()\n            f_mu /= f_var_diag_mean.sqrt().unsqueeze(-1)\n            f_var_diag /= f_var_diag_mean.unsqueeze(-1)\n\n        sum_exp = torch.exp(-f_mu).sum(dim=1).unsqueeze(-1)\n        alpha = (1 - 2 / K + f_mu.exp() / K**2 * sum_exp) / f_var_diag\n        return torch.nan_to_num(alpha / alpha.sum(dim=1).unsqueeze(-1), nan=1.0)\n    else:\n        raise ValueError(\n            \"Prediction path invalid. Check the likelihood, pred_type, link_approx combination!\"\n        )\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace._glm_forward_call(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace._glm_forward_call(likelihood)","title":"<code>likelihood</code>","text":"(<code>Likelihood or str in {'classification', 'regression', 'reward_modeling'}</code>)           \u2013            <p>determines the log likelihood Hessian approximation.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace._glm_forward_call(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace._glm_forward_call(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace._glm_forward_call(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace._glm_forward_call(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> and <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace._glm_functional_samples","title":"_glm_functional_samples","text":"<pre><code>_glm_functional_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior functional on input data <code>x</code> using \"glm\" prediction type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_functional_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior functional on input data `x` using \"glm\" prediction\n    type.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    assert f_var.shape == torch.Size([f_mu.shape[0], f_mu.shape[1], f_mu.shape[1]])\n\n    if diagonal_output:\n        f_var = torch.diagonal(f_var, dim1=1, dim2=2)\n\n    return normal_samples(f_mu, f_var, n_samples, generator)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace._glm_functional_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace._glm_functional_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace._glm_functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace._glm_functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace._glm_functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace._glm_predictive_samples","title":"_glm_predictive_samples","text":"<pre><code>_glm_predictive_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code> using \"glm\" prediction type. I.e., the inverse-link function correponding to the likelihood is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_predictive_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x` using \"glm\" prediction\n    type. I.e., the inverse-link function correponding to the likelihood is applied\n    on top of the functional sample.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    f_samples = self._glm_functional_samples(\n        f_mu, f_var, n_samples, diagonal_output, generator\n    )\n\n    if self.likelihood == Likelihood.REGRESSION:\n        return f_samples\n    else:\n        return torch.softmax(f_samples, dim=-1)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace._glm_predictive_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace._glm_predictive_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace._glm_predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace._glm_predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace._glm_predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.fit","title":"fit","text":"<pre><code>fit(train_loader: DataLoader, override: bool = True, progress_bar: bool = False) -&gt; None\n</code></pre> <p>Fit the local Laplace approximation at the parameters of the model.</p> <p>Parameters:</p> Source code in <code>laplace/baselaplace.py</code> <pre><code>def fit(\n    self,\n    train_loader: DataLoader,\n    override: bool = True,\n    progress_bar: bool = False,\n) -&gt; None:\n    \"\"\"Fit the local Laplace approximation at the parameters of the model.\n\n    Parameters\n    ----------\n    train_loader : torch.data.utils.DataLoader\n        each iterate is a training batch, either `(X, y)` tensors or a dict-like\n        object containing keys as expressed by `self.dict_key_x` and\n        `self.dict_key_y`. `train_loader.dataset` needs to be set to access\n        \\\\(N\\\\), size of the data set.\n    override : bool, default=True\n        whether to initialize H, loss, and n_data again; setting to False is useful for\n        online learning settings to accumulate a sequential posterior approximation.\n    progress_bar : bool, default=False\n        whether to show a progress bar; updated at every batch-Hessian computation.\n        Useful for very large model and large amount of data, esp. when `subset_of_weights='all'`.\n    \"\"\"\n    if override:\n        self._init_H()\n        self.loss: float | torch.Tensor = 0\n        self.n_data: int = 0\n\n    self.model.eval()\n\n    self.mean: torch.Tensor = parameters_to_vector(self.params)\n    if not self.enable_backprop:\n        self.mean = self.mean.detach()\n\n    data: (\n        tuple[torch.Tensor, torch.Tensor] | MutableMapping[str, torch.Tensor | Any]\n    ) = next(iter(train_loader))\n\n    with torch.no_grad():\n        if isinstance(data, MutableMapping):  # To support Huggingface dataset\n            if \"backpack\" in self._backend_cls.__name__.lower() or (\n                isinstance(self, DiagLaplace) and self._backend_cls == CurvlinopsEF\n            ):\n                raise ValueError(\n                    \"Currently DiagEF is not supported under CurvlinopsEF backend \"\n                    + \"for custom models with non-tensor inputs \"\n                    + \"(https://github.com/pytorch/functorch/issues/159). Consider \"\n                    + \"using AsdlEF backend instead. The same limitation applies \"\n                    + \"to all BackPACK backend\"\n                )\n\n            out = self.model(data)\n        else:\n            X = data[0]\n            try:\n                out = self.model(X[:1].to(self._device))\n            except (TypeError, AttributeError):\n                out = self.model(X.to(self._device))\n    self.n_outputs = out.shape[-1]\n    setattr(self.model, \"output_size\", self.n_outputs)\n\n    N = len(train_loader.dataset)\n\n    pbar = tqdm.tqdm(train_loader, disable=not progress_bar)\n    pbar.set_description(\"[Computing Hessian]\")\n\n    for data in pbar:\n        if isinstance(data, MutableMapping):  # To support Huggingface dataset\n            X, y = data, data[self.dict_key_y].to(self._device)\n        else:\n            X, y = data\n            X, y = X.to(self._device), y.to(self._device)\n\n        if self.likelihood == Likelihood.REGRESSION and y.ndim != out.ndim:\n            raise ValueError(\n                f\"The model's output has {out.ndim} dims but \"\n                f\"the target has {y.ndim} dims.\"\n            )\n\n        self.model.zero_grad()\n        loss_batch, H_batch = self._curv_closure(X, y, N=N)\n        self.loss += loss_batch\n        self.H += H_batch\n\n    self.n_data += N\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.fit(train_loader)","title":"<code>train_loader</code>","text":"(<code>DataLoader</code>)           \u2013            <p>each iterate is a training batch, either <code>(X, y)</code> tensors or a dict-like object containing keys as expressed by <code>self.dict_key_x</code> and <code>self.dict_key_y</code>. <code>train_loader.dataset</code> needs to be set to access \\(N\\), size of the data set.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.fit(override)","title":"<code>override</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to initialize H, loss, and n_data again; setting to False is useful for online learning settings to accumulate a sequential posterior approximation.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.fit(progress_bar)","title":"<code>progress_bar</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to show a progress bar; updated at every batch-Hessian computation. Useful for very large model and large amount of data, esp. when <code>subset_of_weights='all'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.square_norm","title":"square_norm","text":"<pre><code>square_norm(value) -&gt; Tensor\n</code></pre> <p>Compute the square norm under post. Precision with <code>value-self.mean</code> as \ud835\udee5:</p> \\[     \\Delta^     op P \\Delta \\] <p>Returns:</p> <ul> <li> <code>square_form</code>           \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def square_norm(self, value) -&gt; torch.Tensor:\n    \"\"\"Compute the square norm under post. Precision with `value-self.mean` as \ud835\udee5:\n\n    $$\n        \\\\Delta^\\top P \\\\Delta\n    $$\n\n    Returns\n    -------\n    square_form\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.log_prob","title":"log_prob","text":"<pre><code>log_prob(value: Tensor, normalized: bool = True) -&gt; Tensor\n</code></pre> <p>Compute the log probability under the (current) Laplace approximation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>log_prob</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def log_prob(self, value: torch.Tensor, normalized: bool = True) -&gt; torch.Tensor:\n    \"\"\"Compute the log probability under the (current) Laplace approximation.\n\n    Parameters\n    ----------\n    value: torch.Tensor\n    normalized : bool, default=True\n        whether to return log of a properly normalized Gaussian or just the\n        terms that depend on `value`.\n\n    Returns\n    -------\n    log_prob : torch.Tensor\n    \"\"\"\n    if not normalized:\n        return -self.square_norm(value) / 2\n    log_prob = (\n        -self.n_params / 2 * log(2 * pi) + self.log_det_posterior_precision / 2\n    )\n    log_prob -= self.square_norm(value) / 2\n    return log_prob\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.log_prob(value)","title":"<code>value</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.log_prob(normalized)","title":"<code>normalized</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to return log of a properly normalized Gaussian or just the terms that depend on <code>value</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.log_marginal_likelihood","title":"log_marginal_likelihood","text":"<pre><code>log_marginal_likelihood(prior_precision: Tensor | None = None, sigma_noise: Tensor | None = None) -&gt; Tensor\n</code></pre> <p>Compute the Laplace approximation to the log marginal likelihood subject to specific Hessian approximations that subclasses implement. Requires that the Laplace approximation has been fit before. The resulting torch.Tensor is differentiable in <code>prior_precision</code> and <code>sigma_noise</code> if these have gradients enabled. By passing <code>prior_precision</code> or <code>sigma_noise</code>, the current value is overwritten. This is useful for iterating on the log marginal likelihood.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>log_marglik</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def log_marginal_likelihood(\n    self,\n    prior_precision: torch.Tensor | None = None,\n    sigma_noise: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the Laplace approximation to the log marginal likelihood subject\n    to specific Hessian approximations that subclasses implement.\n    Requires that the Laplace approximation has been fit before.\n    The resulting torch.Tensor is differentiable in `prior_precision` and\n    `sigma_noise` if these have gradients enabled.\n    By passing `prior_precision` or `sigma_noise`, the current value is\n    overwritten. This is useful for iterating on the log marginal likelihood.\n\n    Parameters\n    ----------\n    prior_precision : torch.Tensor, optional\n        prior precision if should be changed from current `prior_precision` value\n    sigma_noise : torch.Tensor, optional\n        observation noise standard deviation if should be changed\n\n    Returns\n    -------\n    log_marglik : torch.Tensor\n    \"\"\"\n    # update prior precision (useful when iterating on marglik)\n    if prior_precision is not None:\n        self.prior_precision = prior_precision\n\n    # update sigma_noise (useful when iterating on marglik)\n    if sigma_noise is not None:\n        if self.likelihood != Likelihood.REGRESSION:\n            raise ValueError(\"Can only change sigma_noise for regression.\")\n\n        self.sigma_noise = sigma_noise\n\n    return self.log_likelihood - 0.5 * (self.log_det_ratio + self.scatter)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.log_marginal_likelihood(prior_precision)","title":"<code>prior_precision</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>prior precision if should be changed from current <code>prior_precision</code> value</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.log_marginal_likelihood(sigma_noise)","title":"<code>sigma_noise</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>observation noise standard deviation if should be changed</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.__call__","title":"__call__","text":"<pre><code>__call__(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None, fitting: bool = False, **model_kwargs: dict[str, Any]) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def __call__(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n    fitting: bool = False,\n    **model_kwargs: dict[str, Any],\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x`.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here. When Laplace is done only\n        on subset of parameters (i.e. some grad are disabled),\n        only `nn` predictive is supported.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` when `joint=False` in regression.\n        In the case of last-layer Laplace with a diagonal or Kron Hessian,\n        setting this to `True` makes computation much(!) faster for large\n        number of outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used).\n\n    fitting : bool, default=False\n        whether or not this predictive call is done during fitting. Only useful for\n        reward modeling: the likelihood is set to `\"regression\"` when `False` and\n        `\"classification\"` when `True`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    if pred_type not in [pred for pred in PredType]:\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if link_approx not in [la for la in LinkApprox]:\n        raise ValueError(f\"Unsupported link approximation {link_approx}.\")\n\n    if pred_type == PredType.NN and link_approx != LinkApprox.MC:\n        raise ValueError(\n            \"Only mc link approximation is supported for nn prediction type.\"\n        )\n\n    if generator is not None:\n        if (\n            not isinstance(generator, torch.Generator)\n            or generator.device != self._device\n        ):\n            raise ValueError(\"Invalid random generator (check type and device).\")\n\n    likelihood = self.likelihood\n    if likelihood == Likelihood.REWARD_MODELING:\n        likelihood = Likelihood.CLASSIFICATION if fitting else Likelihood.REGRESSION\n\n    if pred_type == PredType.GLM:\n        return self._glm_forward_call(\n            x, likelihood, joint, link_approx, n_samples, diagonal_output\n        )\n    else:\n        if likelihood == Likelihood.REGRESSION:\n            samples = self._nn_predictive_samples(x, n_samples, **model_kwargs)\n            return samples.mean(dim=0), samples.var(dim=0)\n        else:  # classification; the average is computed online\n            return self._nn_predictive_classification(x, n_samples, **model_kwargs)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.__call__(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.__call__(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here. When Laplace is done only on subset of parameters (i.e. some grad are disabled), only <code>nn</code> predictive is supported.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.__call__(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.__call__(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.__call__(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.__call__(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> when <code>joint=False</code> in regression. In the case of last-layer Laplace with a diagonal or Kron Hessian, setting this to <code>True</code> makes computation much(!) faster for large number of outputs.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.__call__(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used).</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.__call__(fitting)","title":"<code>fitting</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether or not this predictive call is done during fitting. Only useful for reward modeling: the likelihood is set to <code>\"regression\"</code> when <code>False</code> and <code>\"classification\"</code> when <code>True</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.functional_samples","title":"functional_samples","text":"<pre><code>functional_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the function-space posterior on input data <code>x</code>. Can be used, for example, for Thompson sampling or to compute an arbitrary expectation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def functional_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the function-space posterior on input data `x`.\n    Can be used, for example, for Thompson sampling or to compute an arbitrary\n    expectation.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if pred_type == PredType.GLM:\n        f_mu, f_var = self._glm_predictive_distribution(x)\n        return self._glm_functional_samples(\n            f_mu, f_var, n_samples, diagonal_output, generator\n        )\n    else:  # 'nn'\n        return self._nn_functional_samples(x, n_samples, generator)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.functional_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.functional_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.predictive_samples","title":"predictive_samples","text":"<pre><code>predictive_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code>. I.e., the respective inverse-link function (e.g. softmax) is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def predictive_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x`. I.e., the respective\n    inverse-link function (e.g. softmax) is applied on top of the functional\n    sample.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if pred_type == PredType.GLM:\n        f_mu, f_var = self._glm_predictive_distribution(x)\n        return self._glm_predictive_samples(\n            f_mu, f_var, n_samples, diagonal_output, generator\n        )\n    else:  # 'nn'\n        return self._nn_predictive_samples(x, n_samples, generator)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.predictive_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.predictive_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.functional_variance","title":"functional_variance","text":"<pre><code>functional_variance(Js: Tensor) -&gt; Tensor\n</code></pre> <p>Compute functional variance for the <code>'glm'</code> predictive: <code>f_var[i] = Js[i] @ P.inv() @ Js[i].T</code>, which is a output x output predictive covariance matrix. Mathematically, we have for a single Jacobian \\(\\mathcal{J} = \\nabla_\\theta f(x;\\theta)\\vert_{\\theta_{MAP}}\\) the output covariance matrix \\( \\mathcal{J} P^{-1} \\mathcal{J}^T \\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>f_var</code> (              <code>Tensor</code> )          \u2013            <p>output covariance <code>(batch, outputs, outputs)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def functional_variance(self, Js: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute functional variance for the `'glm'` predictive:\n    `f_var[i] = Js[i] @ P.inv() @ Js[i].T`, which is a output x output\n    predictive covariance matrix.\n    Mathematically, we have for a single Jacobian\n    \\\\(\\\\mathcal{J} = \\\\nabla_\\\\theta f(x;\\\\theta)\\\\vert_{\\\\theta_{MAP}}\\\\)\n    the output covariance matrix\n    \\\\( \\\\mathcal{J} P^{-1} \\\\mathcal{J}^T \\\\).\n\n    Parameters\n    ----------\n    Js : torch.Tensor\n        Jacobians of model output wrt parameters\n        `(batch, outputs, parameters)`\n\n    Returns\n    -------\n    f_var : torch.Tensor\n        output covariance `(batch, outputs, outputs)`\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.functional_variance(Js)","title":"<code>Js</code>","text":"(<code>Tensor</code>)           \u2013            <p>Jacobians of model output wrt parameters <code>(batch, outputs, parameters)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.functional_covariance","title":"functional_covariance","text":"<pre><code>functional_covariance(Js: Tensor) -&gt; Tensor\n</code></pre> <p>Compute functional covariance for the <code>'glm'</code> predictive: <code>f_cov = Js @ P.inv() @ Js.T</code>, which is a batchoutput x batchoutput predictive covariance matrix.</p> <p>This emulates the GP posterior covariance N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). Useful for joint predictions, such as in batched Bayesian optimization.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>f_cov</code> (              <code>Tensor</code> )          \u2013            <p>output covariance <code>(batch*outputs, batch*outputs)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def functional_covariance(self, Js: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute functional covariance for the `'glm'` predictive:\n    `f_cov = Js @ P.inv() @ Js.T`, which is a batch*output x batch*output\n    predictive covariance matrix.\n\n    This emulates the GP posterior covariance N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n    Useful for joint predictions, such as in batched Bayesian optimization.\n\n    Parameters\n    ----------\n    Js : torch.Tensor\n        Jacobians of model output wrt parameters\n        `(batch*outputs, parameters)`\n\n    Returns\n    -------\n    f_cov : torch.Tensor\n        output covariance `(batch*outputs, batch*outputs)`\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.functional_covariance(Js)","title":"<code>Js</code>","text":"(<code>Tensor</code>)           \u2013            <p>Jacobians of model output wrt parameters <code>(batch*outputs, parameters)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.sample","title":"sample","text":"<pre><code>sample(n_samples: int = 100, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the Laplace posterior approximation, i.e., \\( \\theta \\sim \\mathcal{N}(\\theta_{MAP}, P^{-1})\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def sample(\n    self, n_samples: int = 100, generator: torch.Generator | None = None\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the Laplace posterior approximation, i.e.,\n    \\\\( \\\\theta \\\\sim \\\\mathcal{N}(\\\\theta_{MAP}, P^{-1})\\\\).\n\n    Parameters\n    ----------\n    n_samples : int, default=100\n        number of samples\n\n    generator : torch.Generator, optional\n        random number generator to control the samples\n\n    Returns\n    -------\n    samples: torch.Tensor\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.sample(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.ParametricLaplace.sample(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace","title":"DiagLaplace","text":"<pre><code>DiagLaplace(model: Module, likelihood: Likelihood | str, sigma_noise: float | Tensor = 1.0, prior_precision: float | Tensor = 1.0, prior_mean: float | Tensor = 0.0, temperature: float = 1.0, enable_backprop: bool = False, dict_key_x: str = 'input_ids', dict_key_y: str = 'labels', backend: type[CurvatureInterface] | None = None, backend_kwargs: dict[str, Any] | None = None, asdl_fisher_kwargs: dict[str, Any] | None = None)\n</code></pre> <p>               Bases: <code>ParametricLaplace</code></p> <p>Laplace approximation with diagonal log likelihood Hessian approximation and hence posterior precision. Mathematically, we have \\(P \\approx \\textrm{diag}(P)\\). See <code>BaseLaplace</code> for the full interface.</p> <p>Methods:</p> <ul> <li> <code>fit</code>             \u2013              <p>Fit the local Laplace approximation at the parameters of the model.</p> </li> <li> <code>log_marginal_likelihood</code>             \u2013              <p>Compute the Laplace approximation to the log marginal likelihood subject</p> </li> <li> <code>__call__</code>             \u2013              <p>Compute the posterior predictive on input data <code>x</code>.</p> </li> <li> <code>log_prob</code>             \u2013              <p>Compute the log probability under the (current) Laplace approximation.</p> </li> <li> <code>functional_samples</code>             \u2013              <p>Sample from the function-space posterior on input data <code>x</code>.</p> </li> <li> <code>predictive_samples</code>             \u2013              <p>Sample from the posterior predictive on input data <code>x</code>. I.e., the respective</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>log_likelihood</code>               (<code>Tensor</code>)           \u2013            <p>Compute log likelihood on the training data after <code>.fit()</code> has been called.</p> </li> <li> <code>prior_precision_diag</code>               (<code>Tensor</code>)           \u2013            <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either</p> </li> <li> <code>scatter</code>               (<code>Tensor</code>)           \u2013            <p>Computes the scatter, a term of the log marginal likelihood that</p> </li> <li> <code>log_det_prior_precision</code>               (<code>Tensor</code>)           \u2013            <p>Compute log determinant of the prior precision</p> </li> <li> <code>log_det_ratio</code>               (<code>Tensor</code>)           \u2013            <p>Compute the log determinant ratio, a part of the log marginal likelihood.</p> </li> <li> <code>posterior_precision</code>               (<code>Tensor</code>)           \u2013            <p>Diagonal posterior precision \\(p\\).</p> </li> <li> <code>posterior_scale</code>               (<code>Tensor</code>)           \u2013            <p>Diagonal posterior scale \\(\\sqrt{p^{-1}}\\).</p> </li> <li> <code>posterior_variance</code>               (<code>Tensor</code>)           \u2013            <p>Diagonal posterior variance \\(p^{-1}\\).</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    sigma_noise: float | torch.Tensor = 1.0,\n    prior_precision: float | torch.Tensor = 1.0,\n    prior_mean: float | torch.Tensor = 0.0,\n    temperature: float = 1.0,\n    enable_backprop: bool = False,\n    dict_key_x: str = \"input_ids\",\n    dict_key_y: str = \"labels\",\n    backend: type[CurvatureInterface] | None = None,\n    backend_kwargs: dict[str, Any] | None = None,\n    asdl_fisher_kwargs: dict[str, Any] | None = None,\n):\n    super().__init__(\n        model,\n        likelihood,\n        sigma_noise,\n        prior_precision,\n        prior_mean,\n        temperature,\n        enable_backprop,\n        dict_key_x,\n        dict_key_y,\n        backend,\n        backend_kwargs,\n        asdl_fisher_kwargs,\n    )\n    if not hasattr(self, \"H\"):\n        self._init_H()\n        # posterior mean/mode\n        self.mean: float | torch.Tensor = self.prior_mean\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.log_likelihood","title":"log_likelihood","text":"<pre><code>log_likelihood: Tensor\n</code></pre> <p>Compute log likelihood on the training data after <code>.fit()</code> has been called. The log likelihood is computed on-demand based on the loss and, for example, the observation noise which makes it differentiable in the latter for iterative updates.</p> <p>Returns:</p> <ul> <li> <code>log_likelihood</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.prior_precision_diag","title":"prior_precision_diag","text":"<pre><code>prior_precision_diag: Tensor\n</code></pre> <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either a scalar, layer-wise, or diagonal prior precision.</p> <p>Returns:</p> <ul> <li> <code>prior_precision_diag</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.scatter","title":"scatter","text":"<pre><code>scatter: Tensor\n</code></pre> <p>Computes the scatter, a term of the log marginal likelihood that corresponds to L-2 regularization: <code>scatter</code> = \\((\\theta_{MAP} - \\mu_0)^{T} P_0 (\\theta_{MAP} - \\mu_0) \\).</p> <p>Returns:</p> <ul> <li> <code>scatter</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.log_det_prior_precision","title":"log_det_prior_precision","text":"<pre><code>log_det_prior_precision: Tensor\n</code></pre> <p>Compute log determinant of the prior precision \\(\\log \\det P_0\\)</p> <p>Returns:</p> <ul> <li> <code>log_det</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.log_det_ratio","title":"log_det_ratio","text":"<pre><code>log_det_ratio: Tensor\n</code></pre> <p>Compute the log determinant ratio, a part of the log marginal likelihood.</p> \\[     \\log \\frac{\\det P}{\\det P_0} = \\log \\det P - \\log \\det P_0 \\] <p>Returns:</p> <ul> <li> <code>log_det_ratio</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.posterior_precision","title":"posterior_precision","text":"<pre><code>posterior_precision: Tensor\n</code></pre> <p>Diagonal posterior precision \\(p\\).</p> <p>Returns:</p> <ul> <li> <code>precision</code> (              <code>tensor</code> )          \u2013            <p><code>(parameters)</code></p> </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.posterior_scale","title":"posterior_scale","text":"<pre><code>posterior_scale: Tensor\n</code></pre> <p>Diagonal posterior scale \\(\\sqrt{p^{-1}}\\).</p> <p>Returns:</p> <ul> <li> <code>precision</code> (              <code>tensor</code> )          \u2013            <p><code>(parameters)</code></p> </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.posterior_variance","title":"posterior_variance","text":"<pre><code>posterior_variance: Tensor\n</code></pre> <p>Diagonal posterior variance \\(p^{-1}\\).</p> <p>Returns:</p> <ul> <li> <code>precision</code> (              <code>tensor</code> )          \u2013            <p><code>(parameters)</code></p> </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.fit","title":"fit","text":"<pre><code>fit(train_loader: DataLoader, override: bool = True, progress_bar: bool = False) -&gt; None\n</code></pre> <p>Fit the local Laplace approximation at the parameters of the model.</p> <p>Parameters:</p> Source code in <code>laplace/baselaplace.py</code> <pre><code>def fit(\n    self,\n    train_loader: DataLoader,\n    override: bool = True,\n    progress_bar: bool = False,\n) -&gt; None:\n    \"\"\"Fit the local Laplace approximation at the parameters of the model.\n\n    Parameters\n    ----------\n    train_loader : torch.data.utils.DataLoader\n        each iterate is a training batch, either `(X, y)` tensors or a dict-like\n        object containing keys as expressed by `self.dict_key_x` and\n        `self.dict_key_y`. `train_loader.dataset` needs to be set to access\n        \\\\(N\\\\), size of the data set.\n    override : bool, default=True\n        whether to initialize H, loss, and n_data again; setting to False is useful for\n        online learning settings to accumulate a sequential posterior approximation.\n    progress_bar : bool, default=False\n        whether to show a progress bar; updated at every batch-Hessian computation.\n        Useful for very large model and large amount of data, esp. when `subset_of_weights='all'`.\n    \"\"\"\n    if override:\n        self._init_H()\n        self.loss: float | torch.Tensor = 0\n        self.n_data: int = 0\n\n    self.model.eval()\n\n    self.mean: torch.Tensor = parameters_to_vector(self.params)\n    if not self.enable_backprop:\n        self.mean = self.mean.detach()\n\n    data: (\n        tuple[torch.Tensor, torch.Tensor] | MutableMapping[str, torch.Tensor | Any]\n    ) = next(iter(train_loader))\n\n    with torch.no_grad():\n        if isinstance(data, MutableMapping):  # To support Huggingface dataset\n            if \"backpack\" in self._backend_cls.__name__.lower() or (\n                isinstance(self, DiagLaplace) and self._backend_cls == CurvlinopsEF\n            ):\n                raise ValueError(\n                    \"Currently DiagEF is not supported under CurvlinopsEF backend \"\n                    + \"for custom models with non-tensor inputs \"\n                    + \"(https://github.com/pytorch/functorch/issues/159). Consider \"\n                    + \"using AsdlEF backend instead. The same limitation applies \"\n                    + \"to all BackPACK backend\"\n                )\n\n            out = self.model(data)\n        else:\n            X = data[0]\n            try:\n                out = self.model(X[:1].to(self._device))\n            except (TypeError, AttributeError):\n                out = self.model(X.to(self._device))\n    self.n_outputs = out.shape[-1]\n    setattr(self.model, \"output_size\", self.n_outputs)\n\n    N = len(train_loader.dataset)\n\n    pbar = tqdm.tqdm(train_loader, disable=not progress_bar)\n    pbar.set_description(\"[Computing Hessian]\")\n\n    for data in pbar:\n        if isinstance(data, MutableMapping):  # To support Huggingface dataset\n            X, y = data, data[self.dict_key_y].to(self._device)\n        else:\n            X, y = data\n            X, y = X.to(self._device), y.to(self._device)\n\n        if self.likelihood == Likelihood.REGRESSION and y.ndim != out.ndim:\n            raise ValueError(\n                f\"The model's output has {out.ndim} dims but \"\n                f\"the target has {y.ndim} dims.\"\n            )\n\n        self.model.zero_grad()\n        loss_batch, H_batch = self._curv_closure(X, y, N=N)\n        self.loss += loss_batch\n        self.H += H_batch\n\n    self.n_data += N\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.fit(train_loader)","title":"<code>train_loader</code>","text":"(<code>DataLoader</code>)           \u2013            <p>each iterate is a training batch, either <code>(X, y)</code> tensors or a dict-like object containing keys as expressed by <code>self.dict_key_x</code> and <code>self.dict_key_y</code>. <code>train_loader.dataset</code> needs to be set to access \\(N\\), size of the data set.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.fit(override)","title":"<code>override</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to initialize H, loss, and n_data again; setting to False is useful for online learning settings to accumulate a sequential posterior approximation.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.fit(progress_bar)","title":"<code>progress_bar</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to show a progress bar; updated at every batch-Hessian computation. Useful for very large model and large amount of data, esp. when <code>subset_of_weights='all'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.log_marginal_likelihood","title":"log_marginal_likelihood","text":"<pre><code>log_marginal_likelihood(prior_precision: Tensor | None = None, sigma_noise: Tensor | None = None) -&gt; Tensor\n</code></pre> <p>Compute the Laplace approximation to the log marginal likelihood subject to specific Hessian approximations that subclasses implement. Requires that the Laplace approximation has been fit before. The resulting torch.Tensor is differentiable in <code>prior_precision</code> and <code>sigma_noise</code> if these have gradients enabled. By passing <code>prior_precision</code> or <code>sigma_noise</code>, the current value is overwritten. This is useful for iterating on the log marginal likelihood.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>log_marglik</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def log_marginal_likelihood(\n    self,\n    prior_precision: torch.Tensor | None = None,\n    sigma_noise: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the Laplace approximation to the log marginal likelihood subject\n    to specific Hessian approximations that subclasses implement.\n    Requires that the Laplace approximation has been fit before.\n    The resulting torch.Tensor is differentiable in `prior_precision` and\n    `sigma_noise` if these have gradients enabled.\n    By passing `prior_precision` or `sigma_noise`, the current value is\n    overwritten. This is useful for iterating on the log marginal likelihood.\n\n    Parameters\n    ----------\n    prior_precision : torch.Tensor, optional\n        prior precision if should be changed from current `prior_precision` value\n    sigma_noise : torch.Tensor, optional\n        observation noise standard deviation if should be changed\n\n    Returns\n    -------\n    log_marglik : torch.Tensor\n    \"\"\"\n    # update prior precision (useful when iterating on marglik)\n    if prior_precision is not None:\n        self.prior_precision = prior_precision\n\n    # update sigma_noise (useful when iterating on marglik)\n    if sigma_noise is not None:\n        if self.likelihood != Likelihood.REGRESSION:\n            raise ValueError(\"Can only change sigma_noise for regression.\")\n\n        self.sigma_noise = sigma_noise\n\n    return self.log_likelihood - 0.5 * (self.log_det_ratio + self.scatter)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.log_marginal_likelihood(prior_precision)","title":"<code>prior_precision</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>prior precision if should be changed from current <code>prior_precision</code> value</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.log_marginal_likelihood(sigma_noise)","title":"<code>sigma_noise</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>observation noise standard deviation if should be changed</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.__call__","title":"__call__","text":"<pre><code>__call__(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None, fitting: bool = False, **model_kwargs: dict[str, Any]) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def __call__(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n    fitting: bool = False,\n    **model_kwargs: dict[str, Any],\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x`.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here. When Laplace is done only\n        on subset of parameters (i.e. some grad are disabled),\n        only `nn` predictive is supported.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` when `joint=False` in regression.\n        In the case of last-layer Laplace with a diagonal or Kron Hessian,\n        setting this to `True` makes computation much(!) faster for large\n        number of outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used).\n\n    fitting : bool, default=False\n        whether or not this predictive call is done during fitting. Only useful for\n        reward modeling: the likelihood is set to `\"regression\"` when `False` and\n        `\"classification\"` when `True`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    if pred_type not in [pred for pred in PredType]:\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if link_approx not in [la for la in LinkApprox]:\n        raise ValueError(f\"Unsupported link approximation {link_approx}.\")\n\n    if pred_type == PredType.NN and link_approx != LinkApprox.MC:\n        raise ValueError(\n            \"Only mc link approximation is supported for nn prediction type.\"\n        )\n\n    if generator is not None:\n        if (\n            not isinstance(generator, torch.Generator)\n            or generator.device != self._device\n        ):\n            raise ValueError(\"Invalid random generator (check type and device).\")\n\n    likelihood = self.likelihood\n    if likelihood == Likelihood.REWARD_MODELING:\n        likelihood = Likelihood.CLASSIFICATION if fitting else Likelihood.REGRESSION\n\n    if pred_type == PredType.GLM:\n        return self._glm_forward_call(\n            x, likelihood, joint, link_approx, n_samples, diagonal_output\n        )\n    else:\n        if likelihood == Likelihood.REGRESSION:\n            samples = self._nn_predictive_samples(x, n_samples, **model_kwargs)\n            return samples.mean(dim=0), samples.var(dim=0)\n        else:  # classification; the average is computed online\n            return self._nn_predictive_classification(x, n_samples, **model_kwargs)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.__call__(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.__call__(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here. When Laplace is done only on subset of parameters (i.e. some grad are disabled), only <code>nn</code> predictive is supported.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.__call__(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.__call__(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.__call__(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.__call__(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> when <code>joint=False</code> in regression. In the case of last-layer Laplace with a diagonal or Kron Hessian, setting this to <code>True</code> makes computation much(!) faster for large number of outputs.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.__call__(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used).</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.__call__(fitting)","title":"<code>fitting</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether or not this predictive call is done during fitting. Only useful for reward modeling: the likelihood is set to <code>\"regression\"</code> when <code>False</code> and <code>\"classification\"</code> when <code>True</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace._glm_forward_call","title":"_glm_forward_call","text":"<pre><code>_glm_forward_call(x: Tensor | MutableMapping, likelihood: Likelihood | str, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code> for \"glm\" pred type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_forward_call(\n    self,\n    x: torch.Tensor | MutableMapping,\n    likelihood: Likelihood | str,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x` for \"glm\" pred type.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    likelihood : Likelihood or str in {'classification', 'regression', 'reward_modeling'}\n        determines the log likelihood Hessian approximation.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` and `link_approx='mc'`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    f_mu, f_var = self._glm_predictive_distribution(\n        x, joint=joint and likelihood == Likelihood.REGRESSION\n    )\n\n    if likelihood == Likelihood.REGRESSION:\n        if diagonal_output and not joint:\n            f_var = torch.diagonal(f_var, dim1=-2, dim2=-1)\n        return f_mu, f_var\n\n    if link_approx == LinkApprox.MC:\n        return self._glm_predictive_samples(\n            f_mu,\n            f_var,\n            n_samples=n_samples,\n            diagonal_output=diagonal_output,\n        ).mean(dim=0)\n    elif link_approx == LinkApprox.PROBIT:\n        kappa = 1 / torch.sqrt(1.0 + np.pi / 8 * f_var.diagonal(dim1=1, dim2=2))\n        return torch.softmax(kappa * f_mu, dim=-1)\n    elif \"bridge\" in link_approx:\n        # zero mean correction\n        f_mu -= (\n            f_var.sum(-1)\n            * f_mu.sum(-1).reshape(-1, 1)\n            / f_var.sum(dim=(1, 2)).reshape(-1, 1)\n        )\n        f_var -= torch.einsum(\n            \"bi,bj-&gt;bij\", f_var.sum(-1), f_var.sum(-2)\n        ) / f_var.sum(dim=(1, 2)).reshape(-1, 1, 1)\n\n        # Laplace Bridge\n        _, K = f_mu.size(0), f_mu.size(-1)\n        f_var_diag = torch.diagonal(f_var, dim1=1, dim2=2)\n\n        # optional: variance correction\n        if link_approx == LinkApprox.BRIDGE_NORM:\n            f_var_diag_mean = f_var_diag.mean(dim=1)\n            f_var_diag_mean /= torch.as_tensor(\n                [K / 2], device=self._device, dtype=self._dtype\n            ).sqrt()\n            f_mu /= f_var_diag_mean.sqrt().unsqueeze(-1)\n            f_var_diag /= f_var_diag_mean.unsqueeze(-1)\n\n        sum_exp = torch.exp(-f_mu).sum(dim=1).unsqueeze(-1)\n        alpha = (1 - 2 / K + f_mu.exp() / K**2 * sum_exp) / f_var_diag\n        return torch.nan_to_num(alpha / alpha.sum(dim=1).unsqueeze(-1), nan=1.0)\n    else:\n        raise ValueError(\n            \"Prediction path invalid. Check the likelihood, pred_type, link_approx combination!\"\n        )\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace._glm_forward_call(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace._glm_forward_call(likelihood)","title":"<code>likelihood</code>","text":"(<code>Likelihood or str in {'classification', 'regression', 'reward_modeling'}</code>)           \u2013            <p>determines the log likelihood Hessian approximation.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace._glm_forward_call(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace._glm_forward_call(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace._glm_forward_call(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace._glm_forward_call(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> and <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace._glm_functional_samples","title":"_glm_functional_samples","text":"<pre><code>_glm_functional_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior functional on input data <code>x</code> using \"glm\" prediction type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_functional_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior functional on input data `x` using \"glm\" prediction\n    type.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    assert f_var.shape == torch.Size([f_mu.shape[0], f_mu.shape[1], f_mu.shape[1]])\n\n    if diagonal_output:\n        f_var = torch.diagonal(f_var, dim1=1, dim2=2)\n\n    return normal_samples(f_mu, f_var, n_samples, generator)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace._glm_functional_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace._glm_functional_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace._glm_functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace._glm_functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace._glm_functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace._glm_predictive_samples","title":"_glm_predictive_samples","text":"<pre><code>_glm_predictive_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code> using \"glm\" prediction type. I.e., the inverse-link function correponding to the likelihood is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_predictive_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x` using \"glm\" prediction\n    type. I.e., the inverse-link function correponding to the likelihood is applied\n    on top of the functional sample.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    f_samples = self._glm_functional_samples(\n        f_mu, f_var, n_samples, diagonal_output, generator\n    )\n\n    if self.likelihood == Likelihood.REGRESSION:\n        return f_samples\n    else:\n        return torch.softmax(f_samples, dim=-1)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace._glm_predictive_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace._glm_predictive_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace._glm_predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace._glm_predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace._glm_predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.log_prob","title":"log_prob","text":"<pre><code>log_prob(value: Tensor, normalized: bool = True) -&gt; Tensor\n</code></pre> <p>Compute the log probability under the (current) Laplace approximation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>log_prob</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def log_prob(self, value: torch.Tensor, normalized: bool = True) -&gt; torch.Tensor:\n    \"\"\"Compute the log probability under the (current) Laplace approximation.\n\n    Parameters\n    ----------\n    value: torch.Tensor\n    normalized : bool, default=True\n        whether to return log of a properly normalized Gaussian or just the\n        terms that depend on `value`.\n\n    Returns\n    -------\n    log_prob : torch.Tensor\n    \"\"\"\n    if not normalized:\n        return -self.square_norm(value) / 2\n    log_prob = (\n        -self.n_params / 2 * log(2 * pi) + self.log_det_posterior_precision / 2\n    )\n    log_prob -= self.square_norm(value) / 2\n    return log_prob\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.log_prob(value)","title":"<code>value</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.log_prob(normalized)","title":"<code>normalized</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to return log of a properly normalized Gaussian or just the terms that depend on <code>value</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.functional_samples","title":"functional_samples","text":"<pre><code>functional_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the function-space posterior on input data <code>x</code>. Can be used, for example, for Thompson sampling or to compute an arbitrary expectation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def functional_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the function-space posterior on input data `x`.\n    Can be used, for example, for Thompson sampling or to compute an arbitrary\n    expectation.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if pred_type == PredType.GLM:\n        f_mu, f_var = self._glm_predictive_distribution(x)\n        return self._glm_functional_samples(\n            f_mu, f_var, n_samples, diagonal_output, generator\n        )\n    else:  # 'nn'\n        return self._nn_functional_samples(x, n_samples, generator)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.functional_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.functional_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.predictive_samples","title":"predictive_samples","text":"<pre><code>predictive_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code>. I.e., the respective inverse-link function (e.g. softmax) is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def predictive_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x`. I.e., the respective\n    inverse-link function (e.g. softmax) is applied on top of the functional\n    sample.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if pred_type == PredType.GLM:\n        f_mu, f_var = self._glm_predictive_distribution(x)\n        return self._glm_predictive_samples(\n            f_mu, f_var, n_samples, diagonal_output, generator\n        )\n    else:  # 'nn'\n        return self._nn_predictive_samples(x, n_samples, generator)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.predictive_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.predictive_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.DiagLaplace.predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace","title":"KronLaplace","text":"<pre><code>KronLaplace(model: Module, likelihood: Likelihood | str, sigma_noise: float | Tensor = 1.0, prior_precision: float | Tensor = 1.0, prior_mean: float | Tensor = 0.0, temperature: float = 1.0, enable_backprop: bool = False, dict_key_x: str = 'input_ids', dict_key_y: str = 'labels', backend: type[CurvatureInterface] | None = None, damping: bool = False, backend_kwargs: dict[str, Any] | None = None, asdl_fisher_kwargs: dict[str, Any] | None = None)\n</code></pre> <p>               Bases: <code>ParametricLaplace</code></p> <p>Laplace approximation with Kronecker factored log likelihood Hessian approximation and hence posterior precision. Mathematically, we have for each parameter group, e.g., torch.nn.Module, that \\P\\approx Q \\otimes H. See <code>BaseLaplace</code> for the full interface and see <code>laplace.utils.matrix.Kron</code> and <code>laplace.utils.matrix.KronDecomposed</code> for the structure of the Kronecker factors. <code>Kron</code> is used to aggregate factors by summing up and <code>KronDecomposed</code> is used to add the prior, a Hessian factor (e.g. temperature), and computing posterior covariances, marginal likelihood, etc. Damping can be enabled by setting <code>damping=True</code>.</p> <p>Methods:</p> <ul> <li> <code>log_marginal_likelihood</code>             \u2013              <p>Compute the Laplace approximation to the log marginal likelihood subject</p> </li> <li> <code>__call__</code>             \u2013              <p>Compute the posterior predictive on input data <code>x</code>.</p> </li> <li> <code>log_prob</code>             \u2013              <p>Compute the log probability under the (current) Laplace approximation.</p> </li> <li> <code>functional_samples</code>             \u2013              <p>Sample from the function-space posterior on input data <code>x</code>.</p> </li> <li> <code>predictive_samples</code>             \u2013              <p>Sample from the posterior predictive on input data <code>x</code>. I.e., the respective</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>log_likelihood</code>               (<code>Tensor</code>)           \u2013            <p>Compute log likelihood on the training data after <code>.fit()</code> has been called.</p> </li> <li> <code>prior_precision_diag</code>               (<code>Tensor</code>)           \u2013            <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either</p> </li> <li> <code>scatter</code>               (<code>Tensor</code>)           \u2013            <p>Computes the scatter, a term of the log marginal likelihood that</p> </li> <li> <code>log_det_prior_precision</code>               (<code>Tensor</code>)           \u2013            <p>Compute log determinant of the prior precision</p> </li> <li> <code>log_det_ratio</code>               (<code>Tensor</code>)           \u2013            <p>Compute the log determinant ratio, a part of the log marginal likelihood.</p> </li> <li> <code>posterior_precision</code>               (<code>KronDecomposed</code>)           \u2013            <p>Kronecker factored Posterior precision \\(P\\).</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    sigma_noise: float | torch.Tensor = 1.0,\n    prior_precision: float | torch.Tensor = 1.0,\n    prior_mean: float | torch.Tensor = 0.0,\n    temperature: float = 1.0,\n    enable_backprop: bool = False,\n    dict_key_x: str = \"input_ids\",\n    dict_key_y: str = \"labels\",\n    backend: type[CurvatureInterface] | None = None,\n    damping: bool = False,\n    backend_kwargs: dict[str, Any] | None = None,\n    asdl_fisher_kwargs: dict[str, Any] | None = None,\n):\n    self.damping: bool = damping\n    self.H_facs: Kron | None = None\n    super().__init__(\n        model,\n        likelihood,\n        sigma_noise,\n        prior_precision,\n        prior_mean,\n        temperature,\n        enable_backprop,\n        dict_key_x,\n        dict_key_y,\n        backend,\n        backend_kwargs,\n        asdl_fisher_kwargs,\n    )\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.log_likelihood","title":"log_likelihood","text":"<pre><code>log_likelihood: Tensor\n</code></pre> <p>Compute log likelihood on the training data after <code>.fit()</code> has been called. The log likelihood is computed on-demand based on the loss and, for example, the observation noise which makes it differentiable in the latter for iterative updates.</p> <p>Returns:</p> <ul> <li> <code>log_likelihood</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.prior_precision_diag","title":"prior_precision_diag","text":"<pre><code>prior_precision_diag: Tensor\n</code></pre> <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either a scalar, layer-wise, or diagonal prior precision.</p> <p>Returns:</p> <ul> <li> <code>prior_precision_diag</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.scatter","title":"scatter","text":"<pre><code>scatter: Tensor\n</code></pre> <p>Computes the scatter, a term of the log marginal likelihood that corresponds to L-2 regularization: <code>scatter</code> = \\((\\theta_{MAP} - \\mu_0)^{T} P_0 (\\theta_{MAP} - \\mu_0) \\).</p> <p>Returns:</p> <ul> <li> <code>scatter</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.log_det_prior_precision","title":"log_det_prior_precision","text":"<pre><code>log_det_prior_precision: Tensor\n</code></pre> <p>Compute log determinant of the prior precision \\(\\log \\det P_0\\)</p> <p>Returns:</p> <ul> <li> <code>log_det</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.log_det_ratio","title":"log_det_ratio","text":"<pre><code>log_det_ratio: Tensor\n</code></pre> <p>Compute the log determinant ratio, a part of the log marginal likelihood.</p> \\[     \\log \\frac{\\det P}{\\det P_0} = \\log \\det P - \\log \\det P_0 \\] <p>Returns:</p> <ul> <li> <code>log_det_ratio</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.posterior_precision","title":"posterior_precision","text":"<pre><code>posterior_precision: KronDecomposed\n</code></pre> <p>Kronecker factored Posterior precision \\(P\\).</p> <p>Returns:</p> <ul> <li> <code>precision</code> (              <code>`laplace.utils.matrix.KronDecomposed`</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.log_marginal_likelihood","title":"log_marginal_likelihood","text":"<pre><code>log_marginal_likelihood(prior_precision: Tensor | None = None, sigma_noise: Tensor | None = None) -&gt; Tensor\n</code></pre> <p>Compute the Laplace approximation to the log marginal likelihood subject to specific Hessian approximations that subclasses implement. Requires that the Laplace approximation has been fit before. The resulting torch.Tensor is differentiable in <code>prior_precision</code> and <code>sigma_noise</code> if these have gradients enabled. By passing <code>prior_precision</code> or <code>sigma_noise</code>, the current value is overwritten. This is useful for iterating on the log marginal likelihood.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>log_marglik</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def log_marginal_likelihood(\n    self,\n    prior_precision: torch.Tensor | None = None,\n    sigma_noise: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the Laplace approximation to the log marginal likelihood subject\n    to specific Hessian approximations that subclasses implement.\n    Requires that the Laplace approximation has been fit before.\n    The resulting torch.Tensor is differentiable in `prior_precision` and\n    `sigma_noise` if these have gradients enabled.\n    By passing `prior_precision` or `sigma_noise`, the current value is\n    overwritten. This is useful for iterating on the log marginal likelihood.\n\n    Parameters\n    ----------\n    prior_precision : torch.Tensor, optional\n        prior precision if should be changed from current `prior_precision` value\n    sigma_noise : torch.Tensor, optional\n        observation noise standard deviation if should be changed\n\n    Returns\n    -------\n    log_marglik : torch.Tensor\n    \"\"\"\n    # update prior precision (useful when iterating on marglik)\n    if prior_precision is not None:\n        self.prior_precision = prior_precision\n\n    # update sigma_noise (useful when iterating on marglik)\n    if sigma_noise is not None:\n        if self.likelihood != Likelihood.REGRESSION:\n            raise ValueError(\"Can only change sigma_noise for regression.\")\n\n        self.sigma_noise = sigma_noise\n\n    return self.log_likelihood - 0.5 * (self.log_det_ratio + self.scatter)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.log_marginal_likelihood(prior_precision)","title":"<code>prior_precision</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>prior precision if should be changed from current <code>prior_precision</code> value</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.log_marginal_likelihood(sigma_noise)","title":"<code>sigma_noise</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>observation noise standard deviation if should be changed</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.__call__","title":"__call__","text":"<pre><code>__call__(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None, fitting: bool = False, **model_kwargs: dict[str, Any]) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def __call__(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n    fitting: bool = False,\n    **model_kwargs: dict[str, Any],\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x`.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here. When Laplace is done only\n        on subset of parameters (i.e. some grad are disabled),\n        only `nn` predictive is supported.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` when `joint=False` in regression.\n        In the case of last-layer Laplace with a diagonal or Kron Hessian,\n        setting this to `True` makes computation much(!) faster for large\n        number of outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used).\n\n    fitting : bool, default=False\n        whether or not this predictive call is done during fitting. Only useful for\n        reward modeling: the likelihood is set to `\"regression\"` when `False` and\n        `\"classification\"` when `True`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    if pred_type not in [pred for pred in PredType]:\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if link_approx not in [la for la in LinkApprox]:\n        raise ValueError(f\"Unsupported link approximation {link_approx}.\")\n\n    if pred_type == PredType.NN and link_approx != LinkApprox.MC:\n        raise ValueError(\n            \"Only mc link approximation is supported for nn prediction type.\"\n        )\n\n    if generator is not None:\n        if (\n            not isinstance(generator, torch.Generator)\n            or generator.device != self._device\n        ):\n            raise ValueError(\"Invalid random generator (check type and device).\")\n\n    likelihood = self.likelihood\n    if likelihood == Likelihood.REWARD_MODELING:\n        likelihood = Likelihood.CLASSIFICATION if fitting else Likelihood.REGRESSION\n\n    if pred_type == PredType.GLM:\n        return self._glm_forward_call(\n            x, likelihood, joint, link_approx, n_samples, diagonal_output\n        )\n    else:\n        if likelihood == Likelihood.REGRESSION:\n            samples = self._nn_predictive_samples(x, n_samples, **model_kwargs)\n            return samples.mean(dim=0), samples.var(dim=0)\n        else:  # classification; the average is computed online\n            return self._nn_predictive_classification(x, n_samples, **model_kwargs)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.__call__(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.__call__(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here. When Laplace is done only on subset of parameters (i.e. some grad are disabled), only <code>nn</code> predictive is supported.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.__call__(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.__call__(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.__call__(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.__call__(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> when <code>joint=False</code> in regression. In the case of last-layer Laplace with a diagonal or Kron Hessian, setting this to <code>True</code> makes computation much(!) faster for large number of outputs.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.__call__(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used).</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.__call__(fitting)","title":"<code>fitting</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether or not this predictive call is done during fitting. Only useful for reward modeling: the likelihood is set to <code>\"regression\"</code> when <code>False</code> and <code>\"classification\"</code> when <code>True</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace._glm_forward_call","title":"_glm_forward_call","text":"<pre><code>_glm_forward_call(x: Tensor | MutableMapping, likelihood: Likelihood | str, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code> for \"glm\" pred type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_forward_call(\n    self,\n    x: torch.Tensor | MutableMapping,\n    likelihood: Likelihood | str,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x` for \"glm\" pred type.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    likelihood : Likelihood or str in {'classification', 'regression', 'reward_modeling'}\n        determines the log likelihood Hessian approximation.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` and `link_approx='mc'`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    f_mu, f_var = self._glm_predictive_distribution(\n        x, joint=joint and likelihood == Likelihood.REGRESSION\n    )\n\n    if likelihood == Likelihood.REGRESSION:\n        if diagonal_output and not joint:\n            f_var = torch.diagonal(f_var, dim1=-2, dim2=-1)\n        return f_mu, f_var\n\n    if link_approx == LinkApprox.MC:\n        return self._glm_predictive_samples(\n            f_mu,\n            f_var,\n            n_samples=n_samples,\n            diagonal_output=diagonal_output,\n        ).mean(dim=0)\n    elif link_approx == LinkApprox.PROBIT:\n        kappa = 1 / torch.sqrt(1.0 + np.pi / 8 * f_var.diagonal(dim1=1, dim2=2))\n        return torch.softmax(kappa * f_mu, dim=-1)\n    elif \"bridge\" in link_approx:\n        # zero mean correction\n        f_mu -= (\n            f_var.sum(-1)\n            * f_mu.sum(-1).reshape(-1, 1)\n            / f_var.sum(dim=(1, 2)).reshape(-1, 1)\n        )\n        f_var -= torch.einsum(\n            \"bi,bj-&gt;bij\", f_var.sum(-1), f_var.sum(-2)\n        ) / f_var.sum(dim=(1, 2)).reshape(-1, 1, 1)\n\n        # Laplace Bridge\n        _, K = f_mu.size(0), f_mu.size(-1)\n        f_var_diag = torch.diagonal(f_var, dim1=1, dim2=2)\n\n        # optional: variance correction\n        if link_approx == LinkApprox.BRIDGE_NORM:\n            f_var_diag_mean = f_var_diag.mean(dim=1)\n            f_var_diag_mean /= torch.as_tensor(\n                [K / 2], device=self._device, dtype=self._dtype\n            ).sqrt()\n            f_mu /= f_var_diag_mean.sqrt().unsqueeze(-1)\n            f_var_diag /= f_var_diag_mean.unsqueeze(-1)\n\n        sum_exp = torch.exp(-f_mu).sum(dim=1).unsqueeze(-1)\n        alpha = (1 - 2 / K + f_mu.exp() / K**2 * sum_exp) / f_var_diag\n        return torch.nan_to_num(alpha / alpha.sum(dim=1).unsqueeze(-1), nan=1.0)\n    else:\n        raise ValueError(\n            \"Prediction path invalid. Check the likelihood, pred_type, link_approx combination!\"\n        )\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace._glm_forward_call(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace._glm_forward_call(likelihood)","title":"<code>likelihood</code>","text":"(<code>Likelihood or str in {'classification', 'regression', 'reward_modeling'}</code>)           \u2013            <p>determines the log likelihood Hessian approximation.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace._glm_forward_call(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace._glm_forward_call(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace._glm_forward_call(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace._glm_forward_call(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> and <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace._glm_functional_samples","title":"_glm_functional_samples","text":"<pre><code>_glm_functional_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior functional on input data <code>x</code> using \"glm\" prediction type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_functional_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior functional on input data `x` using \"glm\" prediction\n    type.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    assert f_var.shape == torch.Size([f_mu.shape[0], f_mu.shape[1], f_mu.shape[1]])\n\n    if diagonal_output:\n        f_var = torch.diagonal(f_var, dim1=1, dim2=2)\n\n    return normal_samples(f_mu, f_var, n_samples, generator)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace._glm_functional_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace._glm_functional_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace._glm_functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace._glm_functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace._glm_functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace._glm_predictive_samples","title":"_glm_predictive_samples","text":"<pre><code>_glm_predictive_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code> using \"glm\" prediction type. I.e., the inverse-link function correponding to the likelihood is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_predictive_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x` using \"glm\" prediction\n    type. I.e., the inverse-link function correponding to the likelihood is applied\n    on top of the functional sample.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    f_samples = self._glm_functional_samples(\n        f_mu, f_var, n_samples, diagonal_output, generator\n    )\n\n    if self.likelihood == Likelihood.REGRESSION:\n        return f_samples\n    else:\n        return torch.softmax(f_samples, dim=-1)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace._glm_predictive_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace._glm_predictive_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace._glm_predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace._glm_predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace._glm_predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.log_prob","title":"log_prob","text":"<pre><code>log_prob(value: Tensor, normalized: bool = True) -&gt; Tensor\n</code></pre> <p>Compute the log probability under the (current) Laplace approximation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>log_prob</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def log_prob(self, value: torch.Tensor, normalized: bool = True) -&gt; torch.Tensor:\n    \"\"\"Compute the log probability under the (current) Laplace approximation.\n\n    Parameters\n    ----------\n    value: torch.Tensor\n    normalized : bool, default=True\n        whether to return log of a properly normalized Gaussian or just the\n        terms that depend on `value`.\n\n    Returns\n    -------\n    log_prob : torch.Tensor\n    \"\"\"\n    if not normalized:\n        return -self.square_norm(value) / 2\n    log_prob = (\n        -self.n_params / 2 * log(2 * pi) + self.log_det_posterior_precision / 2\n    )\n    log_prob -= self.square_norm(value) / 2\n    return log_prob\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.log_prob(value)","title":"<code>value</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.log_prob(normalized)","title":"<code>normalized</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to return log of a properly normalized Gaussian or just the terms that depend on <code>value</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.functional_samples","title":"functional_samples","text":"<pre><code>functional_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the function-space posterior on input data <code>x</code>. Can be used, for example, for Thompson sampling or to compute an arbitrary expectation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def functional_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the function-space posterior on input data `x`.\n    Can be used, for example, for Thompson sampling or to compute an arbitrary\n    expectation.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if pred_type == PredType.GLM:\n        f_mu, f_var = self._glm_predictive_distribution(x)\n        return self._glm_functional_samples(\n            f_mu, f_var, n_samples, diagonal_output, generator\n        )\n    else:  # 'nn'\n        return self._nn_functional_samples(x, n_samples, generator)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.functional_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.functional_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.predictive_samples","title":"predictive_samples","text":"<pre><code>predictive_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code>. I.e., the respective inverse-link function (e.g. softmax) is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def predictive_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x`. I.e., the respective\n    inverse-link function (e.g. softmax) is applied on top of the functional\n    sample.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if pred_type == PredType.GLM:\n        f_mu, f_var = self._glm_predictive_distribution(x)\n        return self._glm_predictive_samples(\n            f_mu, f_var, n_samples, diagonal_output, generator\n        )\n    else:  # 'nn'\n        return self._nn_predictive_samples(x, n_samples, generator)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.predictive_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.predictive_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.KronLaplace.predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace","title":"LowRankLaplace","text":"<pre><code>LowRankLaplace(model: Module, likelihood: Likelihood | str, backend: type[CurvatureInterface] = AsdfghjklHessian if find_spec('asdfghjkl') is not None else CurvatureInterface, sigma_noise: float | Tensor = 1, prior_precision: float | Tensor = 1, prior_mean: float | Tensor = 0, temperature: float = 1, enable_backprop: bool = False, dict_key_x: str = 'input_ids', dict_key_y: str = 'labels', backend_kwargs: dict[str, Any] | None = None)\n</code></pre> <p>               Bases: <code>ParametricLaplace</code></p> <p>Laplace approximation with low-rank log likelihood Hessian (approximation). The low-rank matrix is represented by an eigendecomposition (vecs, values). Based on the chosen <code>backend</code>, either a true Hessian or, for example, GGN approximation could be used. The posterior precision is computed as \\( P = V diag(l) V^T + P_0.\\) To sample, compute the functional variance, and log determinant, algebraic tricks are usedto reduce the costs of inversion to the that of a \\(K       imes K\\) matrix if we have a rank of K.</p> <p>Note that only <code>AsdfghjklHessian</code> backend is supported. Install it via: pip install git+https://git@github.com/wiseodd/asdl@asdfghjkl</p> <p>See <code>BaseLaplace</code> for the full interface.</p> <p>Methods:</p> <ul> <li> <code>log_marginal_likelihood</code>             \u2013              <p>Compute the Laplace approximation to the log marginal likelihood subject</p> </li> <li> <code>__call__</code>             \u2013              <p>Compute the posterior predictive on input data <code>x</code>.</p> </li> <li> <code>square_norm</code>             \u2013              <p>Compute the square norm under post. Precision with <code>value-self.mean</code> as \ud835\udee5:</p> </li> <li> <code>log_prob</code>             \u2013              <p>Compute the log probability under the (current) Laplace approximation.</p> </li> <li> <code>functional_samples</code>             \u2013              <p>Sample from the function-space posterior on input data <code>x</code>.</p> </li> <li> <code>predictive_samples</code>             \u2013              <p>Sample from the posterior predictive on input data <code>x</code>. I.e., the respective</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>log_likelihood</code>               (<code>Tensor</code>)           \u2013            <p>Compute log likelihood on the training data after <code>.fit()</code> has been called.</p> </li> <li> <code>prior_precision_diag</code>               (<code>Tensor</code>)           \u2013            <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either</p> </li> <li> <code>scatter</code>               (<code>Tensor</code>)           \u2013            <p>Computes the scatter, a term of the log marginal likelihood that</p> </li> <li> <code>log_det_prior_precision</code>               (<code>Tensor</code>)           \u2013            <p>Compute log determinant of the prior precision</p> </li> <li> <code>log_det_ratio</code>               (<code>Tensor</code>)           \u2013            <p>Compute the log determinant ratio, a part of the log marginal likelihood.</p> </li> <li> <code>posterior_precision</code>               (<code>tuple[tuple[Tensor, Tensor], Tensor]</code>)           \u2013            <p>Return correctly scaled posterior precision that would be constructed</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    backend: type[CurvatureInterface] = AsdfghjklHessian\n    if find_spec(\"asdfghjkl\") is not None\n    else CurvatureInterface,\n    sigma_noise: float | torch.Tensor = 1,\n    prior_precision: float | torch.Tensor = 1,\n    prior_mean: float | torch.Tensor = 0,\n    temperature: float = 1,\n    enable_backprop: bool = False,\n    dict_key_x: str = \"input_ids\",\n    dict_key_y: str = \"labels\",\n    backend_kwargs: dict[str, Any] | None = None,\n):\n    if find_spec(\"asdfghjkl\") is None:\n        raise ImportError(\n            \"\"\"To use LowRankLaplace, please install the old asdfghjkl dependency: \"\"\"\n            \"\"\"pip install git+https://git@github.com/wiseodd/asdl@asdfghjkl\"\"\"\n        )\n\n    super().__init__(\n        model,\n        likelihood,\n        sigma_noise=sigma_noise,\n        prior_precision=prior_precision,\n        prior_mean=prior_mean,\n        temperature=temperature,\n        enable_backprop=enable_backprop,\n        dict_key_x=dict_key_x,\n        dict_key_y=dict_key_y,\n        backend=backend,\n        backend_kwargs=backend_kwargs,\n    )\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.log_likelihood","title":"log_likelihood","text":"<pre><code>log_likelihood: Tensor\n</code></pre> <p>Compute log likelihood on the training data after <code>.fit()</code> has been called. The log likelihood is computed on-demand based on the loss and, for example, the observation noise which makes it differentiable in the latter for iterative updates.</p> <p>Returns:</p> <ul> <li> <code>log_likelihood</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.prior_precision_diag","title":"prior_precision_diag","text":"<pre><code>prior_precision_diag: Tensor\n</code></pre> <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either a scalar, layer-wise, or diagonal prior precision.</p> <p>Returns:</p> <ul> <li> <code>prior_precision_diag</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.scatter","title":"scatter","text":"<pre><code>scatter: Tensor\n</code></pre> <p>Computes the scatter, a term of the log marginal likelihood that corresponds to L-2 regularization: <code>scatter</code> = \\((\\theta_{MAP} - \\mu_0)^{T} P_0 (\\theta_{MAP} - \\mu_0) \\).</p> <p>Returns:</p> <ul> <li> <code>scatter</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.log_det_prior_precision","title":"log_det_prior_precision","text":"<pre><code>log_det_prior_precision: Tensor\n</code></pre> <p>Compute log determinant of the prior precision \\(\\log \\det P_0\\)</p> <p>Returns:</p> <ul> <li> <code>log_det</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.log_det_ratio","title":"log_det_ratio","text":"<pre><code>log_det_ratio: Tensor\n</code></pre> <p>Compute the log determinant ratio, a part of the log marginal likelihood.</p> \\[     \\log \\frac{\\det P}{\\det P_0} = \\log \\det P - \\log \\det P_0 \\] <p>Returns:</p> <ul> <li> <code>log_det_ratio</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.posterior_precision","title":"posterior_precision","text":"<pre><code>posterior_precision: tuple[tuple[Tensor, Tensor], Tensor]\n</code></pre> <p>Return correctly scaled posterior precision that would be constructed as H[0] @ diag(H[1]) @ H[0].T + self.prior_precision_diag.</p> <p>Returns:</p> <ul> <li> <code>H</code> (              <code>tuple(eigenvectors, eigenvalues)</code> )          \u2013            <p>scaled self.H with temperature and loss factors.</p> </li> <li> <code>prior_precision_diag</code> (              <code>Tensor</code> )          \u2013            <p>diagonal prior precision shape <code>parameters</code> to be added to H.</p> </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.log_marginal_likelihood","title":"log_marginal_likelihood","text":"<pre><code>log_marginal_likelihood(prior_precision: Tensor | None = None, sigma_noise: Tensor | None = None) -&gt; Tensor\n</code></pre> <p>Compute the Laplace approximation to the log marginal likelihood subject to specific Hessian approximations that subclasses implement. Requires that the Laplace approximation has been fit before. The resulting torch.Tensor is differentiable in <code>prior_precision</code> and <code>sigma_noise</code> if these have gradients enabled. By passing <code>prior_precision</code> or <code>sigma_noise</code>, the current value is overwritten. This is useful for iterating on the log marginal likelihood.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>log_marglik</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def log_marginal_likelihood(\n    self,\n    prior_precision: torch.Tensor | None = None,\n    sigma_noise: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the Laplace approximation to the log marginal likelihood subject\n    to specific Hessian approximations that subclasses implement.\n    Requires that the Laplace approximation has been fit before.\n    The resulting torch.Tensor is differentiable in `prior_precision` and\n    `sigma_noise` if these have gradients enabled.\n    By passing `prior_precision` or `sigma_noise`, the current value is\n    overwritten. This is useful for iterating on the log marginal likelihood.\n\n    Parameters\n    ----------\n    prior_precision : torch.Tensor, optional\n        prior precision if should be changed from current `prior_precision` value\n    sigma_noise : torch.Tensor, optional\n        observation noise standard deviation if should be changed\n\n    Returns\n    -------\n    log_marglik : torch.Tensor\n    \"\"\"\n    # update prior precision (useful when iterating on marglik)\n    if prior_precision is not None:\n        self.prior_precision = prior_precision\n\n    # update sigma_noise (useful when iterating on marglik)\n    if sigma_noise is not None:\n        if self.likelihood != Likelihood.REGRESSION:\n            raise ValueError(\"Can only change sigma_noise for regression.\")\n\n        self.sigma_noise = sigma_noise\n\n    return self.log_likelihood - 0.5 * (self.log_det_ratio + self.scatter)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.log_marginal_likelihood(prior_precision)","title":"<code>prior_precision</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>prior precision if should be changed from current <code>prior_precision</code> value</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.log_marginal_likelihood(sigma_noise)","title":"<code>sigma_noise</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>observation noise standard deviation if should be changed</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.__call__","title":"__call__","text":"<pre><code>__call__(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None, fitting: bool = False, **model_kwargs: dict[str, Any]) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def __call__(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n    fitting: bool = False,\n    **model_kwargs: dict[str, Any],\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x`.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here. When Laplace is done only\n        on subset of parameters (i.e. some grad are disabled),\n        only `nn` predictive is supported.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` when `joint=False` in regression.\n        In the case of last-layer Laplace with a diagonal or Kron Hessian,\n        setting this to `True` makes computation much(!) faster for large\n        number of outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used).\n\n    fitting : bool, default=False\n        whether or not this predictive call is done during fitting. Only useful for\n        reward modeling: the likelihood is set to `\"regression\"` when `False` and\n        `\"classification\"` when `True`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    if pred_type not in [pred for pred in PredType]:\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if link_approx not in [la for la in LinkApprox]:\n        raise ValueError(f\"Unsupported link approximation {link_approx}.\")\n\n    if pred_type == PredType.NN and link_approx != LinkApprox.MC:\n        raise ValueError(\n            \"Only mc link approximation is supported for nn prediction type.\"\n        )\n\n    if generator is not None:\n        if (\n            not isinstance(generator, torch.Generator)\n            or generator.device != self._device\n        ):\n            raise ValueError(\"Invalid random generator (check type and device).\")\n\n    likelihood = self.likelihood\n    if likelihood == Likelihood.REWARD_MODELING:\n        likelihood = Likelihood.CLASSIFICATION if fitting else Likelihood.REGRESSION\n\n    if pred_type == PredType.GLM:\n        return self._glm_forward_call(\n            x, likelihood, joint, link_approx, n_samples, diagonal_output\n        )\n    else:\n        if likelihood == Likelihood.REGRESSION:\n            samples = self._nn_predictive_samples(x, n_samples, **model_kwargs)\n            return samples.mean(dim=0), samples.var(dim=0)\n        else:  # classification; the average is computed online\n            return self._nn_predictive_classification(x, n_samples, **model_kwargs)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.__call__(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.__call__(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here. When Laplace is done only on subset of parameters (i.e. some grad are disabled), only <code>nn</code> predictive is supported.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.__call__(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.__call__(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.__call__(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.__call__(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> when <code>joint=False</code> in regression. In the case of last-layer Laplace with a diagonal or Kron Hessian, setting this to <code>True</code> makes computation much(!) faster for large number of outputs.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.__call__(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used).</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.__call__(fitting)","title":"<code>fitting</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether or not this predictive call is done during fitting. Only useful for reward modeling: the likelihood is set to <code>\"regression\"</code> when <code>False</code> and <code>\"classification\"</code> when <code>True</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace._glm_forward_call","title":"_glm_forward_call","text":"<pre><code>_glm_forward_call(x: Tensor | MutableMapping, likelihood: Likelihood | str, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code> for \"glm\" pred type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_forward_call(\n    self,\n    x: torch.Tensor | MutableMapping,\n    likelihood: Likelihood | str,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x` for \"glm\" pred type.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    likelihood : Likelihood or str in {'classification', 'regression', 'reward_modeling'}\n        determines the log likelihood Hessian approximation.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` and `link_approx='mc'`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    f_mu, f_var = self._glm_predictive_distribution(\n        x, joint=joint and likelihood == Likelihood.REGRESSION\n    )\n\n    if likelihood == Likelihood.REGRESSION:\n        if diagonal_output and not joint:\n            f_var = torch.diagonal(f_var, dim1=-2, dim2=-1)\n        return f_mu, f_var\n\n    if link_approx == LinkApprox.MC:\n        return self._glm_predictive_samples(\n            f_mu,\n            f_var,\n            n_samples=n_samples,\n            diagonal_output=diagonal_output,\n        ).mean(dim=0)\n    elif link_approx == LinkApprox.PROBIT:\n        kappa = 1 / torch.sqrt(1.0 + np.pi / 8 * f_var.diagonal(dim1=1, dim2=2))\n        return torch.softmax(kappa * f_mu, dim=-1)\n    elif \"bridge\" in link_approx:\n        # zero mean correction\n        f_mu -= (\n            f_var.sum(-1)\n            * f_mu.sum(-1).reshape(-1, 1)\n            / f_var.sum(dim=(1, 2)).reshape(-1, 1)\n        )\n        f_var -= torch.einsum(\n            \"bi,bj-&gt;bij\", f_var.sum(-1), f_var.sum(-2)\n        ) / f_var.sum(dim=(1, 2)).reshape(-1, 1, 1)\n\n        # Laplace Bridge\n        _, K = f_mu.size(0), f_mu.size(-1)\n        f_var_diag = torch.diagonal(f_var, dim1=1, dim2=2)\n\n        # optional: variance correction\n        if link_approx == LinkApprox.BRIDGE_NORM:\n            f_var_diag_mean = f_var_diag.mean(dim=1)\n            f_var_diag_mean /= torch.as_tensor(\n                [K / 2], device=self._device, dtype=self._dtype\n            ).sqrt()\n            f_mu /= f_var_diag_mean.sqrt().unsqueeze(-1)\n            f_var_diag /= f_var_diag_mean.unsqueeze(-1)\n\n        sum_exp = torch.exp(-f_mu).sum(dim=1).unsqueeze(-1)\n        alpha = (1 - 2 / K + f_mu.exp() / K**2 * sum_exp) / f_var_diag\n        return torch.nan_to_num(alpha / alpha.sum(dim=1).unsqueeze(-1), nan=1.0)\n    else:\n        raise ValueError(\n            \"Prediction path invalid. Check the likelihood, pred_type, link_approx combination!\"\n        )\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace._glm_forward_call(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace._glm_forward_call(likelihood)","title":"<code>likelihood</code>","text":"(<code>Likelihood or str in {'classification', 'regression', 'reward_modeling'}</code>)           \u2013            <p>determines the log likelihood Hessian approximation.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace._glm_forward_call(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace._glm_forward_call(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace._glm_forward_call(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace._glm_forward_call(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> and <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace._glm_functional_samples","title":"_glm_functional_samples","text":"<pre><code>_glm_functional_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior functional on input data <code>x</code> using \"glm\" prediction type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_functional_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior functional on input data `x` using \"glm\" prediction\n    type.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    assert f_var.shape == torch.Size([f_mu.shape[0], f_mu.shape[1], f_mu.shape[1]])\n\n    if diagonal_output:\n        f_var = torch.diagonal(f_var, dim1=1, dim2=2)\n\n    return normal_samples(f_mu, f_var, n_samples, generator)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace._glm_functional_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace._glm_functional_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace._glm_functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace._glm_functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace._glm_functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace._glm_predictive_samples","title":"_glm_predictive_samples","text":"<pre><code>_glm_predictive_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code> using \"glm\" prediction type. I.e., the inverse-link function correponding to the likelihood is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_predictive_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x` using \"glm\" prediction\n    type. I.e., the inverse-link function correponding to the likelihood is applied\n    on top of the functional sample.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    f_samples = self._glm_functional_samples(\n        f_mu, f_var, n_samples, diagonal_output, generator\n    )\n\n    if self.likelihood == Likelihood.REGRESSION:\n        return f_samples\n    else:\n        return torch.softmax(f_samples, dim=-1)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace._glm_predictive_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace._glm_predictive_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace._glm_predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace._glm_predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace._glm_predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.square_norm","title":"square_norm","text":"<pre><code>square_norm(value) -&gt; Tensor\n</code></pre> <p>Compute the square norm under post. Precision with <code>value-self.mean</code> as \ud835\udee5:</p> \\[     \\Delta^     op P \\Delta \\] <p>Returns:</p> <ul> <li> <code>square_form</code>           \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def square_norm(self, value) -&gt; torch.Tensor:\n    \"\"\"Compute the square norm under post. Precision with `value-self.mean` as \ud835\udee5:\n\n    $$\n        \\\\Delta^\\top P \\\\Delta\n    $$\n\n    Returns\n    -------\n    square_form\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.log_prob","title":"log_prob","text":"<pre><code>log_prob(value: Tensor, normalized: bool = True) -&gt; Tensor\n</code></pre> <p>Compute the log probability under the (current) Laplace approximation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>log_prob</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def log_prob(self, value: torch.Tensor, normalized: bool = True) -&gt; torch.Tensor:\n    \"\"\"Compute the log probability under the (current) Laplace approximation.\n\n    Parameters\n    ----------\n    value: torch.Tensor\n    normalized : bool, default=True\n        whether to return log of a properly normalized Gaussian or just the\n        terms that depend on `value`.\n\n    Returns\n    -------\n    log_prob : torch.Tensor\n    \"\"\"\n    if not normalized:\n        return -self.square_norm(value) / 2\n    log_prob = (\n        -self.n_params / 2 * log(2 * pi) + self.log_det_posterior_precision / 2\n    )\n    log_prob -= self.square_norm(value) / 2\n    return log_prob\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.log_prob(value)","title":"<code>value</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.log_prob(normalized)","title":"<code>normalized</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to return log of a properly normalized Gaussian or just the terms that depend on <code>value</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.functional_samples","title":"functional_samples","text":"<pre><code>functional_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the function-space posterior on input data <code>x</code>. Can be used, for example, for Thompson sampling or to compute an arbitrary expectation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def functional_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the function-space posterior on input data `x`.\n    Can be used, for example, for Thompson sampling or to compute an arbitrary\n    expectation.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if pred_type == PredType.GLM:\n        f_mu, f_var = self._glm_predictive_distribution(x)\n        return self._glm_functional_samples(\n            f_mu, f_var, n_samples, diagonal_output, generator\n        )\n    else:  # 'nn'\n        return self._nn_functional_samples(x, n_samples, generator)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.functional_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.functional_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.predictive_samples","title":"predictive_samples","text":"<pre><code>predictive_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code>. I.e., the respective inverse-link function (e.g. softmax) is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def predictive_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x`. I.e., the respective\n    inverse-link function (e.g. softmax) is applied on top of the functional\n    sample.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if pred_type == PredType.GLM:\n        f_mu, f_var = self._glm_predictive_distribution(x)\n        return self._glm_predictive_samples(\n            f_mu, f_var, n_samples, diagonal_output, generator\n        )\n    else:  # 'nn'\n        return self._nn_predictive_samples(x, n_samples, generator)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.predictive_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.predictive_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.LowRankLaplace.predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace","title":"FullLaplace","text":"<pre><code>FullLaplace(model: Module, likelihood: Likelihood | str, sigma_noise: float | Tensor = 1.0, prior_precision: float | Tensor = 1.0, prior_mean: float | Tensor = 0.0, temperature: float = 1.0, enable_backprop: bool = False, dict_key_x: str = 'input_ids', dict_key_y: str = 'labels', backend: type[CurvatureInterface] | None = None, backend_kwargs: dict[str, Any] | None = None)\n</code></pre> <p>               Bases: <code>ParametricLaplace</code></p> <p>Laplace approximation with full, i.e., dense, log likelihood Hessian approximation and hence posterior precision. Based on the chosen <code>backend</code> parameter, the full approximation can be, for example, a generalized Gauss-Newton matrix. Mathematically, we have \\(P \\in \\mathbb{R}^{P \\times P}\\). See <code>BaseLaplace</code> for the full interface.</p> <p>Methods:</p> <ul> <li> <code>log_marginal_likelihood</code>             \u2013              <p>Compute the Laplace approximation to the log marginal likelihood subject</p> </li> <li> <code>__call__</code>             \u2013              <p>Compute the posterior predictive on input data <code>x</code>.</p> </li> <li> <code>log_prob</code>             \u2013              <p>Compute the log probability under the (current) Laplace approximation.</p> </li> <li> <code>functional_samples</code>             \u2013              <p>Sample from the function-space posterior on input data <code>x</code>.</p> </li> <li> <code>predictive_samples</code>             \u2013              <p>Sample from the posterior predictive on input data <code>x</code>. I.e., the respective</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>log_likelihood</code>               (<code>Tensor</code>)           \u2013            <p>Compute log likelihood on the training data after <code>.fit()</code> has been called.</p> </li> <li> <code>prior_precision_diag</code>               (<code>Tensor</code>)           \u2013            <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either</p> </li> <li> <code>scatter</code>               (<code>Tensor</code>)           \u2013            <p>Computes the scatter, a term of the log marginal likelihood that</p> </li> <li> <code>log_det_prior_precision</code>               (<code>Tensor</code>)           \u2013            <p>Compute log determinant of the prior precision</p> </li> <li> <code>log_det_ratio</code>               (<code>Tensor</code>)           \u2013            <p>Compute the log determinant ratio, a part of the log marginal likelihood.</p> </li> <li> <code>posterior_scale</code>               (<code>Tensor</code>)           \u2013            <p>Posterior scale (square root of the covariance), i.e.,</p> </li> <li> <code>posterior_covariance</code>               (<code>Tensor</code>)           \u2013            <p>Posterior covariance, i.e., \\(P^{-1}\\).</p> </li> <li> <code>posterior_precision</code>               (<code>Tensor</code>)           \u2013            <p>Posterior precision \\(P\\).</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    sigma_noise: float | torch.Tensor = 1.0,\n    prior_precision: float | torch.Tensor = 1.0,\n    prior_mean: float | torch.Tensor = 0.0,\n    temperature: float = 1.0,\n    enable_backprop: bool = False,\n    dict_key_x: str = \"input_ids\",\n    dict_key_y: str = \"labels\",\n    backend: type[CurvatureInterface] | None = None,\n    backend_kwargs: dict[str, Any] | None = None,\n):\n    super().__init__(\n        model,\n        likelihood,\n        sigma_noise,\n        prior_precision,\n        prior_mean,\n        temperature,\n        enable_backprop,\n        dict_key_x,\n        dict_key_y,\n        backend,\n        backend_kwargs,\n    )\n    self._posterior_scale: torch.Tensor | None = None\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.log_likelihood","title":"log_likelihood","text":"<pre><code>log_likelihood: Tensor\n</code></pre> <p>Compute log likelihood on the training data after <code>.fit()</code> has been called. The log likelihood is computed on-demand based on the loss and, for example, the observation noise which makes it differentiable in the latter for iterative updates.</p> <p>Returns:</p> <ul> <li> <code>log_likelihood</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.prior_precision_diag","title":"prior_precision_diag","text":"<pre><code>prior_precision_diag: Tensor\n</code></pre> <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either a scalar, layer-wise, or diagonal prior precision.</p> <p>Returns:</p> <ul> <li> <code>prior_precision_diag</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.scatter","title":"scatter","text":"<pre><code>scatter: Tensor\n</code></pre> <p>Computes the scatter, a term of the log marginal likelihood that corresponds to L-2 regularization: <code>scatter</code> = \\((\\theta_{MAP} - \\mu_0)^{T} P_0 (\\theta_{MAP} - \\mu_0) \\).</p> <p>Returns:</p> <ul> <li> <code>scatter</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.log_det_prior_precision","title":"log_det_prior_precision","text":"<pre><code>log_det_prior_precision: Tensor\n</code></pre> <p>Compute log determinant of the prior precision \\(\\log \\det P_0\\)</p> <p>Returns:</p> <ul> <li> <code>log_det</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.log_det_ratio","title":"log_det_ratio","text":"<pre><code>log_det_ratio: Tensor\n</code></pre> <p>Compute the log determinant ratio, a part of the log marginal likelihood.</p> \\[     \\log \\frac{\\det P}{\\det P_0} = \\log \\det P - \\log \\det P_0 \\] <p>Returns:</p> <ul> <li> <code>log_det_ratio</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.posterior_scale","title":"posterior_scale","text":"<pre><code>posterior_scale: Tensor\n</code></pre> <p>Posterior scale (square root of the covariance), i.e., \\(P^{-\\frac{1}{2}}\\).</p> <p>Returns:</p> <ul> <li> <code>scale</code> (              <code>tensor</code> )          \u2013            <p><code>(parameters, parameters)</code></p> </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.posterior_covariance","title":"posterior_covariance","text":"<pre><code>posterior_covariance: Tensor\n</code></pre> <p>Posterior covariance, i.e., \\(P^{-1}\\).</p> <p>Returns:</p> <ul> <li> <code>covariance</code> (              <code>tensor</code> )          \u2013            <p><code>(parameters, parameters)</code></p> </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.posterior_precision","title":"posterior_precision","text":"<pre><code>posterior_precision: Tensor\n</code></pre> <p>Posterior precision \\(P\\).</p> <p>Returns:</p> <ul> <li> <code>precision</code> (              <code>tensor</code> )          \u2013            <p><code>(parameters, parameters)</code></p> </li> </ul>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.log_marginal_likelihood","title":"log_marginal_likelihood","text":"<pre><code>log_marginal_likelihood(prior_precision: Tensor | None = None, sigma_noise: Tensor | None = None) -&gt; Tensor\n</code></pre> <p>Compute the Laplace approximation to the log marginal likelihood subject to specific Hessian approximations that subclasses implement. Requires that the Laplace approximation has been fit before. The resulting torch.Tensor is differentiable in <code>prior_precision</code> and <code>sigma_noise</code> if these have gradients enabled. By passing <code>prior_precision</code> or <code>sigma_noise</code>, the current value is overwritten. This is useful for iterating on the log marginal likelihood.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>log_marglik</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def log_marginal_likelihood(\n    self,\n    prior_precision: torch.Tensor | None = None,\n    sigma_noise: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the Laplace approximation to the log marginal likelihood subject\n    to specific Hessian approximations that subclasses implement.\n    Requires that the Laplace approximation has been fit before.\n    The resulting torch.Tensor is differentiable in `prior_precision` and\n    `sigma_noise` if these have gradients enabled.\n    By passing `prior_precision` or `sigma_noise`, the current value is\n    overwritten. This is useful for iterating on the log marginal likelihood.\n\n    Parameters\n    ----------\n    prior_precision : torch.Tensor, optional\n        prior precision if should be changed from current `prior_precision` value\n    sigma_noise : torch.Tensor, optional\n        observation noise standard deviation if should be changed\n\n    Returns\n    -------\n    log_marglik : torch.Tensor\n    \"\"\"\n    # update prior precision (useful when iterating on marglik)\n    if prior_precision is not None:\n        self.prior_precision = prior_precision\n\n    # update sigma_noise (useful when iterating on marglik)\n    if sigma_noise is not None:\n        if self.likelihood != Likelihood.REGRESSION:\n            raise ValueError(\"Can only change sigma_noise for regression.\")\n\n        self.sigma_noise = sigma_noise\n\n    return self.log_likelihood - 0.5 * (self.log_det_ratio + self.scatter)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.log_marginal_likelihood(prior_precision)","title":"<code>prior_precision</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>prior precision if should be changed from current <code>prior_precision</code> value</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.log_marginal_likelihood(sigma_noise)","title":"<code>sigma_noise</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>observation noise standard deviation if should be changed</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.__call__","title":"__call__","text":"<pre><code>__call__(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None, fitting: bool = False, **model_kwargs: dict[str, Any]) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def __call__(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n    fitting: bool = False,\n    **model_kwargs: dict[str, Any],\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x`.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here. When Laplace is done only\n        on subset of parameters (i.e. some grad are disabled),\n        only `nn` predictive is supported.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` when `joint=False` in regression.\n        In the case of last-layer Laplace with a diagonal or Kron Hessian,\n        setting this to `True` makes computation much(!) faster for large\n        number of outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used).\n\n    fitting : bool, default=False\n        whether or not this predictive call is done during fitting. Only useful for\n        reward modeling: the likelihood is set to `\"regression\"` when `False` and\n        `\"classification\"` when `True`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    if pred_type not in [pred for pred in PredType]:\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if link_approx not in [la for la in LinkApprox]:\n        raise ValueError(f\"Unsupported link approximation {link_approx}.\")\n\n    if pred_type == PredType.NN and link_approx != LinkApprox.MC:\n        raise ValueError(\n            \"Only mc link approximation is supported for nn prediction type.\"\n        )\n\n    if generator is not None:\n        if (\n            not isinstance(generator, torch.Generator)\n            or generator.device != self._device\n        ):\n            raise ValueError(\"Invalid random generator (check type and device).\")\n\n    likelihood = self.likelihood\n    if likelihood == Likelihood.REWARD_MODELING:\n        likelihood = Likelihood.CLASSIFICATION if fitting else Likelihood.REGRESSION\n\n    if pred_type == PredType.GLM:\n        return self._glm_forward_call(\n            x, likelihood, joint, link_approx, n_samples, diagonal_output\n        )\n    else:\n        if likelihood == Likelihood.REGRESSION:\n            samples = self._nn_predictive_samples(x, n_samples, **model_kwargs)\n            return samples.mean(dim=0), samples.var(dim=0)\n        else:  # classification; the average is computed online\n            return self._nn_predictive_classification(x, n_samples, **model_kwargs)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.__call__(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.__call__(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here. When Laplace is done only on subset of parameters (i.e. some grad are disabled), only <code>nn</code> predictive is supported.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.__call__(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.__call__(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.__call__(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.__call__(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> when <code>joint=False</code> in regression. In the case of last-layer Laplace with a diagonal or Kron Hessian, setting this to <code>True</code> makes computation much(!) faster for large number of outputs.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.__call__(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used).</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.__call__(fitting)","title":"<code>fitting</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether or not this predictive call is done during fitting. Only useful for reward modeling: the likelihood is set to <code>\"regression\"</code> when <code>False</code> and <code>\"classification\"</code> when <code>True</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace._glm_forward_call","title":"_glm_forward_call","text":"<pre><code>_glm_forward_call(x: Tensor | MutableMapping, likelihood: Likelihood | str, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code> for \"glm\" pred type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_forward_call(\n    self,\n    x: torch.Tensor | MutableMapping,\n    likelihood: Likelihood | str,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x` for \"glm\" pred type.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    likelihood : Likelihood or str in {'classification', 'regression', 'reward_modeling'}\n        determines the log likelihood Hessian approximation.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` and `link_approx='mc'`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    f_mu, f_var = self._glm_predictive_distribution(\n        x, joint=joint and likelihood == Likelihood.REGRESSION\n    )\n\n    if likelihood == Likelihood.REGRESSION:\n        if diagonal_output and not joint:\n            f_var = torch.diagonal(f_var, dim1=-2, dim2=-1)\n        return f_mu, f_var\n\n    if link_approx == LinkApprox.MC:\n        return self._glm_predictive_samples(\n            f_mu,\n            f_var,\n            n_samples=n_samples,\n            diagonal_output=diagonal_output,\n        ).mean(dim=0)\n    elif link_approx == LinkApprox.PROBIT:\n        kappa = 1 / torch.sqrt(1.0 + np.pi / 8 * f_var.diagonal(dim1=1, dim2=2))\n        return torch.softmax(kappa * f_mu, dim=-1)\n    elif \"bridge\" in link_approx:\n        # zero mean correction\n        f_mu -= (\n            f_var.sum(-1)\n            * f_mu.sum(-1).reshape(-1, 1)\n            / f_var.sum(dim=(1, 2)).reshape(-1, 1)\n        )\n        f_var -= torch.einsum(\n            \"bi,bj-&gt;bij\", f_var.sum(-1), f_var.sum(-2)\n        ) / f_var.sum(dim=(1, 2)).reshape(-1, 1, 1)\n\n        # Laplace Bridge\n        _, K = f_mu.size(0), f_mu.size(-1)\n        f_var_diag = torch.diagonal(f_var, dim1=1, dim2=2)\n\n        # optional: variance correction\n        if link_approx == LinkApprox.BRIDGE_NORM:\n            f_var_diag_mean = f_var_diag.mean(dim=1)\n            f_var_diag_mean /= torch.as_tensor(\n                [K / 2], device=self._device, dtype=self._dtype\n            ).sqrt()\n            f_mu /= f_var_diag_mean.sqrt().unsqueeze(-1)\n            f_var_diag /= f_var_diag_mean.unsqueeze(-1)\n\n        sum_exp = torch.exp(-f_mu).sum(dim=1).unsqueeze(-1)\n        alpha = (1 - 2 / K + f_mu.exp() / K**2 * sum_exp) / f_var_diag\n        return torch.nan_to_num(alpha / alpha.sum(dim=1).unsqueeze(-1), nan=1.0)\n    else:\n        raise ValueError(\n            \"Prediction path invalid. Check the likelihood, pred_type, link_approx combination!\"\n        )\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace._glm_forward_call(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace._glm_forward_call(likelihood)","title":"<code>likelihood</code>","text":"(<code>Likelihood or str in {'classification', 'regression', 'reward_modeling'}</code>)           \u2013            <p>determines the log likelihood Hessian approximation.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace._glm_forward_call(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace._glm_forward_call(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace._glm_forward_call(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace._glm_forward_call(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> and <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace._glm_functional_samples","title":"_glm_functional_samples","text":"<pre><code>_glm_functional_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior functional on input data <code>x</code> using \"glm\" prediction type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_functional_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior functional on input data `x` using \"glm\" prediction\n    type.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    assert f_var.shape == torch.Size([f_mu.shape[0], f_mu.shape[1], f_mu.shape[1]])\n\n    if diagonal_output:\n        f_var = torch.diagonal(f_var, dim1=1, dim2=2)\n\n    return normal_samples(f_mu, f_var, n_samples, generator)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace._glm_functional_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace._glm_functional_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace._glm_functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace._glm_functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace._glm_functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace._glm_predictive_samples","title":"_glm_predictive_samples","text":"<pre><code>_glm_predictive_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code> using \"glm\" prediction type. I.e., the inverse-link function correponding to the likelihood is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_predictive_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x` using \"glm\" prediction\n    type. I.e., the inverse-link function correponding to the likelihood is applied\n    on top of the functional sample.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    f_samples = self._glm_functional_samples(\n        f_mu, f_var, n_samples, diagonal_output, generator\n    )\n\n    if self.likelihood == Likelihood.REGRESSION:\n        return f_samples\n    else:\n        return torch.softmax(f_samples, dim=-1)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace._glm_predictive_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace._glm_predictive_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace._glm_predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace._glm_predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace._glm_predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.log_prob","title":"log_prob","text":"<pre><code>log_prob(value: Tensor, normalized: bool = True) -&gt; Tensor\n</code></pre> <p>Compute the log probability under the (current) Laplace approximation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>log_prob</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def log_prob(self, value: torch.Tensor, normalized: bool = True) -&gt; torch.Tensor:\n    \"\"\"Compute the log probability under the (current) Laplace approximation.\n\n    Parameters\n    ----------\n    value: torch.Tensor\n    normalized : bool, default=True\n        whether to return log of a properly normalized Gaussian or just the\n        terms that depend on `value`.\n\n    Returns\n    -------\n    log_prob : torch.Tensor\n    \"\"\"\n    if not normalized:\n        return -self.square_norm(value) / 2\n    log_prob = (\n        -self.n_params / 2 * log(2 * pi) + self.log_det_posterior_precision / 2\n    )\n    log_prob -= self.square_norm(value) / 2\n    return log_prob\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.log_prob(value)","title":"<code>value</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.log_prob(normalized)","title":"<code>normalized</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to return log of a properly normalized Gaussian or just the terms that depend on <code>value</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.functional_samples","title":"functional_samples","text":"<pre><code>functional_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the function-space posterior on input data <code>x</code>. Can be used, for example, for Thompson sampling or to compute an arbitrary expectation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def functional_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the function-space posterior on input data `x`.\n    Can be used, for example, for Thompson sampling or to compute an arbitrary\n    expectation.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if pred_type == PredType.GLM:\n        f_mu, f_var = self._glm_predictive_distribution(x)\n        return self._glm_functional_samples(\n            f_mu, f_var, n_samples, diagonal_output, generator\n        )\n    else:  # 'nn'\n        return self._nn_functional_samples(x, n_samples, generator)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.functional_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.functional_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.predictive_samples","title":"predictive_samples","text":"<pre><code>predictive_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code>. I.e., the respective inverse-link function (e.g. softmax) is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def predictive_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x`. I.e., the respective\n    inverse-link function (e.g. softmax) is applied on top of the functional\n    sample.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if pred_type == PredType.GLM:\n        f_mu, f_var = self._glm_predictive_distribution(x)\n        return self._glm_predictive_samples(\n            f_mu, f_var, n_samples, diagonal_output, generator\n        )\n    else:  # 'nn'\n        return self._nn_predictive_samples(x, n_samples, generator)\n</code></pre>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.predictive_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.predictive_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/parametriclaplace/#laplace.baselaplace.FullLaplace.predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/subnetlaplace/","title":"Subnet Laplace","text":""},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace","title":"laplace.subnetlaplace","text":"<p>Classes:</p> <ul> <li> <code>SubnetLaplace</code>           \u2013            <p>Class for subnetwork Laplace, which computes the Laplace approximation over just a subset</p> </li> <li> <code>DiagSubnetLaplace</code>           \u2013            <p>Subnetwork Laplace approximation with diagonal log likelihood Hessian approximation</p> </li> <li> <code>FullSubnetLaplace</code>           \u2013            <p>Subnetwork Laplace approximation with full, i.e., dense, log likelihood Hessian</p> </li> </ul>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace","title":"SubnetLaplace","text":"<pre><code>SubnetLaplace(model: Module, likelihood: Likelihood | str, subnetwork_indices: LongTensor, sigma_noise: float | Tensor = 1.0, prior_precision: float | Tensor = 1.0, prior_mean: float | Tensor = 0.0, temperature: float = 1.0, backend: Type[CurvatureInterface] | None = None, backend_kwargs: dict | None = None, asdl_fisher_kwargs: dict | None = None)\n</code></pre> <p>               Bases: <code>ParametricLaplace</code></p> <p>Class for subnetwork Laplace, which computes the Laplace approximation over just a subset of the model parameters (i.e. a subnetwork within the neural network), as proposed in [1]. Subnetwork Laplace can only be used with either a full or a diagonal Hessian approximation.</p> <p>A Laplace approximation is represented by a MAP which is given by the <code>model</code> parameter and a posterior precision or covariance specifying a Gaussian distribution \\(\\mathcal{N}(\\theta_{MAP}, P^{-1})\\). Here, only a subset of the model parameters (i.e. a subnetwork of the neural network) are treated probabilistically. The goal of this class is to compute the posterior precision \\(P\\) which sums as</p> \\[     P = \\sum_{n=1}^N \\nabla^2_\\theta \\log p(\\mathcal{D}_n \\mid \\theta)     \\vert_{\\theta_{MAP}} + \\nabla^2_\\theta \\log p(\\theta) \\vert_{\\theta_{MAP}}. \\] <p>The prior is assumed to be Gaussian and therefore we have a simple form for \\(\\nabla^2_\\theta \\log p(\\theta) \\vert_{\\theta_{MAP}} = P_0 \\). In particular, we assume a scalar or diagonal prior precision so that in all cases \\(P_0 = \\textrm{diag}(p_0)\\) and the structure of \\(p_0\\) can be varied.</p> <p>The subnetwork Laplace approximation only supports a full, i.e., dense, log likelihood Hessian approximation and hence posterior precision.  Based on the chosen <code>backend</code> parameter, the full approximation can be, for example, a generalized Gauss-Newton matrix.  Mathematically, we have \\(P \\in \\mathbb{R}^{P \\times P}\\). See <code>FullLaplace</code> and <code>BaseLaplace</code> for the full interface.</p> References <p>[1] Daxberger, E., Nalisnick, E., Allingham, JU., Antor\u00e1n, J., Hern\u00e1ndez-Lobato, JM. Bayesian Deep Learning via Subnetwork Inference. ICML 2021.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>fit</code>             \u2013              <p>Fit the local Laplace approximation at the parameters of the model.</p> </li> <li> <code>log_marginal_likelihood</code>             \u2013              <p>Compute the Laplace approximation to the log marginal likelihood subject</p> </li> <li> <code>__call__</code>             \u2013              <p>Compute the posterior predictive on input data <code>x</code>.</p> </li> <li> <code>square_norm</code>             \u2013              <p>Compute the square norm under post. Precision with <code>value-self.mean</code> as \ud835\udee5:</p> </li> <li> <code>log_prob</code>             \u2013              <p>Compute the log probability under the (current) Laplace approximation.</p> </li> <li> <code>functional_samples</code>             \u2013              <p>Sample from the function-space posterior on input data <code>x</code>.</p> </li> <li> <code>predictive_samples</code>             \u2013              <p>Sample from the posterior predictive on input data <code>x</code>. I.e., the respective</p> </li> <li> <code>functional_variance</code>             \u2013              <p>Compute functional variance for the <code>'glm'</code> predictive:</p> </li> <li> <code>functional_covariance</code>             \u2013              <p>Compute functional covariance for the <code>'glm'</code> predictive:</p> </li> <li> <code>sample</code>             \u2013              <p>Sample from the Laplace posterior approximation, i.e.,</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>log_likelihood</code>               (<code>Tensor</code>)           \u2013            <p>Compute log likelihood on the training data after <code>.fit()</code> has been called.</p> </li> <li> <code>log_det_prior_precision</code>               (<code>Tensor</code>)           \u2013            <p>Compute log determinant of the prior precision</p> </li> <li> <code>log_det_posterior_precision</code>               (<code>Tensor</code>)           \u2013            <p>Compute log determinant of the posterior precision</p> </li> <li> <code>log_det_ratio</code>               (<code>Tensor</code>)           \u2013            <p>Compute the log determinant ratio, a part of the log marginal likelihood.</p> </li> <li> <code>posterior_precision</code>               (<code>Tensor</code>)           \u2013            <p>Compute or return the posterior precision \\(P\\).</p> </li> <li> <code>prior_precision_diag</code>               (<code>Tensor</code>)           \u2013            <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either</p> </li> </ul> Source code in <code>laplace/subnetlaplace.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    subnetwork_indices: torch.LongTensor,\n    sigma_noise: float | torch.Tensor = 1.0,\n    prior_precision: float | torch.Tensor = 1.0,\n    prior_mean: float | torch.Tensor = 0.0,\n    temperature: float = 1.0,\n    backend: Type[CurvatureInterface] | None = None,\n    backend_kwargs: dict | None = None,\n    asdl_fisher_kwargs: dict | None = None,\n) -&gt; None:\n    if asdl_fisher_kwargs is not None:\n        raise ValueError(\"Subnetwork Laplace does not support asdl_fisher_kwargs.\")\n\n    self.H = None\n    super().__init__(\n        model,\n        likelihood,\n        sigma_noise=sigma_noise,\n        prior_precision=prior_precision,\n        prior_mean=prior_mean,\n        temperature=temperature,\n        backend=backend,\n        backend_kwargs=backend_kwargs,\n    )\n\n    if backend is not None and not issubclass(backend, (GGNInterface, EFInterface)):\n        raise ValueError(\"SubnetLaplace can only be used with GGN and EF.\")\n\n    # check validity of subnetwork indices and pass them to backend\n    self._check_subnetwork_indices(subnetwork_indices)\n    self.backend.subnetwork_indices = subnetwork_indices\n    self.n_params_subnet = len(subnetwork_indices)\n    self._init_H()\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace(model)","title":"<code>model</code>","text":"(<code>torch.nn.Module or `laplace.utils.feature_extractor.FeatureExtractor`</code>)           \u2013"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace(likelihood)","title":"<code>likelihood</code>","text":"(<code>('classification', 'regression')</code>, default:                   <code>'classification'</code> )           \u2013            <p>determines the log likelihood Hessian approximation</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace(subnetwork_indices)","title":"<code>subnetwork_indices</code>","text":"(<code>LongTensor</code>)           \u2013            <p>indices of the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>) that define the subnetwork to apply the Laplace approximation over</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace(sigma_noise)","title":"<code>sigma_noise</code>","text":"(<code>Tensor or float</code>, default:                   <code>1</code> )           \u2013            <p>observation noise for the regression setting; must be 1 for classification</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace(prior_precision)","title":"<code>prior_precision</code>","text":"(<code>Tensor or float</code>, default:                   <code>1</code> )           \u2013            <p>prior precision of a Gaussian prior (= weight decay); can be scalar, per-layer, or diagonal in the most general case</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace(prior_mean)","title":"<code>prior_mean</code>","text":"(<code>Tensor or float</code>, default:                   <code>0</code> )           \u2013            <p>prior mean of a Gaussian prior, useful for continual learning</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace(temperature)","title":"<code>temperature</code>","text":"(<code>float</code>, default:                   <code>1</code> )           \u2013            <p>temperature of the likelihood; lower temperature leads to more concentrated posterior and vice versa.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace(backend)","title":"<code>backend</code>","text":"(<code>subclasses of `laplace.curvature.{GGNInterface,EFInterface}`</code>, default:                   <code>None</code> )           \u2013            <p>backend for access to curvature/Hessian approximations</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace(backend_kwargs)","title":"<code>backend_kwargs</code>","text":"(<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>arguments passed to the backend on initialization, for example to set the number of MC samples for stochastic approximations.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.log_likelihood","title":"log_likelihood","text":"<pre><code>log_likelihood: Tensor\n</code></pre> <p>Compute log likelihood on the training data after <code>.fit()</code> has been called. The log likelihood is computed on-demand based on the loss and, for example, the observation noise which makes it differentiable in the latter for iterative updates.</p> <p>Returns:</p> <ul> <li> <code>log_likelihood</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.log_det_prior_precision","title":"log_det_prior_precision","text":"<pre><code>log_det_prior_precision: Tensor\n</code></pre> <p>Compute log determinant of the prior precision \\(\\log \\det P_0\\)</p> <p>Returns:</p> <ul> <li> <code>log_det</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.log_det_posterior_precision","title":"log_det_posterior_precision","text":"<pre><code>log_det_posterior_precision: Tensor\n</code></pre> <p>Compute log determinant of the posterior precision \\(\\log \\det P\\) which depends on the subclasses structure used for the Hessian approximation.</p> <p>Returns:</p> <ul> <li> <code>log_det</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.log_det_ratio","title":"log_det_ratio","text":"<pre><code>log_det_ratio: Tensor\n</code></pre> <p>Compute the log determinant ratio, a part of the log marginal likelihood.</p> \\[     \\log \\frac{\\det P}{\\det P_0} = \\log \\det P - \\log \\det P_0 \\] <p>Returns:</p> <ul> <li> <code>log_det_ratio</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.posterior_precision","title":"posterior_precision","text":"<pre><code>posterior_precision: Tensor\n</code></pre> <p>Compute or return the posterior precision \\(P\\).</p> <p>Returns:</p> <ul> <li> <code>posterior_prec</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.prior_precision_diag","title":"prior_precision_diag","text":"<pre><code>prior_precision_diag: Tensor\n</code></pre> <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either a scalar or diagonal prior precision.</p> <p>Returns:</p> <ul> <li> <code>prior_precision_diag</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.fit","title":"fit","text":"<pre><code>fit(train_loader: DataLoader, override: bool = True, progress_bar: bool = False) -&gt; None\n</code></pre> <p>Fit the local Laplace approximation at the parameters of the model.</p> <p>Parameters:</p> Source code in <code>laplace/baselaplace.py</code> <pre><code>def fit(\n    self,\n    train_loader: DataLoader,\n    override: bool = True,\n    progress_bar: bool = False,\n) -&gt; None:\n    \"\"\"Fit the local Laplace approximation at the parameters of the model.\n\n    Parameters\n    ----------\n    train_loader : torch.data.utils.DataLoader\n        each iterate is a training batch, either `(X, y)` tensors or a dict-like\n        object containing keys as expressed by `self.dict_key_x` and\n        `self.dict_key_y`. `train_loader.dataset` needs to be set to access\n        \\\\(N\\\\), size of the data set.\n    override : bool, default=True\n        whether to initialize H, loss, and n_data again; setting to False is useful for\n        online learning settings to accumulate a sequential posterior approximation.\n    progress_bar : bool, default=False\n        whether to show a progress bar; updated at every batch-Hessian computation.\n        Useful for very large model and large amount of data, esp. when `subset_of_weights='all'`.\n    \"\"\"\n    if override:\n        self._init_H()\n        self.loss: float | torch.Tensor = 0\n        self.n_data: int = 0\n\n    self.model.eval()\n\n    self.mean: torch.Tensor = parameters_to_vector(self.params)\n    if not self.enable_backprop:\n        self.mean = self.mean.detach()\n\n    data: (\n        tuple[torch.Tensor, torch.Tensor] | MutableMapping[str, torch.Tensor | Any]\n    ) = next(iter(train_loader))\n\n    with torch.no_grad():\n        if isinstance(data, MutableMapping):  # To support Huggingface dataset\n            if \"backpack\" in self._backend_cls.__name__.lower() or (\n                isinstance(self, DiagLaplace) and self._backend_cls == CurvlinopsEF\n            ):\n                raise ValueError(\n                    \"Currently DiagEF is not supported under CurvlinopsEF backend \"\n                    + \"for custom models with non-tensor inputs \"\n                    + \"(https://github.com/pytorch/functorch/issues/159). Consider \"\n                    + \"using AsdlEF backend instead. The same limitation applies \"\n                    + \"to all BackPACK backend\"\n                )\n\n            out = self.model(data)\n        else:\n            X = data[0]\n            try:\n                out = self.model(X[:1].to(self._device))\n            except (TypeError, AttributeError):\n                out = self.model(X.to(self._device))\n    self.n_outputs = out.shape[-1]\n    setattr(self.model, \"output_size\", self.n_outputs)\n\n    N = len(train_loader.dataset)\n\n    pbar = tqdm.tqdm(train_loader, disable=not progress_bar)\n    pbar.set_description(\"[Computing Hessian]\")\n\n    for data in pbar:\n        if isinstance(data, MutableMapping):  # To support Huggingface dataset\n            X, y = data, data[self.dict_key_y].to(self._device)\n        else:\n            X, y = data\n            X, y = X.to(self._device), y.to(self._device)\n\n        if self.likelihood == Likelihood.REGRESSION and y.ndim != out.ndim:\n            raise ValueError(\n                f\"The model's output has {out.ndim} dims but \"\n                f\"the target has {y.ndim} dims.\"\n            )\n\n        self.model.zero_grad()\n        loss_batch, H_batch = self._curv_closure(X, y, N=N)\n        self.loss += loss_batch\n        self.H += H_batch\n\n    self.n_data += N\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.fit(train_loader)","title":"<code>train_loader</code>","text":"(<code>DataLoader</code>)           \u2013            <p>each iterate is a training batch, either <code>(X, y)</code> tensors or a dict-like object containing keys as expressed by <code>self.dict_key_x</code> and <code>self.dict_key_y</code>. <code>train_loader.dataset</code> needs to be set to access \\(N\\), size of the data set.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.fit(override)","title":"<code>override</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to initialize H, loss, and n_data again; setting to False is useful for online learning settings to accumulate a sequential posterior approximation.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.fit(progress_bar)","title":"<code>progress_bar</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to show a progress bar; updated at every batch-Hessian computation. Useful for very large model and large amount of data, esp. when <code>subset_of_weights='all'</code>.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.log_marginal_likelihood","title":"log_marginal_likelihood","text":"<pre><code>log_marginal_likelihood(prior_precision: Tensor | None = None, sigma_noise: Tensor | None = None) -&gt; Tensor\n</code></pre> <p>Compute the Laplace approximation to the log marginal likelihood subject to specific Hessian approximations that subclasses implement. Requires that the Laplace approximation has been fit before. The resulting torch.Tensor is differentiable in <code>prior_precision</code> and <code>sigma_noise</code> if these have gradients enabled. By passing <code>prior_precision</code> or <code>sigma_noise</code>, the current value is overwritten. This is useful for iterating on the log marginal likelihood.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>log_marglik</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def log_marginal_likelihood(\n    self,\n    prior_precision: torch.Tensor | None = None,\n    sigma_noise: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the Laplace approximation to the log marginal likelihood subject\n    to specific Hessian approximations that subclasses implement.\n    Requires that the Laplace approximation has been fit before.\n    The resulting torch.Tensor is differentiable in `prior_precision` and\n    `sigma_noise` if these have gradients enabled.\n    By passing `prior_precision` or `sigma_noise`, the current value is\n    overwritten. This is useful for iterating on the log marginal likelihood.\n\n    Parameters\n    ----------\n    prior_precision : torch.Tensor, optional\n        prior precision if should be changed from current `prior_precision` value\n    sigma_noise : torch.Tensor, optional\n        observation noise standard deviation if should be changed\n\n    Returns\n    -------\n    log_marglik : torch.Tensor\n    \"\"\"\n    # update prior precision (useful when iterating on marglik)\n    if prior_precision is not None:\n        self.prior_precision = prior_precision\n\n    # update sigma_noise (useful when iterating on marglik)\n    if sigma_noise is not None:\n        if self.likelihood != Likelihood.REGRESSION:\n            raise ValueError(\"Can only change sigma_noise for regression.\")\n\n        self.sigma_noise = sigma_noise\n\n    return self.log_likelihood - 0.5 * (self.log_det_ratio + self.scatter)\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.log_marginal_likelihood(prior_precision)","title":"<code>prior_precision</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>prior precision if should be changed from current <code>prior_precision</code> value</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.log_marginal_likelihood(sigma_noise)","title":"<code>sigma_noise</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>observation noise standard deviation if should be changed</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.__call__","title":"__call__","text":"<pre><code>__call__(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None, fitting: bool = False, **model_kwargs: dict[str, Any]) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def __call__(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n    fitting: bool = False,\n    **model_kwargs: dict[str, Any],\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x`.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here. When Laplace is done only\n        on subset of parameters (i.e. some grad are disabled),\n        only `nn` predictive is supported.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` when `joint=False` in regression.\n        In the case of last-layer Laplace with a diagonal or Kron Hessian,\n        setting this to `True` makes computation much(!) faster for large\n        number of outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used).\n\n    fitting : bool, default=False\n        whether or not this predictive call is done during fitting. Only useful for\n        reward modeling: the likelihood is set to `\"regression\"` when `False` and\n        `\"classification\"` when `True`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    if pred_type not in [pred for pred in PredType]:\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if link_approx not in [la for la in LinkApprox]:\n        raise ValueError(f\"Unsupported link approximation {link_approx}.\")\n\n    if pred_type == PredType.NN and link_approx != LinkApprox.MC:\n        raise ValueError(\n            \"Only mc link approximation is supported for nn prediction type.\"\n        )\n\n    if generator is not None:\n        if (\n            not isinstance(generator, torch.Generator)\n            or generator.device != self._device\n        ):\n            raise ValueError(\"Invalid random generator (check type and device).\")\n\n    likelihood = self.likelihood\n    if likelihood == Likelihood.REWARD_MODELING:\n        likelihood = Likelihood.CLASSIFICATION if fitting else Likelihood.REGRESSION\n\n    if pred_type == PredType.GLM:\n        return self._glm_forward_call(\n            x, likelihood, joint, link_approx, n_samples, diagonal_output\n        )\n    else:\n        if likelihood == Likelihood.REGRESSION:\n            samples = self._nn_predictive_samples(x, n_samples, **model_kwargs)\n            return samples.mean(dim=0), samples.var(dim=0)\n        else:  # classification; the average is computed online\n            return self._nn_predictive_classification(x, n_samples, **model_kwargs)\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.__call__(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.__call__(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here. When Laplace is done only on subset of parameters (i.e. some grad are disabled), only <code>nn</code> predictive is supported.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.__call__(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.__call__(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.__call__(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.__call__(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> when <code>joint=False</code> in regression. In the case of last-layer Laplace with a diagonal or Kron Hessian, setting this to <code>True</code> makes computation much(!) faster for large number of outputs.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.__call__(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used).</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.__call__(fitting)","title":"<code>fitting</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether or not this predictive call is done during fitting. Only useful for reward modeling: the likelihood is set to <code>\"regression\"</code> when <code>False</code> and <code>\"classification\"</code> when <code>True</code>.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace._glm_forward_call","title":"_glm_forward_call","text":"<pre><code>_glm_forward_call(x: Tensor | MutableMapping, likelihood: Likelihood | str, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code> for \"glm\" pred type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_forward_call(\n    self,\n    x: torch.Tensor | MutableMapping,\n    likelihood: Likelihood | str,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x` for \"glm\" pred type.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    likelihood : Likelihood or str in {'classification', 'regression', 'reward_modeling'}\n        determines the log likelihood Hessian approximation.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` and `link_approx='mc'`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    f_mu, f_var = self._glm_predictive_distribution(\n        x, joint=joint and likelihood == Likelihood.REGRESSION\n    )\n\n    if likelihood == Likelihood.REGRESSION:\n        if diagonal_output and not joint:\n            f_var = torch.diagonal(f_var, dim1=-2, dim2=-1)\n        return f_mu, f_var\n\n    if link_approx == LinkApprox.MC:\n        return self._glm_predictive_samples(\n            f_mu,\n            f_var,\n            n_samples=n_samples,\n            diagonal_output=diagonal_output,\n        ).mean(dim=0)\n    elif link_approx == LinkApprox.PROBIT:\n        kappa = 1 / torch.sqrt(1.0 + np.pi / 8 * f_var.diagonal(dim1=1, dim2=2))\n        return torch.softmax(kappa * f_mu, dim=-1)\n    elif \"bridge\" in link_approx:\n        # zero mean correction\n        f_mu -= (\n            f_var.sum(-1)\n            * f_mu.sum(-1).reshape(-1, 1)\n            / f_var.sum(dim=(1, 2)).reshape(-1, 1)\n        )\n        f_var -= torch.einsum(\n            \"bi,bj-&gt;bij\", f_var.sum(-1), f_var.sum(-2)\n        ) / f_var.sum(dim=(1, 2)).reshape(-1, 1, 1)\n\n        # Laplace Bridge\n        _, K = f_mu.size(0), f_mu.size(-1)\n        f_var_diag = torch.diagonal(f_var, dim1=1, dim2=2)\n\n        # optional: variance correction\n        if link_approx == LinkApprox.BRIDGE_NORM:\n            f_var_diag_mean = f_var_diag.mean(dim=1)\n            f_var_diag_mean /= torch.as_tensor(\n                [K / 2], device=self._device, dtype=self._dtype\n            ).sqrt()\n            f_mu /= f_var_diag_mean.sqrt().unsqueeze(-1)\n            f_var_diag /= f_var_diag_mean.unsqueeze(-1)\n\n        sum_exp = torch.exp(-f_mu).sum(dim=1).unsqueeze(-1)\n        alpha = (1 - 2 / K + f_mu.exp() / K**2 * sum_exp) / f_var_diag\n        return torch.nan_to_num(alpha / alpha.sum(dim=1).unsqueeze(-1), nan=1.0)\n    else:\n        raise ValueError(\n            \"Prediction path invalid. Check the likelihood, pred_type, link_approx combination!\"\n        )\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace._glm_forward_call(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace._glm_forward_call(likelihood)","title":"<code>likelihood</code>","text":"(<code>Likelihood or str in {'classification', 'regression', 'reward_modeling'}</code>)           \u2013            <p>determines the log likelihood Hessian approximation.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace._glm_forward_call(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace._glm_forward_call(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace._glm_forward_call(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace._glm_forward_call(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> and <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace._glm_functional_samples","title":"_glm_functional_samples","text":"<pre><code>_glm_functional_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior functional on input data <code>x</code> using \"glm\" prediction type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_functional_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior functional on input data `x` using \"glm\" prediction\n    type.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    assert f_var.shape == torch.Size([f_mu.shape[0], f_mu.shape[1], f_mu.shape[1]])\n\n    if diagonal_output:\n        f_var = torch.diagonal(f_var, dim1=1, dim2=2)\n\n    return normal_samples(f_mu, f_var, n_samples, generator)\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace._glm_functional_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace._glm_functional_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace._glm_functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace._glm_functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace._glm_functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace._glm_predictive_samples","title":"_glm_predictive_samples","text":"<pre><code>_glm_predictive_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code> using \"glm\" prediction type. I.e., the inverse-link function correponding to the likelihood is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_predictive_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x` using \"glm\" prediction\n    type. I.e., the inverse-link function correponding to the likelihood is applied\n    on top of the functional sample.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    f_samples = self._glm_functional_samples(\n        f_mu, f_var, n_samples, diagonal_output, generator\n    )\n\n    if self.likelihood == Likelihood.REGRESSION:\n        return f_samples\n    else:\n        return torch.softmax(f_samples, dim=-1)\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace._glm_predictive_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace._glm_predictive_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace._glm_predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace._glm_predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace._glm_predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.square_norm","title":"square_norm","text":"<pre><code>square_norm(value) -&gt; Tensor\n</code></pre> <p>Compute the square norm under post. Precision with <code>value-self.mean</code> as \ud835\udee5:</p> \\[     \\Delta^     op P \\Delta \\] <p>Returns:</p> <ul> <li> <code>square_form</code>           \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def square_norm(self, value) -&gt; torch.Tensor:\n    \"\"\"Compute the square norm under post. Precision with `value-self.mean` as \ud835\udee5:\n\n    $$\n        \\\\Delta^\\top P \\\\Delta\n    $$\n\n    Returns\n    -------\n    square_form\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.log_prob","title":"log_prob","text":"<pre><code>log_prob(value: Tensor, normalized: bool = True) -&gt; Tensor\n</code></pre> <p>Compute the log probability under the (current) Laplace approximation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>log_prob</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def log_prob(self, value: torch.Tensor, normalized: bool = True) -&gt; torch.Tensor:\n    \"\"\"Compute the log probability under the (current) Laplace approximation.\n\n    Parameters\n    ----------\n    value: torch.Tensor\n    normalized : bool, default=True\n        whether to return log of a properly normalized Gaussian or just the\n        terms that depend on `value`.\n\n    Returns\n    -------\n    log_prob : torch.Tensor\n    \"\"\"\n    if not normalized:\n        return -self.square_norm(value) / 2\n    log_prob = (\n        -self.n_params / 2 * log(2 * pi) + self.log_det_posterior_precision / 2\n    )\n    log_prob -= self.square_norm(value) / 2\n    return log_prob\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.log_prob(value)","title":"<code>value</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.log_prob(normalized)","title":"<code>normalized</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to return log of a properly normalized Gaussian or just the terms that depend on <code>value</code>.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.functional_samples","title":"functional_samples","text":"<pre><code>functional_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the function-space posterior on input data <code>x</code>. Can be used, for example, for Thompson sampling or to compute an arbitrary expectation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def functional_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the function-space posterior on input data `x`.\n    Can be used, for example, for Thompson sampling or to compute an arbitrary\n    expectation.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if pred_type == PredType.GLM:\n        f_mu, f_var = self._glm_predictive_distribution(x)\n        return self._glm_functional_samples(\n            f_mu, f_var, n_samples, diagonal_output, generator\n        )\n    else:  # 'nn'\n        return self._nn_functional_samples(x, n_samples, generator)\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.functional_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.functional_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.predictive_samples","title":"predictive_samples","text":"<pre><code>predictive_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code>. I.e., the respective inverse-link function (e.g. softmax) is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def predictive_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x`. I.e., the respective\n    inverse-link function (e.g. softmax) is applied on top of the functional\n    sample.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if pred_type == PredType.GLM:\n        f_mu, f_var = self._glm_predictive_distribution(x)\n        return self._glm_predictive_samples(\n            f_mu, f_var, n_samples, diagonal_output, generator\n        )\n    else:  # 'nn'\n        return self._nn_predictive_samples(x, n_samples, generator)\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.predictive_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.predictive_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.functional_variance","title":"functional_variance","text":"<pre><code>functional_variance(Js: Tensor) -&gt; Tensor\n</code></pre> <p>Compute functional variance for the <code>'glm'</code> predictive: <code>f_var[i] = Js[i] @ P.inv() @ Js[i].T</code>, which is a output x output predictive covariance matrix. Mathematically, we have for a single Jacobian \\(\\mathcal{J} = \\nabla_\\theta f(x;\\theta)\\vert_{\\theta_{MAP}}\\) the output covariance matrix \\( \\mathcal{J} P^{-1} \\mathcal{J}^T \\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>f_var</code> (              <code>Tensor</code> )          \u2013            <p>output covariance <code>(batch, outputs, outputs)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def functional_variance(self, Js: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute functional variance for the `'glm'` predictive:\n    `f_var[i] = Js[i] @ P.inv() @ Js[i].T`, which is a output x output\n    predictive covariance matrix.\n    Mathematically, we have for a single Jacobian\n    \\\\(\\\\mathcal{J} = \\\\nabla_\\\\theta f(x;\\\\theta)\\\\vert_{\\\\theta_{MAP}}\\\\)\n    the output covariance matrix\n    \\\\( \\\\mathcal{J} P^{-1} \\\\mathcal{J}^T \\\\).\n\n    Parameters\n    ----------\n    Js : torch.Tensor\n        Jacobians of model output wrt parameters\n        `(batch, outputs, parameters)`\n\n    Returns\n    -------\n    f_var : torch.Tensor\n        output covariance `(batch, outputs, outputs)`\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.functional_variance(Js)","title":"<code>Js</code>","text":"(<code>Tensor</code>)           \u2013            <p>Jacobians of model output wrt parameters <code>(batch, outputs, parameters)</code></p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.functional_covariance","title":"functional_covariance","text":"<pre><code>functional_covariance(Js: Tensor) -&gt; Tensor\n</code></pre> <p>Compute functional covariance for the <code>'glm'</code> predictive: <code>f_cov = Js @ P.inv() @ Js.T</code>, which is a batchoutput x batchoutput predictive covariance matrix.</p> <p>This emulates the GP posterior covariance N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). Useful for joint predictions, such as in batched Bayesian optimization.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>f_cov</code> (              <code>Tensor</code> )          \u2013            <p>output covariance <code>(batch*outputs, batch*outputs)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def functional_covariance(self, Js: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute functional covariance for the `'glm'` predictive:\n    `f_cov = Js @ P.inv() @ Js.T`, which is a batch*output x batch*output\n    predictive covariance matrix.\n\n    This emulates the GP posterior covariance N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n    Useful for joint predictions, such as in batched Bayesian optimization.\n\n    Parameters\n    ----------\n    Js : torch.Tensor\n        Jacobians of model output wrt parameters\n        `(batch*outputs, parameters)`\n\n    Returns\n    -------\n    f_cov : torch.Tensor\n        output covariance `(batch*outputs, batch*outputs)`\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.functional_covariance(Js)","title":"<code>Js</code>","text":"(<code>Tensor</code>)           \u2013            <p>Jacobians of model output wrt parameters <code>(batch*outputs, parameters)</code></p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.sample","title":"sample","text":"<pre><code>sample(n_samples: int = 100, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the Laplace posterior approximation, i.e., \\( \\theta \\sim \\mathcal{N}(\\theta_{MAP}, P^{-1})\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def sample(\n    self, n_samples: int = 100, generator: torch.Generator | None = None\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the Laplace posterior approximation, i.e.,\n    \\\\( \\\\theta \\\\sim \\\\mathcal{N}(\\\\theta_{MAP}, P^{-1})\\\\).\n\n    Parameters\n    ----------\n    n_samples : int, default=100\n        number of samples\n\n    generator : torch.Generator, optional\n        random number generator to control the samples\n\n    Returns\n    -------\n    samples: torch.Tensor\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.sample(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace.sample(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.SubnetLaplace._check_subnetwork_indices","title":"_check_subnetwork_indices","text":"<pre><code>_check_subnetwork_indices(subnetwork_indices: LongTensor | None) -&gt; None\n</code></pre> <p>Check that subnetwork indices are valid indices of the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>).</p> Source code in <code>laplace/subnetlaplace.py</code> <pre><code>def _check_subnetwork_indices(\n    self, subnetwork_indices: torch.LongTensor | None\n) -&gt; None:\n    \"\"\"Check that subnetwork indices are valid indices of the vectorized model parameters\n    (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`).\n    \"\"\"\n    if subnetwork_indices is None:\n        raise ValueError(\"Subnetwork indices cannot be None.\")\n    elif not (\n        isinstance(subnetwork_indices, torch.LongTensor)\n        and subnetwork_indices.numel() &gt; 0\n        and len(subnetwork_indices.shape) == 1\n    ):\n        raise ValueError(\n            \"Subnetwork indices must be non-empty 1-dimensional torch.LongTensor.\"\n        )\n    elif not (\n        len(subnetwork_indices[subnetwork_indices &lt; 0]) == 0\n        and len(subnetwork_indices[subnetwork_indices &gt;= self.n_params]) == 0\n    ):\n        raise ValueError(\n            f\"Subnetwork indices must lie between 0 and n_params={self.n_params}.\"\n        )\n    elif not (len(subnetwork_indices.unique()) == len(subnetwork_indices)):\n        raise ValueError(\"Subnetwork indices must not contain duplicate entries.\")\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace","title":"DiagSubnetLaplace","text":"<pre><code>DiagSubnetLaplace(model: Module, likelihood: Likelihood | str, subnetwork_indices: LongTensor, sigma_noise: float | Tensor = 1.0, prior_precision: float | Tensor = 1.0, prior_mean: float | Tensor = 0.0, temperature: float = 1.0, backend: Type[CurvatureInterface] | None = None, backend_kwargs: dict | None = None, asdl_fisher_kwargs: dict | None = None)\n</code></pre> <p>               Bases: <code>SubnetLaplace</code>, <code>DiagLaplace</code></p> <p>Subnetwork Laplace approximation with diagonal log likelihood Hessian approximation and hence posterior precision. Mathematically, we have \\(P \\approx \\textrm{diag}(P)\\). See <code>DiagLaplace</code>, <code>SubnetLaplace</code>, and <code>BaseLaplace</code> for the full interface.</p> <p>Methods:</p> <ul> <li> <code>fit</code>             \u2013              <p>Fit the local Laplace approximation at the parameters of the model.</p> </li> <li> <code>log_marginal_likelihood</code>             \u2013              <p>Compute the Laplace approximation to the log marginal likelihood subject</p> </li> <li> <code>__call__</code>             \u2013              <p>Compute the posterior predictive on input data <code>x</code>.</p> </li> <li> <code>log_prob</code>             \u2013              <p>Compute the log probability under the (current) Laplace approximation.</p> </li> <li> <code>functional_samples</code>             \u2013              <p>Sample from the function-space posterior on input data <code>x</code>.</p> </li> <li> <code>predictive_samples</code>             \u2013              <p>Sample from the posterior predictive on input data <code>x</code>. I.e., the respective</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>log_likelihood</code>               (<code>Tensor</code>)           \u2013            <p>Compute log likelihood on the training data after <code>.fit()</code> has been called.</p> </li> <li> <code>prior_precision_diag</code>               (<code>Tensor</code>)           \u2013            <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either</p> </li> <li> <code>log_det_prior_precision</code>               (<code>Tensor</code>)           \u2013            <p>Compute log determinant of the prior precision</p> </li> <li> <code>log_det_ratio</code>               (<code>Tensor</code>)           \u2013            <p>Compute the log determinant ratio, a part of the log marginal likelihood.</p> </li> <li> <code>posterior_precision</code>               (<code>Tensor</code>)           \u2013            <p>Diagonal posterior precision \\(p\\).</p> </li> <li> <code>posterior_scale</code>               (<code>Tensor</code>)           \u2013            <p>Diagonal posterior scale \\(\\sqrt{p^{-1}}\\).</p> </li> <li> <code>posterior_variance</code>               (<code>Tensor</code>)           \u2013            <p>Diagonal posterior variance \\(p^{-1}\\).</p> </li> </ul> Source code in <code>laplace/subnetlaplace.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    subnetwork_indices: torch.LongTensor,\n    sigma_noise: float | torch.Tensor = 1.0,\n    prior_precision: float | torch.Tensor = 1.0,\n    prior_mean: float | torch.Tensor = 0.0,\n    temperature: float = 1.0,\n    backend: Type[CurvatureInterface] | None = None,\n    backend_kwargs: dict | None = None,\n    asdl_fisher_kwargs: dict | None = None,\n) -&gt; None:\n    if asdl_fisher_kwargs is not None:\n        raise ValueError(\"Subnetwork Laplace does not support asdl_fisher_kwargs.\")\n\n    self.H = None\n    super().__init__(\n        model,\n        likelihood,\n        sigma_noise=sigma_noise,\n        prior_precision=prior_precision,\n        prior_mean=prior_mean,\n        temperature=temperature,\n        backend=backend,\n        backend_kwargs=backend_kwargs,\n    )\n\n    if backend is not None and not issubclass(backend, (GGNInterface, EFInterface)):\n        raise ValueError(\"SubnetLaplace can only be used with GGN and EF.\")\n\n    # check validity of subnetwork indices and pass them to backend\n    self._check_subnetwork_indices(subnetwork_indices)\n    self.backend.subnetwork_indices = subnetwork_indices\n    self.n_params_subnet = len(subnetwork_indices)\n    self._init_H()\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.log_likelihood","title":"log_likelihood","text":"<pre><code>log_likelihood: Tensor\n</code></pre> <p>Compute log likelihood on the training data after <code>.fit()</code> has been called. The log likelihood is computed on-demand based on the loss and, for example, the observation noise which makes it differentiable in the latter for iterative updates.</p> <p>Returns:</p> <ul> <li> <code>log_likelihood</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.prior_precision_diag","title":"prior_precision_diag","text":"<pre><code>prior_precision_diag: Tensor\n</code></pre> <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either a scalar or diagonal prior precision.</p> <p>Returns:</p> <ul> <li> <code>prior_precision_diag</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.log_det_prior_precision","title":"log_det_prior_precision","text":"<pre><code>log_det_prior_precision: Tensor\n</code></pre> <p>Compute log determinant of the prior precision \\(\\log \\det P_0\\)</p> <p>Returns:</p> <ul> <li> <code>log_det</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.log_det_ratio","title":"log_det_ratio","text":"<pre><code>log_det_ratio: Tensor\n</code></pre> <p>Compute the log determinant ratio, a part of the log marginal likelihood.</p> \\[     \\log \\frac{\\det P}{\\det P_0} = \\log \\det P - \\log \\det P_0 \\] <p>Returns:</p> <ul> <li> <code>log_det_ratio</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.posterior_precision","title":"posterior_precision","text":"<pre><code>posterior_precision: Tensor\n</code></pre> <p>Diagonal posterior precision \\(p\\).</p> <p>Returns:</p> <ul> <li> <code>precision</code> (              <code>tensor</code> )          \u2013            <p><code>(parameters)</code></p> </li> </ul>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.posterior_scale","title":"posterior_scale","text":"<pre><code>posterior_scale: Tensor\n</code></pre> <p>Diagonal posterior scale \\(\\sqrt{p^{-1}}\\).</p> <p>Returns:</p> <ul> <li> <code>precision</code> (              <code>tensor</code> )          \u2013            <p><code>(parameters)</code></p> </li> </ul>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.posterior_variance","title":"posterior_variance","text":"<pre><code>posterior_variance: Tensor\n</code></pre> <p>Diagonal posterior variance \\(p^{-1}\\).</p> <p>Returns:</p> <ul> <li> <code>precision</code> (              <code>tensor</code> )          \u2013            <p><code>(parameters)</code></p> </li> </ul>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.fit","title":"fit","text":"<pre><code>fit(train_loader: DataLoader, override: bool = True, progress_bar: bool = False) -&gt; None\n</code></pre> <p>Fit the local Laplace approximation at the parameters of the model.</p> <p>Parameters:</p> Source code in <code>laplace/baselaplace.py</code> <pre><code>def fit(\n    self,\n    train_loader: DataLoader,\n    override: bool = True,\n    progress_bar: bool = False,\n) -&gt; None:\n    \"\"\"Fit the local Laplace approximation at the parameters of the model.\n\n    Parameters\n    ----------\n    train_loader : torch.data.utils.DataLoader\n        each iterate is a training batch, either `(X, y)` tensors or a dict-like\n        object containing keys as expressed by `self.dict_key_x` and\n        `self.dict_key_y`. `train_loader.dataset` needs to be set to access\n        \\\\(N\\\\), size of the data set.\n    override : bool, default=True\n        whether to initialize H, loss, and n_data again; setting to False is useful for\n        online learning settings to accumulate a sequential posterior approximation.\n    progress_bar : bool, default=False\n        whether to show a progress bar; updated at every batch-Hessian computation.\n        Useful for very large model and large amount of data, esp. when `subset_of_weights='all'`.\n    \"\"\"\n    if override:\n        self._init_H()\n        self.loss: float | torch.Tensor = 0\n        self.n_data: int = 0\n\n    self.model.eval()\n\n    self.mean: torch.Tensor = parameters_to_vector(self.params)\n    if not self.enable_backprop:\n        self.mean = self.mean.detach()\n\n    data: (\n        tuple[torch.Tensor, torch.Tensor] | MutableMapping[str, torch.Tensor | Any]\n    ) = next(iter(train_loader))\n\n    with torch.no_grad():\n        if isinstance(data, MutableMapping):  # To support Huggingface dataset\n            if \"backpack\" in self._backend_cls.__name__.lower() or (\n                isinstance(self, DiagLaplace) and self._backend_cls == CurvlinopsEF\n            ):\n                raise ValueError(\n                    \"Currently DiagEF is not supported under CurvlinopsEF backend \"\n                    + \"for custom models with non-tensor inputs \"\n                    + \"(https://github.com/pytorch/functorch/issues/159). Consider \"\n                    + \"using AsdlEF backend instead. The same limitation applies \"\n                    + \"to all BackPACK backend\"\n                )\n\n            out = self.model(data)\n        else:\n            X = data[0]\n            try:\n                out = self.model(X[:1].to(self._device))\n            except (TypeError, AttributeError):\n                out = self.model(X.to(self._device))\n    self.n_outputs = out.shape[-1]\n    setattr(self.model, \"output_size\", self.n_outputs)\n\n    N = len(train_loader.dataset)\n\n    pbar = tqdm.tqdm(train_loader, disable=not progress_bar)\n    pbar.set_description(\"[Computing Hessian]\")\n\n    for data in pbar:\n        if isinstance(data, MutableMapping):  # To support Huggingface dataset\n            X, y = data, data[self.dict_key_y].to(self._device)\n        else:\n            X, y = data\n            X, y = X.to(self._device), y.to(self._device)\n\n        if self.likelihood == Likelihood.REGRESSION and y.ndim != out.ndim:\n            raise ValueError(\n                f\"The model's output has {out.ndim} dims but \"\n                f\"the target has {y.ndim} dims.\"\n            )\n\n        self.model.zero_grad()\n        loss_batch, H_batch = self._curv_closure(X, y, N=N)\n        self.loss += loss_batch\n        self.H += H_batch\n\n    self.n_data += N\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.fit(train_loader)","title":"<code>train_loader</code>","text":"(<code>DataLoader</code>)           \u2013            <p>each iterate is a training batch, either <code>(X, y)</code> tensors or a dict-like object containing keys as expressed by <code>self.dict_key_x</code> and <code>self.dict_key_y</code>. <code>train_loader.dataset</code> needs to be set to access \\(N\\), size of the data set.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.fit(override)","title":"<code>override</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to initialize H, loss, and n_data again; setting to False is useful for online learning settings to accumulate a sequential posterior approximation.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.fit(progress_bar)","title":"<code>progress_bar</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to show a progress bar; updated at every batch-Hessian computation. Useful for very large model and large amount of data, esp. when <code>subset_of_weights='all'</code>.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.log_marginal_likelihood","title":"log_marginal_likelihood","text":"<pre><code>log_marginal_likelihood(prior_precision: Tensor | None = None, sigma_noise: Tensor | None = None) -&gt; Tensor\n</code></pre> <p>Compute the Laplace approximation to the log marginal likelihood subject to specific Hessian approximations that subclasses implement. Requires that the Laplace approximation has been fit before. The resulting torch.Tensor is differentiable in <code>prior_precision</code> and <code>sigma_noise</code> if these have gradients enabled. By passing <code>prior_precision</code> or <code>sigma_noise</code>, the current value is overwritten. This is useful for iterating on the log marginal likelihood.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>log_marglik</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def log_marginal_likelihood(\n    self,\n    prior_precision: torch.Tensor | None = None,\n    sigma_noise: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the Laplace approximation to the log marginal likelihood subject\n    to specific Hessian approximations that subclasses implement.\n    Requires that the Laplace approximation has been fit before.\n    The resulting torch.Tensor is differentiable in `prior_precision` and\n    `sigma_noise` if these have gradients enabled.\n    By passing `prior_precision` or `sigma_noise`, the current value is\n    overwritten. This is useful for iterating on the log marginal likelihood.\n\n    Parameters\n    ----------\n    prior_precision : torch.Tensor, optional\n        prior precision if should be changed from current `prior_precision` value\n    sigma_noise : torch.Tensor, optional\n        observation noise standard deviation if should be changed\n\n    Returns\n    -------\n    log_marglik : torch.Tensor\n    \"\"\"\n    # update prior precision (useful when iterating on marglik)\n    if prior_precision is not None:\n        self.prior_precision = prior_precision\n\n    # update sigma_noise (useful when iterating on marglik)\n    if sigma_noise is not None:\n        if self.likelihood != Likelihood.REGRESSION:\n            raise ValueError(\"Can only change sigma_noise for regression.\")\n\n        self.sigma_noise = sigma_noise\n\n    return self.log_likelihood - 0.5 * (self.log_det_ratio + self.scatter)\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.log_marginal_likelihood(prior_precision)","title":"<code>prior_precision</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>prior precision if should be changed from current <code>prior_precision</code> value</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.log_marginal_likelihood(sigma_noise)","title":"<code>sigma_noise</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>observation noise standard deviation if should be changed</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.__call__","title":"__call__","text":"<pre><code>__call__(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None, fitting: bool = False, **model_kwargs: dict[str, Any]) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def __call__(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n    fitting: bool = False,\n    **model_kwargs: dict[str, Any],\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x`.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here. When Laplace is done only\n        on subset of parameters (i.e. some grad are disabled),\n        only `nn` predictive is supported.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` when `joint=False` in regression.\n        In the case of last-layer Laplace with a diagonal or Kron Hessian,\n        setting this to `True` makes computation much(!) faster for large\n        number of outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used).\n\n    fitting : bool, default=False\n        whether or not this predictive call is done during fitting. Only useful for\n        reward modeling: the likelihood is set to `\"regression\"` when `False` and\n        `\"classification\"` when `True`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    if pred_type not in [pred for pred in PredType]:\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if link_approx not in [la for la in LinkApprox]:\n        raise ValueError(f\"Unsupported link approximation {link_approx}.\")\n\n    if pred_type == PredType.NN and link_approx != LinkApprox.MC:\n        raise ValueError(\n            \"Only mc link approximation is supported for nn prediction type.\"\n        )\n\n    if generator is not None:\n        if (\n            not isinstance(generator, torch.Generator)\n            or generator.device != self._device\n        ):\n            raise ValueError(\"Invalid random generator (check type and device).\")\n\n    likelihood = self.likelihood\n    if likelihood == Likelihood.REWARD_MODELING:\n        likelihood = Likelihood.CLASSIFICATION if fitting else Likelihood.REGRESSION\n\n    if pred_type == PredType.GLM:\n        return self._glm_forward_call(\n            x, likelihood, joint, link_approx, n_samples, diagonal_output\n        )\n    else:\n        if likelihood == Likelihood.REGRESSION:\n            samples = self._nn_predictive_samples(x, n_samples, **model_kwargs)\n            return samples.mean(dim=0), samples.var(dim=0)\n        else:  # classification; the average is computed online\n            return self._nn_predictive_classification(x, n_samples, **model_kwargs)\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.__call__(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.__call__(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here. When Laplace is done only on subset of parameters (i.e. some grad are disabled), only <code>nn</code> predictive is supported.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.__call__(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.__call__(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.__call__(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.__call__(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> when <code>joint=False</code> in regression. In the case of last-layer Laplace with a diagonal or Kron Hessian, setting this to <code>True</code> makes computation much(!) faster for large number of outputs.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.__call__(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used).</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.__call__(fitting)","title":"<code>fitting</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether or not this predictive call is done during fitting. Only useful for reward modeling: the likelihood is set to <code>\"regression\"</code> when <code>False</code> and <code>\"classification\"</code> when <code>True</code>.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace._glm_forward_call","title":"_glm_forward_call","text":"<pre><code>_glm_forward_call(x: Tensor | MutableMapping, likelihood: Likelihood | str, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code> for \"glm\" pred type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_forward_call(\n    self,\n    x: torch.Tensor | MutableMapping,\n    likelihood: Likelihood | str,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x` for \"glm\" pred type.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    likelihood : Likelihood or str in {'classification', 'regression', 'reward_modeling'}\n        determines the log likelihood Hessian approximation.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` and `link_approx='mc'`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    f_mu, f_var = self._glm_predictive_distribution(\n        x, joint=joint and likelihood == Likelihood.REGRESSION\n    )\n\n    if likelihood == Likelihood.REGRESSION:\n        if diagonal_output and not joint:\n            f_var = torch.diagonal(f_var, dim1=-2, dim2=-1)\n        return f_mu, f_var\n\n    if link_approx == LinkApprox.MC:\n        return self._glm_predictive_samples(\n            f_mu,\n            f_var,\n            n_samples=n_samples,\n            diagonal_output=diagonal_output,\n        ).mean(dim=0)\n    elif link_approx == LinkApprox.PROBIT:\n        kappa = 1 / torch.sqrt(1.0 + np.pi / 8 * f_var.diagonal(dim1=1, dim2=2))\n        return torch.softmax(kappa * f_mu, dim=-1)\n    elif \"bridge\" in link_approx:\n        # zero mean correction\n        f_mu -= (\n            f_var.sum(-1)\n            * f_mu.sum(-1).reshape(-1, 1)\n            / f_var.sum(dim=(1, 2)).reshape(-1, 1)\n        )\n        f_var -= torch.einsum(\n            \"bi,bj-&gt;bij\", f_var.sum(-1), f_var.sum(-2)\n        ) / f_var.sum(dim=(1, 2)).reshape(-1, 1, 1)\n\n        # Laplace Bridge\n        _, K = f_mu.size(0), f_mu.size(-1)\n        f_var_diag = torch.diagonal(f_var, dim1=1, dim2=2)\n\n        # optional: variance correction\n        if link_approx == LinkApprox.BRIDGE_NORM:\n            f_var_diag_mean = f_var_diag.mean(dim=1)\n            f_var_diag_mean /= torch.as_tensor(\n                [K / 2], device=self._device, dtype=self._dtype\n            ).sqrt()\n            f_mu /= f_var_diag_mean.sqrt().unsqueeze(-1)\n            f_var_diag /= f_var_diag_mean.unsqueeze(-1)\n\n        sum_exp = torch.exp(-f_mu).sum(dim=1).unsqueeze(-1)\n        alpha = (1 - 2 / K + f_mu.exp() / K**2 * sum_exp) / f_var_diag\n        return torch.nan_to_num(alpha / alpha.sum(dim=1).unsqueeze(-1), nan=1.0)\n    else:\n        raise ValueError(\n            \"Prediction path invalid. Check the likelihood, pred_type, link_approx combination!\"\n        )\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace._glm_forward_call(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace._glm_forward_call(likelihood)","title":"<code>likelihood</code>","text":"(<code>Likelihood or str in {'classification', 'regression', 'reward_modeling'}</code>)           \u2013            <p>determines the log likelihood Hessian approximation.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace._glm_forward_call(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace._glm_forward_call(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace._glm_forward_call(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace._glm_forward_call(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> and <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace._glm_functional_samples","title":"_glm_functional_samples","text":"<pre><code>_glm_functional_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior functional on input data <code>x</code> using \"glm\" prediction type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_functional_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior functional on input data `x` using \"glm\" prediction\n    type.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    assert f_var.shape == torch.Size([f_mu.shape[0], f_mu.shape[1], f_mu.shape[1]])\n\n    if diagonal_output:\n        f_var = torch.diagonal(f_var, dim1=1, dim2=2)\n\n    return normal_samples(f_mu, f_var, n_samples, generator)\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace._glm_functional_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace._glm_functional_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace._glm_functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace._glm_functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace._glm_functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace._glm_predictive_samples","title":"_glm_predictive_samples","text":"<pre><code>_glm_predictive_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code> using \"glm\" prediction type. I.e., the inverse-link function correponding to the likelihood is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_predictive_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x` using \"glm\" prediction\n    type. I.e., the inverse-link function correponding to the likelihood is applied\n    on top of the functional sample.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    f_samples = self._glm_functional_samples(\n        f_mu, f_var, n_samples, diagonal_output, generator\n    )\n\n    if self.likelihood == Likelihood.REGRESSION:\n        return f_samples\n    else:\n        return torch.softmax(f_samples, dim=-1)\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace._glm_predictive_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace._glm_predictive_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace._glm_predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace._glm_predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace._glm_predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.log_prob","title":"log_prob","text":"<pre><code>log_prob(value: Tensor, normalized: bool = True) -&gt; Tensor\n</code></pre> <p>Compute the log probability under the (current) Laplace approximation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>log_prob</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def log_prob(self, value: torch.Tensor, normalized: bool = True) -&gt; torch.Tensor:\n    \"\"\"Compute the log probability under the (current) Laplace approximation.\n\n    Parameters\n    ----------\n    value: torch.Tensor\n    normalized : bool, default=True\n        whether to return log of a properly normalized Gaussian or just the\n        terms that depend on `value`.\n\n    Returns\n    -------\n    log_prob : torch.Tensor\n    \"\"\"\n    if not normalized:\n        return -self.square_norm(value) / 2\n    log_prob = (\n        -self.n_params / 2 * log(2 * pi) + self.log_det_posterior_precision / 2\n    )\n    log_prob -= self.square_norm(value) / 2\n    return log_prob\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.log_prob(value)","title":"<code>value</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.log_prob(normalized)","title":"<code>normalized</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to return log of a properly normalized Gaussian or just the terms that depend on <code>value</code>.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.functional_samples","title":"functional_samples","text":"<pre><code>functional_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the function-space posterior on input data <code>x</code>. Can be used, for example, for Thompson sampling or to compute an arbitrary expectation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def functional_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the function-space posterior on input data `x`.\n    Can be used, for example, for Thompson sampling or to compute an arbitrary\n    expectation.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if pred_type == PredType.GLM:\n        f_mu, f_var = self._glm_predictive_distribution(x)\n        return self._glm_functional_samples(\n            f_mu, f_var, n_samples, diagonal_output, generator\n        )\n    else:  # 'nn'\n        return self._nn_functional_samples(x, n_samples, generator)\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.functional_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.functional_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.predictive_samples","title":"predictive_samples","text":"<pre><code>predictive_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code>. I.e., the respective inverse-link function (e.g. softmax) is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def predictive_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x`. I.e., the respective\n    inverse-link function (e.g. softmax) is applied on top of the functional\n    sample.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if pred_type == PredType.GLM:\n        f_mu, f_var = self._glm_predictive_distribution(x)\n        return self._glm_predictive_samples(\n            f_mu, f_var, n_samples, diagonal_output, generator\n        )\n    else:  # 'nn'\n        return self._nn_predictive_samples(x, n_samples, generator)\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.predictive_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.predictive_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace.predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.DiagSubnetLaplace._check_subnetwork_indices","title":"_check_subnetwork_indices","text":"<pre><code>_check_subnetwork_indices(subnetwork_indices: LongTensor | None) -&gt; None\n</code></pre> <p>Check that subnetwork indices are valid indices of the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>).</p> Source code in <code>laplace/subnetlaplace.py</code> <pre><code>def _check_subnetwork_indices(\n    self, subnetwork_indices: torch.LongTensor | None\n) -&gt; None:\n    \"\"\"Check that subnetwork indices are valid indices of the vectorized model parameters\n    (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`).\n    \"\"\"\n    if subnetwork_indices is None:\n        raise ValueError(\"Subnetwork indices cannot be None.\")\n    elif not (\n        isinstance(subnetwork_indices, torch.LongTensor)\n        and subnetwork_indices.numel() &gt; 0\n        and len(subnetwork_indices.shape) == 1\n    ):\n        raise ValueError(\n            \"Subnetwork indices must be non-empty 1-dimensional torch.LongTensor.\"\n        )\n    elif not (\n        len(subnetwork_indices[subnetwork_indices &lt; 0]) == 0\n        and len(subnetwork_indices[subnetwork_indices &gt;= self.n_params]) == 0\n    ):\n        raise ValueError(\n            f\"Subnetwork indices must lie between 0 and n_params={self.n_params}.\"\n        )\n    elif not (len(subnetwork_indices.unique()) == len(subnetwork_indices)):\n        raise ValueError(\"Subnetwork indices must not contain duplicate entries.\")\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace","title":"FullSubnetLaplace","text":"<pre><code>FullSubnetLaplace(model: Module, likelihood: Likelihood | str, subnetwork_indices: LongTensor, sigma_noise: float | Tensor = 1.0, prior_precision: float | Tensor = 1.0, prior_mean: float | Tensor = 0.0, temperature: float = 1.0, backend: Type[CurvatureInterface] | None = None, backend_kwargs: dict | None = None, asdl_fisher_kwargs: dict | None = None)\n</code></pre> <p>               Bases: <code>SubnetLaplace</code>, <code>FullLaplace</code></p> <p>Subnetwork Laplace approximation with full, i.e., dense, log likelihood Hessian approximation and hence posterior precision. Based on the chosen <code>backend</code> parameter, the full approximation can be, for example, a generalized Gauss-Newton matrix. Mathematically, we have \\(P \\in \\mathbb{R}^{P \\times P}\\). See <code>FullLaplace</code>, <code>SubnetLaplace</code>, and <code>BaseLaplace</code> for the full interface.</p> <p>Methods:</p> <ul> <li> <code>log_marginal_likelihood</code>             \u2013              <p>Compute the Laplace approximation to the log marginal likelihood subject</p> </li> <li> <code>__call__</code>             \u2013              <p>Compute the posterior predictive on input data <code>x</code>.</p> </li> <li> <code>log_prob</code>             \u2013              <p>Compute the log probability under the (current) Laplace approximation.</p> </li> <li> <code>functional_samples</code>             \u2013              <p>Sample from the function-space posterior on input data <code>x</code>.</p> </li> <li> <code>predictive_samples</code>             \u2013              <p>Sample from the posterior predictive on input data <code>x</code>. I.e., the respective</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>log_likelihood</code>               (<code>Tensor</code>)           \u2013            <p>Compute log likelihood on the training data after <code>.fit()</code> has been called.</p> </li> <li> <code>prior_precision_diag</code>               (<code>Tensor</code>)           \u2013            <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either</p> </li> <li> <code>log_det_prior_precision</code>               (<code>Tensor</code>)           \u2013            <p>Compute log determinant of the prior precision</p> </li> <li> <code>log_det_ratio</code>               (<code>Tensor</code>)           \u2013            <p>Compute the log determinant ratio, a part of the log marginal likelihood.</p> </li> <li> <code>posterior_precision</code>               (<code>Tensor</code>)           \u2013            <p>Posterior precision \\(P\\).</p> </li> <li> <code>posterior_scale</code>               (<code>Tensor</code>)           \u2013            <p>Posterior scale (square root of the covariance), i.e.,</p> </li> <li> <code>posterior_covariance</code>               (<code>Tensor</code>)           \u2013            <p>Posterior covariance, i.e., \\(P^{-1}\\).</p> </li> </ul> Source code in <code>laplace/subnetlaplace.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    likelihood: Likelihood | str,\n    subnetwork_indices: torch.LongTensor,\n    sigma_noise: float | torch.Tensor = 1.0,\n    prior_precision: float | torch.Tensor = 1.0,\n    prior_mean: float | torch.Tensor = 0.0,\n    temperature: float = 1.0,\n    backend: Type[CurvatureInterface] | None = None,\n    backend_kwargs: dict | None = None,\n    asdl_fisher_kwargs: dict | None = None,\n) -&gt; None:\n    if asdl_fisher_kwargs is not None:\n        raise ValueError(\"Subnetwork Laplace does not support asdl_fisher_kwargs.\")\n\n    self.H = None\n    super().__init__(\n        model,\n        likelihood,\n        sigma_noise=sigma_noise,\n        prior_precision=prior_precision,\n        prior_mean=prior_mean,\n        temperature=temperature,\n        backend=backend,\n        backend_kwargs=backend_kwargs,\n    )\n\n    if backend is not None and not issubclass(backend, (GGNInterface, EFInterface)):\n        raise ValueError(\"SubnetLaplace can only be used with GGN and EF.\")\n\n    # check validity of subnetwork indices and pass them to backend\n    self._check_subnetwork_indices(subnetwork_indices)\n    self.backend.subnetwork_indices = subnetwork_indices\n    self.n_params_subnet = len(subnetwork_indices)\n    self._init_H()\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.log_likelihood","title":"log_likelihood","text":"<pre><code>log_likelihood: Tensor\n</code></pre> <p>Compute log likelihood on the training data after <code>.fit()</code> has been called. The log likelihood is computed on-demand based on the loss and, for example, the observation noise which makes it differentiable in the latter for iterative updates.</p> <p>Returns:</p> <ul> <li> <code>log_likelihood</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.prior_precision_diag","title":"prior_precision_diag","text":"<pre><code>prior_precision_diag: Tensor\n</code></pre> <p>Obtain the diagonal prior precision \\(p_0\\) constructed from either a scalar or diagonal prior precision.</p> <p>Returns:</p> <ul> <li> <code>prior_precision_diag</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.log_det_prior_precision","title":"log_det_prior_precision","text":"<pre><code>log_det_prior_precision: Tensor\n</code></pre> <p>Compute log determinant of the prior precision \\(\\log \\det P_0\\)</p> <p>Returns:</p> <ul> <li> <code>log_det</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.log_det_ratio","title":"log_det_ratio","text":"<pre><code>log_det_ratio: Tensor\n</code></pre> <p>Compute the log determinant ratio, a part of the log marginal likelihood.</p> \\[     \\log \\frac{\\det P}{\\det P_0} = \\log \\det P - \\log \\det P_0 \\] <p>Returns:</p> <ul> <li> <code>log_det_ratio</code> (              <code>Tensor</code> )          \u2013            </li> </ul>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.posterior_precision","title":"posterior_precision","text":"<pre><code>posterior_precision: Tensor\n</code></pre> <p>Posterior precision \\(P\\).</p> <p>Returns:</p> <ul> <li> <code>precision</code> (              <code>tensor</code> )          \u2013            <p><code>(parameters, parameters)</code></p> </li> </ul>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.posterior_scale","title":"posterior_scale","text":"<pre><code>posterior_scale: Tensor\n</code></pre> <p>Posterior scale (square root of the covariance), i.e., \\(P^{-\\frac{1}{2}}\\).</p> <p>Returns:</p> <ul> <li> <code>scale</code> (              <code>tensor</code> )          \u2013            <p><code>(parameters, parameters)</code></p> </li> </ul>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.posterior_covariance","title":"posterior_covariance","text":"<pre><code>posterior_covariance: Tensor\n</code></pre> <p>Posterior covariance, i.e., \\(P^{-1}\\).</p> <p>Returns:</p> <ul> <li> <code>covariance</code> (              <code>tensor</code> )          \u2013            <p><code>(parameters, parameters)</code></p> </li> </ul>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.log_marginal_likelihood","title":"log_marginal_likelihood","text":"<pre><code>log_marginal_likelihood(prior_precision: Tensor | None = None, sigma_noise: Tensor | None = None) -&gt; Tensor\n</code></pre> <p>Compute the Laplace approximation to the log marginal likelihood subject to specific Hessian approximations that subclasses implement. Requires that the Laplace approximation has been fit before. The resulting torch.Tensor is differentiable in <code>prior_precision</code> and <code>sigma_noise</code> if these have gradients enabled. By passing <code>prior_precision</code> or <code>sigma_noise</code>, the current value is overwritten. This is useful for iterating on the log marginal likelihood.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>log_marglik</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def log_marginal_likelihood(\n    self,\n    prior_precision: torch.Tensor | None = None,\n    sigma_noise: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the Laplace approximation to the log marginal likelihood subject\n    to specific Hessian approximations that subclasses implement.\n    Requires that the Laplace approximation has been fit before.\n    The resulting torch.Tensor is differentiable in `prior_precision` and\n    `sigma_noise` if these have gradients enabled.\n    By passing `prior_precision` or `sigma_noise`, the current value is\n    overwritten. This is useful for iterating on the log marginal likelihood.\n\n    Parameters\n    ----------\n    prior_precision : torch.Tensor, optional\n        prior precision if should be changed from current `prior_precision` value\n    sigma_noise : torch.Tensor, optional\n        observation noise standard deviation if should be changed\n\n    Returns\n    -------\n    log_marglik : torch.Tensor\n    \"\"\"\n    # update prior precision (useful when iterating on marglik)\n    if prior_precision is not None:\n        self.prior_precision = prior_precision\n\n    # update sigma_noise (useful when iterating on marglik)\n    if sigma_noise is not None:\n        if self.likelihood != Likelihood.REGRESSION:\n            raise ValueError(\"Can only change sigma_noise for regression.\")\n\n        self.sigma_noise = sigma_noise\n\n    return self.log_likelihood - 0.5 * (self.log_det_ratio + self.scatter)\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.log_marginal_likelihood(prior_precision)","title":"<code>prior_precision</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>prior precision if should be changed from current <code>prior_precision</code> value</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.log_marginal_likelihood(sigma_noise)","title":"<code>sigma_noise</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>observation noise standard deviation if should be changed</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.__call__","title":"__call__","text":"<pre><code>__call__(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None, fitting: bool = False, **model_kwargs: dict[str, Any]) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def __call__(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n    fitting: bool = False,\n    **model_kwargs: dict[str, Any],\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x`.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here. When Laplace is done only\n        on subset of parameters (i.e. some grad are disabled),\n        only `nn` predictive is supported.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` when `joint=False` in regression.\n        In the case of last-layer Laplace with a diagonal or Kron Hessian,\n        setting this to `True` makes computation much(!) faster for large\n        number of outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used).\n\n    fitting : bool, default=False\n        whether or not this predictive call is done during fitting. Only useful for\n        reward modeling: the likelihood is set to `\"regression\"` when `False` and\n        `\"classification\"` when `True`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    if pred_type not in [pred for pred in PredType]:\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if link_approx not in [la for la in LinkApprox]:\n        raise ValueError(f\"Unsupported link approximation {link_approx}.\")\n\n    if pred_type == PredType.NN and link_approx != LinkApprox.MC:\n        raise ValueError(\n            \"Only mc link approximation is supported for nn prediction type.\"\n        )\n\n    if generator is not None:\n        if (\n            not isinstance(generator, torch.Generator)\n            or generator.device != self._device\n        ):\n            raise ValueError(\"Invalid random generator (check type and device).\")\n\n    likelihood = self.likelihood\n    if likelihood == Likelihood.REWARD_MODELING:\n        likelihood = Likelihood.CLASSIFICATION if fitting else Likelihood.REGRESSION\n\n    if pred_type == PredType.GLM:\n        return self._glm_forward_call(\n            x, likelihood, joint, link_approx, n_samples, diagonal_output\n        )\n    else:\n        if likelihood == Likelihood.REGRESSION:\n            samples = self._nn_predictive_samples(x, n_samples, **model_kwargs)\n            return samples.mean(dim=0), samples.var(dim=0)\n        else:  # classification; the average is computed online\n            return self._nn_predictive_classification(x, n_samples, **model_kwargs)\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.__call__(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.__call__(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here. When Laplace is done only on subset of parameters (i.e. some grad are disabled), only <code>nn</code> predictive is supported.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.__call__(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.__call__(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.__call__(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.__call__(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> when <code>joint=False</code> in regression. In the case of last-layer Laplace with a diagonal or Kron Hessian, setting this to <code>True</code> makes computation much(!) faster for large number of outputs.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.__call__(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used).</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.__call__(fitting)","title":"<code>fitting</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether or not this predictive call is done during fitting. Only useful for reward modeling: the likelihood is set to <code>\"regression\"</code> when <code>False</code> and <code>\"classification\"</code> when <code>True</code>.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace._glm_forward_call","title":"_glm_forward_call","text":"<pre><code>_glm_forward_call(x: Tensor | MutableMapping, likelihood: Likelihood | str, joint: bool = False, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, diagonal_output: bool = False) -&gt; Tensor | tuple[Tensor, Tensor]\n</code></pre> <p>Compute the posterior predictive on input data <code>x</code> for \"glm\" pred type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>predictive</code> (              <code>Tensor or tuple[Tensor]</code> )          \u2013            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with a distribution over classes (similar to a Softmax). For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned with the mean and the predictive variance. For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor is returned with the mean and the predictive covariance.</p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_forward_call(\n    self,\n    x: torch.Tensor | MutableMapping,\n    likelihood: Likelihood | str,\n    joint: bool = False,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n) -&gt; torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute the posterior predictive on input data `x` for \"glm\" pred type.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain\n        the said tensor.\n\n    likelihood : Likelihood or str in {'classification', 'regression', 'reward_modeling'}\n        determines the log likelihood Hessian approximation.\n\n    link_approx : {'mc', 'probit', 'bridge', 'bridge_norm'}\n        how to approximate the classification link function for the `'glm'`.\n        For `pred_type='nn'`, only 'mc' is possible.\n\n    joint : bool\n        Whether to output a joint predictive distribution in regression with\n        `pred_type='glm'`. If set to `True`, the predictive distribution\n        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).\n        If `False`, then only outputs the marginal predictive distribution.\n        Only available for regression and GLM predictive.\n\n    n_samples : int\n        number of samples for `link_approx='mc'`.\n\n    diagonal_output : bool\n        whether to use a diagonalized posterior predictive on the outputs.\n        Only works for `pred_type='glm'` and `link_approx='mc'`.\n\n    Returns\n    -------\n    predictive: torch.Tensor or tuple[torch.Tensor]\n        For `likelihood='classification'`, a torch.Tensor is returned with\n        a distribution over classes (similar to a Softmax).\n        For `likelihood='regression'`, a tuple of torch.Tensor is returned\n        with the mean and the predictive variance.\n        For `likelihood='regression'` and `joint=True`, a tuple of torch.Tensor\n        is returned with the mean and the predictive covariance.\n    \"\"\"\n    f_mu, f_var = self._glm_predictive_distribution(\n        x, joint=joint and likelihood == Likelihood.REGRESSION\n    )\n\n    if likelihood == Likelihood.REGRESSION:\n        if diagonal_output and not joint:\n            f_var = torch.diagonal(f_var, dim1=-2, dim2=-1)\n        return f_mu, f_var\n\n    if link_approx == LinkApprox.MC:\n        return self._glm_predictive_samples(\n            f_mu,\n            f_var,\n            n_samples=n_samples,\n            diagonal_output=diagonal_output,\n        ).mean(dim=0)\n    elif link_approx == LinkApprox.PROBIT:\n        kappa = 1 / torch.sqrt(1.0 + np.pi / 8 * f_var.diagonal(dim1=1, dim2=2))\n        return torch.softmax(kappa * f_mu, dim=-1)\n    elif \"bridge\" in link_approx:\n        # zero mean correction\n        f_mu -= (\n            f_var.sum(-1)\n            * f_mu.sum(-1).reshape(-1, 1)\n            / f_var.sum(dim=(1, 2)).reshape(-1, 1)\n        )\n        f_var -= torch.einsum(\n            \"bi,bj-&gt;bij\", f_var.sum(-1), f_var.sum(-2)\n        ) / f_var.sum(dim=(1, 2)).reshape(-1, 1, 1)\n\n        # Laplace Bridge\n        _, K = f_mu.size(0), f_mu.size(-1)\n        f_var_diag = torch.diagonal(f_var, dim1=1, dim2=2)\n\n        # optional: variance correction\n        if link_approx == LinkApprox.BRIDGE_NORM:\n            f_var_diag_mean = f_var_diag.mean(dim=1)\n            f_var_diag_mean /= torch.as_tensor(\n                [K / 2], device=self._device, dtype=self._dtype\n            ).sqrt()\n            f_mu /= f_var_diag_mean.sqrt().unsqueeze(-1)\n            f_var_diag /= f_var_diag_mean.unsqueeze(-1)\n\n        sum_exp = torch.exp(-f_mu).sum(dim=1).unsqueeze(-1)\n        alpha = (1 - 2 / K + f_mu.exp() / K**2 * sum_exp) / f_var_diag\n        return torch.nan_to_num(alpha / alpha.sum(dim=1).unsqueeze(-1), nan=1.0)\n    else:\n        raise ValueError(\n            \"Prediction path invalid. Check the likelihood, pred_type, link_approx combination!\"\n        )\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace._glm_forward_call(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain the said tensor.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace._glm_forward_call(likelihood)","title":"<code>likelihood</code>","text":"(<code>Likelihood or str in {'classification', 'regression', 'reward_modeling'}</code>)           \u2013            <p>determines the log likelihood Hessian approximation.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace._glm_forward_call(link_approx)","title":"<code>link_approx</code>","text":"(<code>('mc', 'probit', 'bridge', 'bridge_norm')</code>, default:                   <code>'mc'</code> )           \u2013            <p>how to approximate the classification link function for the <code>'glm'</code>. For <code>pred_type='nn'</code>, only 'mc' is possible.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace._glm_forward_call(joint)","title":"<code>joint</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to output a joint predictive distribution in regression with <code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]). If <code>False</code>, then only outputs the marginal predictive distribution. Only available for regression and GLM predictive.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace._glm_forward_call(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples for <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace._glm_forward_call(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized posterior predictive on the outputs. Only works for <code>pred_type='glm'</code> and <code>link_approx='mc'</code>.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace._glm_functional_samples","title":"_glm_functional_samples","text":"<pre><code>_glm_functional_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior functional on input data <code>x</code> using \"glm\" prediction type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_functional_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior functional on input data `x` using \"glm\" prediction\n    type.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    assert f_var.shape == torch.Size([f_mu.shape[0], f_mu.shape[1], f_mu.shape[1]])\n\n    if diagonal_output:\n        f_var = torch.diagonal(f_var, dim1=1, dim2=2)\n\n    return normal_samples(f_mu, f_var, n_samples, generator)\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace._glm_functional_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace._glm_functional_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace._glm_functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace._glm_functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace._glm_functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace._glm_predictive_samples","title":"_glm_predictive_samples","text":"<pre><code>_glm_predictive_samples(f_mu: Tensor, f_var: Tensor, n_samples: int, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code> using \"glm\" prediction type. I.e., the inverse-link function correponding to the likelihood is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def _glm_predictive_samples(\n    self,\n    f_mu: torch.Tensor,\n    f_var: torch.Tensor,\n    n_samples: int,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x` using \"glm\" prediction\n    type. I.e., the inverse-link function correponding to the likelihood is applied\n    on top of the functional sample.\n\n    Parameters\n    ----------\n    f_mu : torch.Tensor or MutableMapping\n        glm predictive mean `(batch_size, output_shape)`\n\n    f_var : torch.Tensor or MutableMapping\n        glm predictive covariances `(batch_size, output_shape, output_shape)`\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    f_samples = self._glm_functional_samples(\n        f_mu, f_var, n_samples, diagonal_output, generator\n    )\n\n    if self.likelihood == Likelihood.REGRESSION:\n        return f_samples\n    else:\n        return torch.softmax(f_samples, dim=-1)\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace._glm_predictive_samples(f_mu)","title":"<code>f_mu</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace._glm_predictive_samples(f_var)","title":"<code>f_var</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace._glm_predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>)           \u2013            <p>number of samples</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace._glm_predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace._glm_predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.log_prob","title":"log_prob","text":"<pre><code>log_prob(value: Tensor, normalized: bool = True) -&gt; Tensor\n</code></pre> <p>Compute the log probability under the (current) Laplace approximation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>log_prob</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def log_prob(self, value: torch.Tensor, normalized: bool = True) -&gt; torch.Tensor:\n    \"\"\"Compute the log probability under the (current) Laplace approximation.\n\n    Parameters\n    ----------\n    value: torch.Tensor\n    normalized : bool, default=True\n        whether to return log of a properly normalized Gaussian or just the\n        terms that depend on `value`.\n\n    Returns\n    -------\n    log_prob : torch.Tensor\n    \"\"\"\n    if not normalized:\n        return -self.square_norm(value) / 2\n    log_prob = (\n        -self.n_params / 2 * log(2 * pi) + self.log_det_posterior_precision / 2\n    )\n    log_prob -= self.square_norm(value) / 2\n    return log_prob\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.log_prob(value)","title":"<code>value</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.log_prob(normalized)","title":"<code>normalized</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to return log of a properly normalized Gaussian or just the terms that depend on <code>value</code>.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.functional_samples","title":"functional_samples","text":"<pre><code>functional_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the function-space posterior on input data <code>x</code>. Can be used, for example, for Thompson sampling or to compute an arbitrary expectation.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def functional_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the function-space posterior on input data `x`.\n    Can be used, for example, for Thompson sampling or to compute an arbitrary\n    expectation.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if pred_type == PredType.GLM:\n        f_mu, f_var = self._glm_predictive_distribution(x)\n        return self._glm_functional_samples(\n            f_mu, f_var, n_samples, diagonal_output, generator\n        )\n    else:  # 'nn'\n        return self._nn_functional_samples(x, n_samples, generator)\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.functional_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.functional_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.functional_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.functional_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.functional_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.predictive_samples","title":"predictive_samples","text":"<pre><code>predictive_samples(x: Tensor | MutableMapping[str, Tensor | Any], pred_type: PredType | str = GLM, n_samples: int = 100, diagonal_output: bool = False, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Sample from the posterior predictive on input data <code>x</code>. I.e., the respective inverse-link function (e.g. softmax) is applied on top of the functional sample.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>Tensor</code> )          \u2013            <p>samples <code>(n_samples, batch_size, output_shape)</code></p> </li> </ul> Source code in <code>laplace/baselaplace.py</code> <pre><code>def predictive_samples(\n    self,\n    x: torch.Tensor | MutableMapping[str, torch.Tensor | Any],\n    pred_type: PredType | str = PredType.GLM,\n    n_samples: int = 100,\n    diagonal_output: bool = False,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Sample from the posterior predictive on input data `x`. I.e., the respective\n    inverse-link function (e.g. softmax) is applied on top of the functional\n    sample.\n\n    Parameters\n    ----------\n    x : torch.Tensor or MutableMapping\n        input data `(batch_size, input_shape)`\n\n    pred_type : {'glm', 'nn'}, default='glm'\n        type of posterior predictive, linearized GLM predictive or neural\n        network sampling predictive. The GLM predictive is consistent with\n        the curvature approximations used here.\n\n    n_samples : int\n        number of samples\n\n    diagonal_output : bool\n        whether to use a diagonalized glm posterior predictive on the outputs.\n        Only applies when `pred_type='glm'`.\n\n    generator : torch.Generator, optional\n        random number generator to control the samples (if sampling used)\n\n    Returns\n    -------\n    samples : torch.Tensor\n        samples `(n_samples, batch_size, output_shape)`\n    \"\"\"\n    if pred_type not in PredType.__members__.values():\n        raise ValueError(\"Only glm and nn supported as prediction types.\")\n\n    if pred_type == PredType.GLM:\n        f_mu, f_var = self._glm_predictive_distribution(x)\n        return self._glm_predictive_samples(\n            f_mu, f_var, n_samples, diagonal_output, generator\n        )\n    else:  # 'nn'\n        return self._nn_predictive_samples(x, n_samples, generator)\n</code></pre>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.predictive_samples(x)","title":"<code>x</code>","text":"(<code>Tensor or MutableMapping</code>)           \u2013            <p>input data <code>(batch_size, input_shape)</code></p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.predictive_samples(pred_type)","title":"<code>pred_type</code>","text":"(<code>('glm', 'nn')</code>, default:                   <code>'glm'</code> )           \u2013            <p>type of posterior predictive, linearized GLM predictive or neural network sampling predictive. The GLM predictive is consistent with the curvature approximations used here.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.predictive_samples(n_samples)","title":"<code>n_samples</code>","text":"(<code>int</code>, default:                   <code>100</code> )           \u2013            <p>number of samples</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.predictive_samples(diagonal_output)","title":"<code>diagonal_output</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to use a diagonalized glm posterior predictive on the outputs. Only applies when <code>pred_type='glm'</code>.</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace.predictive_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator to control the samples (if sampling used)</p>"},{"location":"api_reference/subnetlaplace/#laplace.subnetlaplace.FullSubnetLaplace._check_subnetwork_indices","title":"_check_subnetwork_indices","text":"<pre><code>_check_subnetwork_indices(subnetwork_indices: LongTensor | None) -&gt; None\n</code></pre> <p>Check that subnetwork indices are valid indices of the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>).</p> Source code in <code>laplace/subnetlaplace.py</code> <pre><code>def _check_subnetwork_indices(\n    self, subnetwork_indices: torch.LongTensor | None\n) -&gt; None:\n    \"\"\"Check that subnetwork indices are valid indices of the vectorized model parameters\n    (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`).\n    \"\"\"\n    if subnetwork_indices is None:\n        raise ValueError(\"Subnetwork indices cannot be None.\")\n    elif not (\n        isinstance(subnetwork_indices, torch.LongTensor)\n        and subnetwork_indices.numel() &gt; 0\n        and len(subnetwork_indices.shape) == 1\n    ):\n        raise ValueError(\n            \"Subnetwork indices must be non-empty 1-dimensional torch.LongTensor.\"\n        )\n    elif not (\n        len(subnetwork_indices[subnetwork_indices &lt; 0]) == 0\n        and len(subnetwork_indices[subnetwork_indices &gt;= self.n_params]) == 0\n    ):\n        raise ValueError(\n            f\"Subnetwork indices must lie between 0 and n_params={self.n_params}.\"\n        )\n    elif not (len(subnetwork_indices.unique()) == len(subnetwork_indices)):\n        raise ValueError(\"Subnetwork indices must not contain duplicate entries.\")\n</code></pre>"},{"location":"api_reference/utils/","title":"Utilities","text":""},{"location":"api_reference/utils/#laplace.utils","title":"laplace.utils","text":"<p>Classes:</p> <ul> <li> <code>SoDSampler</code>           \u2013            </li> <li> <code>FeatureExtractor</code>           \u2013            <p>Feature extractor for a PyTorch neural network.</p> </li> <li> <code>Kron</code>           \u2013            <p>Kronecker factored approximate curvature representation for a corresponding</p> </li> <li> <code>KronDecomposed</code>           \u2013            <p>Decomposed Kronecker factored approximate curvature representation</p> </li> <li> <code>SubnetMask</code>           \u2013            <p>Baseclass for all subnetwork masks in this library (for subnetwork Laplace).</p> </li> <li> <code>RandomSubnetMask</code>           \u2013            <p>Subnetwork mask of parameters sampled uniformly at random.</p> </li> <li> <code>LargestMagnitudeSubnetMask</code>           \u2013            <p>Subnetwork mask identifying the parameters with the largest magnitude.</p> </li> <li> <code>LargestVarianceDiagLaplaceSubnetMask</code>           \u2013            <p>Subnetwork mask identifying the parameters with the largest marginal variances</p> </li> <li> <code>LargestVarianceSWAGSubnetMask</code>           \u2013            <p>Subnetwork mask identifying the parameters with the largest marginal variances</p> </li> <li> <code>ParamNameSubnetMask</code>           \u2013            <p>Subnetwork mask corresponding to the specified parameters of the neural network.</p> </li> <li> <code>ModuleNameSubnetMask</code>           \u2013            <p>Subnetwork mask corresponding to the specified modules of the neural network.</p> </li> <li> <code>LastLayerSubnetMask</code>           \u2013            <p>Subnetwork mask corresponding to the last layer of the neural network.</p> </li> <li> <code>RunningNLLMetric</code>           \u2013            <p>NLL metrics that</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>get_nll</code>             \u2013              </li> <li> <code>validate</code>             \u2013              </li> <li> <code>parameters_per_layer</code>             \u2013              <p>Get number of parameters per layer.</p> </li> <li> <code>invsqrt_precision</code>             \u2013              <p>Compute <code>M^{-0.5}</code> as a tridiagonal matrix.</p> </li> <li> <code>kron</code>             \u2013              <p>Computes the Kronecker product between two tensors.</p> </li> <li> <code>diagonal_add_scalar</code>             \u2013              <p>Add scalar value <code>value</code> to diagonal of <code>X</code>.</p> </li> <li> <code>symeig</code>             \u2013              <p>Symetric eigendecomposition avoiding failure cases by</p> </li> <li> <code>block_diag</code>             \u2013              <p>Compose block-diagonal matrix of individual blocks.</p> </li> <li> <code>normal_samples</code>             \u2013              <p>Produce samples from a batch of Normal distributions either parameterized</p> </li> <li> <code>_is_batchnorm</code>             \u2013              </li> <li> <code>_is_valid_scalar</code>             \u2013              </li> <li> <code>expand_prior_precision</code>             \u2013              <p>Expand prior precision to match the shape of the model parameters.</p> </li> <li> <code>fix_prior_prec_structure</code>             \u2013              <p>Create a tensor of prior precision with the correct shape, depending on the</p> </li> <li> <code>fit_diagonal_swag_var</code>             \u2013              <p>Fit diagonal SWAG [1], which estimates marginal variances of model parameters by</p> </li> </ul>"},{"location":"api_reference/utils/#laplace.utils.SoDSampler","title":"SoDSampler","text":"<pre><code>SoDSampler(N, M, seed: int = 0)\n</code></pre> <p>               Bases: <code>Sampler</code></p> Source code in <code>laplace/utils/utils.py</code> <pre><code>def __init__(self, N, M, seed: int = 0):\n    rng = np.random.default_rng(seed)\n    self.indices = torch.tensor(rng.choice(list(range(N)), M, replace=False))\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.FeatureExtractor","title":"FeatureExtractor","text":"<pre><code>FeatureExtractor(model: Module, last_layer_name: str | None = None, enable_backprop: bool = False, feature_reduction: FeatureReduction | str | None = None)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Feature extractor for a PyTorch neural network. A wrapper which can return the output of the penultimate layer in addition to the output of the last layer for each forward pass. If the name of the last layer is not known, it can determine it automatically. It assumes that the last layer is linear and that for every forward pass the last layer is the same. If the name of the last layer is known, it can be passed as a parameter at initilization; this is the safest way to use this class. Based on https://gist.github.com/fkodom/27ed045c9051a39102e8bcf4ce31df76.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>forward</code>             \u2013              <p>Forward pass. If the last layer is not known yet, it will be</p> </li> <li> <code>forward_with_features</code>             \u2013              <p>Forward pass which returns the output of the penultimate layer along</p> </li> <li> <code>set_last_layer</code>             \u2013              <p>Set the last layer of the model by its name. This sets the forward</p> </li> <li> <code>find_last_layer</code>             \u2013              <p>Automatically determines the last layer of the model with one</p> </li> </ul> Source code in <code>laplace/utils/feature_extractor.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    last_layer_name: str | None = None,\n    enable_backprop: bool = False,\n    feature_reduction: FeatureReduction | str | None = None,\n) -&gt; None:\n    if feature_reduction is not None and feature_reduction not in [\n        fr.value for fr in FeatureReduction\n    ]:\n        raise ValueError(\n            \"`feature_reduction` must take value in the `FeatureReduction enum` or \"\n            \"one of `{'pick_first', 'pick_last', 'average'}`!\"\n        )\n\n    super().__init__()\n    self.model: nn.Module = model\n    self._features: dict[str, torch.Tensor] = dict()\n    self.enable_backprop: bool = enable_backprop\n    self.feature_reduction: FeatureReduction | None = feature_reduction\n\n    self.last_layer: nn.Module | None\n    if last_layer_name is None:\n        self.last_layer = None\n    else:\n        self.set_last_layer(last_layer_name)\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.FeatureExtractor(model)","title":"<code>model</code>","text":"(<code>Module</code>)           \u2013            <p>PyTorch model</p>"},{"location":"api_reference/utils/#laplace.utils.FeatureExtractor(last_layer_name)","title":"<code>last_layer_name</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>if the name of the last layer is already known, otherwise it will be determined automatically.</p>"},{"location":"api_reference/utils/#laplace.utils.FeatureExtractor(enable_backprop)","title":"<code>enable_backprop</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to enable backprop through the feature extactor to get the gradients of the inputs. Useful for e.g. Bayesian optimization.</p>"},{"location":"api_reference/utils/#laplace.utils.FeatureExtractor(feature_reduction)","title":"<code>feature_reduction</code>","text":"(<code>FeatureReduction | str | None</code>, default:                   <code>None</code> )           \u2013            <p>when the last-layer <code>features</code> is a tensor of dim &gt;= 3, this tells how to reduce it into a dim-2 tensor. E.g. in LLMs for non-language modeling problems, the penultultimate output is a tensor of shape <code>(batch_size, seq_len, embd_dim)</code>. But the last layer maps <code>(batch_size, embd_dim)</code> to <code>(batch_size, n_classes)</code>. Note: Make sure that this option faithfully reflects the reduction in the model definition. When inputting a string, available options are <code>{'pick_first', 'pick_last', 'average'}</code>.</p>"},{"location":"api_reference/utils/#laplace.utils.FeatureExtractor.forward","title":"forward","text":"<pre><code>forward(x: Tensor | MutableMapping[str, Tensor | Any]) -&gt; Tensor\n</code></pre> <p>Forward pass. If the last layer is not known yet, it will be determined when this function is called for the first time.</p> <p>Parameters:</p> Source code in <code>laplace/utils/feature_extractor.py</code> <pre><code>def forward(\n    self, x: torch.Tensor | MutableMapping[str, torch.Tensor | Any]\n) -&gt; torch.Tensor:\n    \"\"\"Forward pass. If the last layer is not known yet, it will be\n    determined when this function is called for the first time.\n\n    Parameters\n    ----------\n    x : torch.Tensor or a dict-like object containing the input tensors\n        one batch of data to use as input for the forward pass\n    \"\"\"\n    if self.last_layer is None:\n        # if this is the first forward pass and last layer is unknown\n        out = self.find_last_layer(x)\n    else:\n        # if last and penultimate layers are already known\n        out = self.model(x)\n    return out\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.FeatureExtractor.forward(x)","title":"<code>x</code>","text":"(<code>torch.Tensor or a dict-like object containing the input tensors</code>)           \u2013            <p>one batch of data to use as input for the forward pass</p>"},{"location":"api_reference/utils/#laplace.utils.FeatureExtractor.forward_with_features","title":"forward_with_features","text":"<pre><code>forward_with_features(x: Tensor | MutableMapping[str, Tensor | Any]) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Forward pass which returns the output of the penultimate layer along with the output of the last layer. If the last layer is not known yet, it will be determined when this function is called for the first time.</p> <p>Parameters:</p> Source code in <code>laplace/utils/feature_extractor.py</code> <pre><code>def forward_with_features(\n    self, x: torch.Tensor | MutableMapping[str, torch.Tensor | Any]\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Forward pass which returns the output of the penultimate layer along\n    with the output of the last layer. If the last layer is not known yet,\n    it will be determined when this function is called for the first time.\n\n    Parameters\n    ----------\n    x : torch.Tensor or a dict-like object containing the input tensors\n        one batch of data to use as input for the forward pass\n    \"\"\"\n    out = self.forward(x)\n    features = self._features[self._last_layer_name]\n\n    if features.dim() &gt; 2 and self.feature_reduction is not None:\n        n_intermediate_dims = len(features.shape) - 2\n\n        if self.feature_reduction == FeatureReduction.PICK_FIRST:\n            features = features[\n                (slice(None), *([0] * n_intermediate_dims), slice(None))\n            ].squeeze()\n        elif self.feature_reduction == FeatureReduction.PICK_LAST:\n            features = features[\n                (slice(None), *([-1] * n_intermediate_dims), slice(None))\n            ].squeeze()\n        else:\n            ndim = features.ndim\n            features = features.mean(\n                dim=tuple(d for d in range(ndim) if d not in [0, ndim - 1])\n            ).squeeze()\n\n    return out, features\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.FeatureExtractor.forward_with_features(x)","title":"<code>x</code>","text":"(<code>torch.Tensor or a dict-like object containing the input tensors</code>)           \u2013            <p>one batch of data to use as input for the forward pass</p>"},{"location":"api_reference/utils/#laplace.utils.FeatureExtractor.set_last_layer","title":"set_last_layer","text":"<pre><code>set_last_layer(last_layer_name: str) -&gt; None\n</code></pre> <p>Set the last layer of the model by its name. This sets the forward hook to get the output of the penultimate layer.</p> <p>Parameters:</p> Source code in <code>laplace/utils/feature_extractor.py</code> <pre><code>def set_last_layer(self, last_layer_name: str) -&gt; None:\n    \"\"\"Set the last layer of the model by its name. This sets the forward\n    hook to get the output of the penultimate layer.\n\n    Parameters\n    ----------\n    last_layer_name : str\n        the name of the last layer (fixed in `model.named_modules()`).\n    \"\"\"\n    # set last_layer attributes and check if it is linear\n    self._last_layer_name = last_layer_name\n    self.last_layer = dict(self.model.named_modules())[last_layer_name]\n    if not isinstance(self.last_layer, nn.Linear):\n        raise ValueError(\"Use model with a linear last layer.\")\n\n    # set forward hook to extract features in future forward passes\n    self.last_layer.register_forward_hook(self._get_hook(last_layer_name))\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.FeatureExtractor.set_last_layer(last_layer_name)","title":"<code>last_layer_name</code>","text":"(<code>str</code>)           \u2013            <p>the name of the last layer (fixed in <code>model.named_modules()</code>).</p>"},{"location":"api_reference/utils/#laplace.utils.FeatureExtractor.find_last_layer","title":"find_last_layer","text":"<pre><code>find_last_layer(x: Tensor | MutableMapping[str, Tensor | Any]) -&gt; Tensor\n</code></pre> <p>Automatically determines the last layer of the model with one forward pass. It assumes that the last layer is the same for every forward pass and that it is an instance of <code>torch.nn.Linear</code>. Might not work with every architecture, but is tested with all PyTorch torchvision classification models (besides SqueezeNet, which has no linear last layer).</p> <p>Parameters:</p> Source code in <code>laplace/utils/feature_extractor.py</code> <pre><code>def find_last_layer(\n    self, x: torch.Tensor | MutableMapping[str, torch.Tensor | Any]\n) -&gt; torch.Tensor:\n    \"\"\"Automatically determines the last layer of the model with one\n    forward pass. It assumes that the last layer is the same for every\n    forward pass and that it is an instance of `torch.nn.Linear`.\n    Might not work with every architecture, but is tested with all PyTorch\n    torchvision classification models (besides SqueezeNet, which has no\n    linear last layer).\n\n    Parameters\n    ----------\n    x : torch.Tensor or dict-like object containing the input tensors\n        one batch of data to use as input for the forward pass\n    \"\"\"\n    if self.last_layer is not None:\n        raise ValueError(\"Last layer is already known.\")\n\n    act_out = dict()\n\n    def get_act_hook(name):\n        def act_hook(_, input, __):\n            # only accepts one input (expects linear layer)\n            try:\n                act_out[name] = input[0].detach()\n            except (IndexError, AttributeError):\n                act_out[name] = None\n            # remove hook\n            handles[name].remove()\n\n        return act_hook\n\n    # set hooks for all modules\n    handles = dict()\n    for name, module in self.model.named_modules():\n        handles[name] = module.register_forward_hook(get_act_hook(name))\n\n    # check if model has more than one module\n    # (there might be pathological exceptions)\n    if len(handles) &lt;= 2:\n        raise ValueError(\"The model only has one module.\")\n\n    # forward pass to find execution order\n    out = self.model(x)\n\n    # find the last layer, store features, return output of forward pass\n    keys = list(act_out.keys())\n    for key in reversed(keys):\n        layer = dict(self.model.named_modules())[key]\n        if len(list(layer.children())) == 0:\n            self.set_last_layer(key)\n\n            # save features from first forward pass\n            self._features[key] = act_out[key]\n\n            return out\n\n    raise ValueError(\"Something went wrong (all modules have children).\")\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.FeatureExtractor.find_last_layer(x)","title":"<code>x</code>","text":"(<code>torch.Tensor or dict-like object containing the input tensors</code>)           \u2013            <p>one batch of data to use as input for the forward pass</p>"},{"location":"api_reference/utils/#laplace.utils.Kron","title":"Kron","text":"<pre><code>Kron(kfacs: list[tuple[Tensor] | Tensor])\n</code></pre> <p>Kronecker factored approximate curvature representation for a corresponding neural network. Each element in <code>kfacs</code> is either a tuple or single matrix. A tuple represents two Kronecker factors \\(Q\\), and \\(H\\) and a single element is just a full block Hessian approximation.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>init_from_model</code>             \u2013              <p>Initialize Kronecker factors based on a models architecture.</p> </li> <li> <code>__add__</code>             \u2013              <p>Add up Kronecker factors <code>self</code> and <code>other</code>.</p> </li> <li> <code>__mul__</code>             \u2013              <p>Multiply all Kronecker factors by scalar.</p> </li> <li> <code>decompose</code>             \u2013              <p>Eigendecompose Kronecker factors and turn into <code>KronDecomposed</code>.</p> </li> <li> <code>bmm</code>             \u2013              <p>Batched matrix multiplication with the Kronecker factors.</p> </li> <li> <code>logdet</code>             \u2013              <p>Compute log determinant of the Kronecker factors and sums them up.</p> </li> <li> <code>diag</code>             \u2013              <p>Extract diagonal of the entire Kronecker factorization.</p> </li> <li> <code>to_matrix</code>             \u2013              <p>Make the Kronecker factorization dense by computing the kronecker product.</p> </li> </ul> Source code in <code>laplace/utils/matrix.py</code> <pre><code>def __init__(self, kfacs: list[tuple[torch.Tensor] | torch.Tensor]) -&gt; None:\n    self.kfacs: list[tuple[torch.Tensor] | torch.Tensor] = kfacs\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.Kron(kfacs)","title":"<code>kfacs</code>","text":"(<code>list[Iterable[Tensor] | Tensor]</code>)           \u2013            <p>each element in the list is a tuple of two Kronecker factors Q, H or a single matrix approximating the Hessian (in case of bias, for example)</p>"},{"location":"api_reference/utils/#laplace.utils.Kron.init_from_model","title":"init_from_model","text":"<pre><code>init_from_model(model: Module | Iterable[Parameter], device: device, dtype: dtype) -&gt; Kron\n</code></pre> <p>Initialize Kronecker factors based on a models architecture.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>kron</code> (              <code>Kron</code> )          \u2013            </li> </ul> Source code in <code>laplace/utils/matrix.py</code> <pre><code>@classmethod\ndef init_from_model(\n    cls,\n    model: nn.Module | Iterable[nn.Parameter],\n    device: torch.device,\n    dtype: torch.dtype,\n) -&gt; Kron:\n    \"\"\"Initialize Kronecker factors based on a models architecture.\n\n    Parameters\n    ----------\n    model: `nn.Module` or iterable of parameters, e.g. `model.parameters()`\n    device: The device where each of the Kronecker factor lives in.\n    dtype: The data type of each Kronecker factor.\n\n    Returns\n    -------\n    kron : Kron\n    \"\"\"\n    if isinstance(model, torch.nn.Module):\n        params = model.parameters()\n    else:\n        params = model\n\n    kfacs = list()\n    for p in params:\n        if p.ndim == 1:  # bias\n            P = p.size(0)\n            kfacs.append([torch.zeros(P, P, device=device, dtype=dtype)])\n        elif 4 &gt;= p.ndim &gt;= 2:  # fully connected or conv\n            if p.ndim == 2:  # fully connected\n                P_in, P_out = p.size()\n            else:\n                P_in, P_out = p.shape[0], np.prod(p.shape[1:])\n\n            kfacs.append(\n                [\n                    torch.zeros(P_in, P_in, device=device, dtype=dtype),\n                    torch.zeros(P_out, P_out, device=device, dtype=dtype),\n                ]\n            )\n        else:\n            raise ValueError(\"Invalid parameter shape in network.\")\n\n    return cls(kfacs)\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.Kron.init_from_model(model)","title":"<code>model</code>","text":"(<code>Module | Iterable[Parameter]</code>)           \u2013"},{"location":"api_reference/utils/#laplace.utils.Kron.init_from_model(device)","title":"<code>device</code>","text":"(<code>device</code>)           \u2013"},{"location":"api_reference/utils/#laplace.utils.Kron.init_from_model(dtype)","title":"<code>dtype</code>","text":"(<code>dtype</code>)           \u2013"},{"location":"api_reference/utils/#laplace.utils.Kron.__add__","title":"__add__","text":"<pre><code>__add__(other: Kron) -&gt; Kron\n</code></pre> <p>Add up Kronecker factors <code>self</code> and <code>other</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>kron</code> (              <code>Kron</code> )          \u2013            </li> </ul> Source code in <code>laplace/utils/matrix.py</code> <pre><code>def __add__(self, other: Kron) -&gt; Kron:\n    \"\"\"Add up Kronecker factors `self` and `other`.\n\n    Parameters\n    ----------\n    other : Kron\n\n    Returns\n    -------\n    kron : Kron\n    \"\"\"\n    if not isinstance(other, Kron):\n        raise ValueError(\"Can only add Kron to Kron.\")\n\n    kfacs = [\n        [Hi.add(Hj) for Hi, Hj in zip(Fi, Fj)]\n        for Fi, Fj in zip(self.kfacs, other.kfacs)\n    ]\n\n    return Kron(kfacs)\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.Kron.__add__(other)","title":"<code>other</code>","text":"(<code>Kron</code>)           \u2013"},{"location":"api_reference/utils/#laplace.utils.Kron.__mul__","title":"__mul__","text":"<pre><code>__mul__(scalar: float | Tensor) -&gt; Kron\n</code></pre> <p>Multiply all Kronecker factors by scalar. The multiplication is distributed across the number of factors using <code>pow(scalar, 1 / len(F))</code>. <code>len(F)</code> is either <code>1</code> or <code>2</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>kron</code> (              <code>Kron</code> )          \u2013            </li> </ul> Source code in <code>laplace/utils/matrix.py</code> <pre><code>def __mul__(self, scalar: float | torch.Tensor) -&gt; Kron:\n    \"\"\"Multiply all Kronecker factors by scalar.\n    The multiplication is distributed across the number of factors\n    using `pow(scalar, 1 / len(F))`. `len(F)` is either `1` or `2`.\n\n    Parameters\n    ----------\n    scalar : float, torch.Tensor\n\n    Returns\n    -------\n    kron : Kron\n    \"\"\"\n    if not _is_valid_scalar(scalar):\n        raise ValueError(\"Input not valid python or torch scalar.\")\n\n    # distribute factors evenly so that each group is multiplied by factor\n    kfacs = [[pow(scalar, 1 / len(F)) * Hi for Hi in F] for F in self.kfacs]\n    return Kron(kfacs)\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.Kron.__mul__(scalar)","title":"<code>scalar</code>","text":"(<code>(float, Tensor)</code>)           \u2013"},{"location":"api_reference/utils/#laplace.utils.Kron.decompose","title":"decompose","text":"<pre><code>decompose(damping: bool = False) -&gt; KronDecomposed\n</code></pre> <p>Eigendecompose Kronecker factors and turn into <code>KronDecomposed</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>kron_decomposed</code> (              <code>KronDecomposed</code> )          \u2013            </li> </ul> Source code in <code>laplace/utils/matrix.py</code> <pre><code>def decompose(self, damping: bool = False) -&gt; KronDecomposed:\n    \"\"\"Eigendecompose Kronecker factors and turn into `KronDecomposed`.\n    Parameters\n    ----------\n    damping : bool\n        use damping\n\n    Returns\n    -------\n    kron_decomposed : KronDecomposed\n    \"\"\"\n    eigvecs, eigvals = list(), list()\n    for F in self.kfacs:\n        Qs, ls = list(), list()\n        for Hi in F:\n            if Hi.ndim &gt; 1:\n                # Dense Kronecker factor.\n                eigval, Q = symeig(Hi)\n            else:\n                # Diagonal Kronecker factor.\n                eigval = Hi\n                # This might be too memory intensive since len(Hi) can be large.\n                Q = torch.eye(len(Hi), dtype=Hi.dtype, device=Hi.device)\n            Qs.append(Q)\n            ls.append(eigval)\n        eigvecs.append(Qs)\n        eigvals.append(ls)\n    return KronDecomposed(eigvecs, eigvals, damping=damping)\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.Kron.decompose(damping)","title":"<code>damping</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>use damping</p>"},{"location":"api_reference/utils/#laplace.utils.Kron._bmm","title":"_bmm","text":"<pre><code>_bmm(W: Tensor) -&gt; Tensor\n</code></pre> <p>Implementation of <code>bmm</code> which casts the parameters to the right shape.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>SW</code> (              <code>Tensor</code> )          \u2013            <p>result <code>(batch, classes, params)</code></p> </li> </ul> Source code in <code>laplace/utils/matrix.py</code> <pre><code>def _bmm(self, W: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Implementation of `bmm` which casts the parameters to the right shape.\n\n    Parameters\n    ----------\n    W : torch.Tensor\n        matrix `(batch, classes, params)`\n\n    Returns\n    -------\n    SW : torch.Tensor\n        result `(batch, classes, params)`\n    \"\"\"\n    # self @ W[batch, k, params]\n    assert len(W.size()) == 3\n    B, K, P = W.size()\n    W = W.reshape(B * K, P)\n    cur_p = 0\n    SW = list()\n    for Fs in self.kfacs:\n        if len(Fs) == 1:\n            Q = Fs[0]\n            p = len(Q)\n            W_p = W[:, cur_p : cur_p + p].T\n            SW.append((Q @ W_p).T if Q.ndim &gt; 1 else (Q.view(-1, 1) * W_p).T)\n            cur_p += p\n        elif len(Fs) == 2:\n            Q, H = Fs\n            p_in, p_out = len(Q), len(H)\n            p = p_in * p_out\n            W_p = W[:, cur_p : cur_p + p].reshape(B * K, p_in, p_out)\n            QW_p = Q @ W_p if Q.ndim &gt; 1 else Q.view(-1, 1) * W_p\n            QW_pHt = QW_p @ H.T if H.ndim &gt; 1 else QW_p * H.view(1, -1)\n            SW.append(QW_pHt.reshape(B * K, p_in * p_out))\n            cur_p += p\n        else:\n            raise AttributeError(\"Shape mismatch\")\n    SW = torch.cat(SW, dim=1).reshape(B, K, P)\n    return SW\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.Kron._bmm(W)","title":"<code>W</code>","text":"(<code>Tensor</code>)           \u2013            <p>matrix <code>(batch, classes, params)</code></p>"},{"location":"api_reference/utils/#laplace.utils.Kron.bmm","title":"bmm","text":"<pre><code>bmm(W: Tensor, exponent: float = 1) -&gt; Tensor\n</code></pre> <p>Batched matrix multiplication with the Kronecker factors. If Kron is <code>H</code>, we compute <code>H @ W</code>. This is useful for computing the predictive or a regularization based on Kronecker factors as in continual learning.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>SW</code> (              <code>Tensor</code> )          \u2013            <p>result <code>(batch, classes, params)</code></p> </li> </ul> Source code in <code>laplace/utils/matrix.py</code> <pre><code>def bmm(self, W: torch.Tensor, exponent: float = 1) -&gt; torch.Tensor:\n    \"\"\"Batched matrix multiplication with the Kronecker factors.\n    If Kron is `H`, we compute `H @ W`.\n    This is useful for computing the predictive or a regularization\n    based on Kronecker factors as in continual learning.\n\n    Parameters\n    ----------\n    W : torch.Tensor\n        matrix `(batch, classes, params)`\n    exponent: float, default=1\n        only can be `1` for Kron, requires `KronDecomposed` for other\n        exponent values of the Kronecker factors.\n\n    Returns\n    -------\n    SW : torch.Tensor\n        result `(batch, classes, params)`\n    \"\"\"\n    if exponent != 1:\n        raise ValueError(\"Only supported after decomposition.\")\n    if W.ndim == 1:\n        return self._bmm(W.unsqueeze(0).unsqueeze(0)).squeeze()\n    elif W.ndim == 2:\n        return self._bmm(W.unsqueeze(1)).squeeze()\n    elif W.ndim == 3:\n        return self._bmm(W)\n    else:\n        raise ValueError(\"Invalid shape for W\")\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.Kron.bmm(W)","title":"<code>W</code>","text":"(<code>Tensor</code>)           \u2013            <p>matrix <code>(batch, classes, params)</code></p>"},{"location":"api_reference/utils/#laplace.utils.Kron.bmm(exponent)","title":"<code>exponent</code>","text":"(<code>float</code>, default:                   <code>1</code> )           \u2013            <p>only can be <code>1</code> for Kron, requires <code>KronDecomposed</code> for other exponent values of the Kronecker factors.</p>"},{"location":"api_reference/utils/#laplace.utils.Kron.logdet","title":"logdet","text":"<pre><code>logdet() -&gt; Tensor\n</code></pre> <p>Compute log determinant of the Kronecker factors and sums them up. This corresponds to the log determinant of the entire Hessian approximation.</p> <p>Returns:</p> <ul> <li> <code>logdet</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/utils/matrix.py</code> <pre><code>def logdet(self) -&gt; torch.Tensor:\n    \"\"\"Compute log determinant of the Kronecker factors and sums them up.\n    This corresponds to the log determinant of the entire Hessian approximation.\n\n    Returns\n    -------\n    logdet : torch.Tensor\n    \"\"\"\n    logdet = 0\n    for F in self.kfacs:\n        if len(F) == 1:\n            logdet += F[0].logdet() if F[0].ndim &gt; 1 else F[0].log().sum()\n        else:  # len(F) == 2\n            Hi, Hj = F\n            p_in, p_out = len(Hi), len(Hj)\n            logdet += p_out * Hi.logdet() if Hi.ndim &gt; 1 else p_out * Hi.log().sum()\n            logdet += p_in * Hj.logdet() if Hj.ndim &gt; 1 else p_in * Hj.log().sum()\n    return logdet\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.Kron.diag","title":"diag","text":"<pre><code>diag() -&gt; Tensor\n</code></pre> <p>Extract diagonal of the entire Kronecker factorization.</p> <p>Returns:</p> <ul> <li> <code>diag</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/utils/matrix.py</code> <pre><code>def diag(self) -&gt; torch.Tensor:\n    \"\"\"Extract diagonal of the entire Kronecker factorization.\n\n    Returns\n    -------\n    diag : torch.Tensor\n    \"\"\"\n    diags = list()\n    for F in self.kfacs:\n        F0 = F[0].diag() if F[0].ndim &gt; 1 else F[0]\n        if len(F) == 1:\n            diags.append(F0)\n        else:\n            F1 = F[1].diag() if F[1].ndim &gt; 1 else F[1]\n            diags.append(torch.outer(F0, F1).flatten())\n    return torch.cat(diags)\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.Kron.to_matrix","title":"to_matrix","text":"<pre><code>to_matrix() -&gt; Tensor\n</code></pre> <p>Make the Kronecker factorization dense by computing the kronecker product. Warning: this should only be used for testing purposes as it will allocate large amounts of memory for big architectures.</p> <p>Returns:</p> <ul> <li> <code>block_diag</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/utils/matrix.py</code> <pre><code>def to_matrix(self) -&gt; torch.Tensor:\n    \"\"\"Make the Kronecker factorization dense by computing the kronecker product.\n    Warning: this should only be used for testing purposes as it will allocate\n    large amounts of memory for big architectures.\n\n    Returns\n    -------\n    block_diag : torch.Tensor\n    \"\"\"\n    blocks = list()\n    for F in self.kfacs:\n        F0 = F[0] if F[0].ndim &gt; 1 else F[0].diag()\n        if len(F) == 1:\n            blocks.append(F0)\n        else:\n            F1 = F[1] if F[1].ndim &gt; 1 else F[1].diag()\n            blocks.append(kron(F0, F1))\n    return block_diag(blocks)\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.KronDecomposed","title":"KronDecomposed","text":"<pre><code>KronDecomposed(eigenvectors: list[tuple[Tensor]], eigenvalues: list[tuple[Tensor]], deltas: Tensor | None = None, damping: bool = False)\n</code></pre> <p>Decomposed Kronecker factored approximate curvature representation for a corresponding neural network. Each matrix in <code>Kron</code> is decomposed to obtain <code>KronDecomposed</code>. Front-loading decomposition allows cheap repeated computation of inverses and log determinants. In contrast to <code>Kron</code>, we can add scalar or layerwise scalars but we cannot add other <code>Kron</code> or <code>KronDecomposed</code> anymore.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>__add__</code>             \u2013              <p>Add scalar per layer or only scalar to Kronecker factors.</p> </li> <li> <code>__mul__</code>             \u2013              <p>Multiply by a scalar by changing the eigenvalues.</p> </li> <li> <code>logdet</code>             \u2013              <p>Compute log determinant of the Kronecker factors and sums them up.</p> </li> <li> <code>bmm</code>             \u2013              <p>Batched matrix multiplication with the decomposed Kronecker factors.</p> </li> <li> <code>diag</code>             \u2013              <p>Extract diagonal of the entire decomposed Kronecker factorization.</p> </li> <li> <code>to_matrix</code>             \u2013              <p>Make the Kronecker factorization dense by computing the kronecker product.</p> </li> </ul> Source code in <code>laplace/utils/matrix.py</code> <pre><code>def __init__(\n    self,\n    eigenvectors: list[tuple[torch.Tensor]],\n    eigenvalues: list[tuple[torch.Tensor]],\n    deltas: torch.Tensor | None = None,\n    damping: bool = False,\n):\n    self.eigenvectors: list[tuple[torch.Tensor]] = eigenvectors\n    self.eigenvalues: list[tuple[torch.Tensor]] = eigenvalues\n\n    device: torch.device = eigenvectors[0][0].device\n    dtype: torch.dtype = eigenvectors[0][0].dtype\n\n    if deltas is None:\n        self.deltas: torch.Tensor = torch.zeros(\n            len(self), device=device, dtype=dtype\n        )\n    else:\n        self._check_deltas(deltas)\n        self.deltas: torch.Tensor = deltas\n    self.damping: bool = damping\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.KronDecomposed(eigenvectors)","title":"<code>eigenvectors</code>","text":"(<code>list[Tuple[Tensor]]</code>)           \u2013            <p>eigenvectors corresponding to matrices in a corresponding <code>Kron</code></p>"},{"location":"api_reference/utils/#laplace.utils.KronDecomposed(eigenvalues)","title":"<code>eigenvalues</code>","text":"(<code>list[Tuple[Tensor]]</code>)           \u2013            <p>eigenvalues corresponding to matrices in a corresponding <code>Kron</code></p>"},{"location":"api_reference/utils/#laplace.utils.KronDecomposed(deltas)","title":"<code>deltas</code>","text":"(<code>Tensor</code>, default:                   <code>None</code> )           \u2013            <p>addend for each group of Kronecker factors representing, for example, a prior precision</p>"},{"location":"api_reference/utils/#laplace.utils.KronDecomposed(dampen)","title":"<code>dampen</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>use dampen approximation mixing prior and Kron partially multiplicatively</p>"},{"location":"api_reference/utils/#laplace.utils.KronDecomposed.__add__","title":"__add__","text":"<pre><code>__add__(deltas: Tensor) -&gt; KronDecomposed\n</code></pre> <p>Add scalar per layer or only scalar to Kronecker factors.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>kron</code> (              <code>KronDecomposed</code> )          \u2013            </li> </ul> Source code in <code>laplace/utils/matrix.py</code> <pre><code>def __add__(self, deltas: torch.Tensor) -&gt; KronDecomposed:\n    \"\"\"Add scalar per layer or only scalar to Kronecker factors.\n\n    Parameters\n    ----------\n    deltas : torch.Tensor\n        either same length as `eigenvalues` or scalar.\n\n    Returns\n    -------\n    kron : KronDecomposed\n    \"\"\"\n    self._check_deltas(deltas)\n    return KronDecomposed(self.eigenvectors, self.eigenvalues, self.deltas + deltas)\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.KronDecomposed.__add__(deltas)","title":"<code>deltas</code>","text":"(<code>Tensor</code>)           \u2013            <p>either same length as <code>eigenvalues</code> or scalar.</p>"},{"location":"api_reference/utils/#laplace.utils.KronDecomposed.__mul__","title":"__mul__","text":"<pre><code>__mul__(scalar: Tensor | float) -&gt; KronDecomposed\n</code></pre> <p>Multiply by a scalar by changing the eigenvalues. Same as for the case of <code>Kron</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>kron</code> (              <code>KronDecomposed</code> )          \u2013            </li> </ul> Source code in <code>laplace/utils/matrix.py</code> <pre><code>def __mul__(self, scalar: torch.Tensor | float) -&gt; KronDecomposed:\n    \"\"\"Multiply by a scalar by changing the eigenvalues.\n    Same as for the case of `Kron`.\n\n    Parameters\n    ----------\n    scalar : torch.Tensor or float\n\n    Returns\n    -------\n    kron : KronDecomposed\n    \"\"\"\n    if not _is_valid_scalar(scalar):\n        raise ValueError(\"Invalid argument, can only multiply Kron with scalar.\")\n\n    eigenvalues = [\n        [pow(scalar, 1 / len(ls)) * eigval for eigval in ls]\n        for ls in self.eigenvalues\n    ]\n    return KronDecomposed(self.eigenvectors, eigenvalues, self.deltas)\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.KronDecomposed.__mul__(scalar)","title":"<code>scalar</code>","text":"(<code>Tensor or float</code>)           \u2013"},{"location":"api_reference/utils/#laplace.utils.KronDecomposed.logdet","title":"logdet","text":"<pre><code>logdet() -&gt; Tensor\n</code></pre> <p>Compute log determinant of the Kronecker factors and sums them up. This corresponds to the log determinant of the entire Hessian approximation. In contrast to <code>Kron.logdet()</code>, additive <code>deltas</code> corresponding to prior precisions are added.</p> <p>Returns:</p> <ul> <li> <code>logdet</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/utils/matrix.py</code> <pre><code>def logdet(self) -&gt; torch.Tensor:\n    \"\"\"Compute log determinant of the Kronecker factors and sums them up.\n    This corresponds to the log determinant of the entire Hessian approximation.\n    In contrast to `Kron.logdet()`, additive `deltas` corresponding to prior\n    precisions are added.\n\n    Returns\n    -------\n    logdet : torch.Tensor\n    \"\"\"\n    logdet = 0\n    for ls, delta in zip(self.eigenvalues, self.deltas):\n        if len(ls) == 1:  # not KFAC just full\n            logdet += torch.log(ls[0] + delta).sum()\n        elif len(ls) == 2:\n            l1, l2 = ls\n            if self.damping:\n                l1d, l2d = l1 + torch.sqrt(delta), l2 + torch.sqrt(delta)\n                logdet += torch.log(torch.outer(l1d, l2d)).sum()\n            else:\n                logdet += torch.log(torch.outer(l1, l2) + delta).sum()\n        else:\n            raise ValueError(\"Too many Kronecker factors. Something went wrong.\")\n    return logdet\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.KronDecomposed._bmm","title":"_bmm","text":"<pre><code>_bmm(W: Tensor, exponent: float = -1) -&gt; Tensor\n</code></pre> <p>Implementation of <code>bmm</code>, i.e., <code>self ** exponent @ W</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>SW</code> (              <code>Tensor</code> )          \u2013            <p>result <code>(batch, classes, params)</code></p> </li> </ul> Source code in <code>laplace/utils/matrix.py</code> <pre><code>def _bmm(self, W: torch.Tensor, exponent: float = -1) -&gt; torch.Tensor:\n    \"\"\"Implementation of `bmm`, i.e., `self ** exponent @ W`.\n\n    Parameters\n    ----------\n    W : torch.Tensor\n        matrix `(batch, classes, params)`\n    exponent : float\n        exponent on `self`\n\n    Returns\n    -------\n    SW : torch.Tensor\n        result `(batch, classes, params)`\n    \"\"\"\n    # self @ W[batch, k, params]\n    assert len(W.size()) == 3\n    B, K, P = W.size()\n    W = W.reshape(B * K, P)\n    cur_p = 0\n    SW = list()\n    for i, (ls, Qs, delta) in enumerate(\n        zip(self.eigenvalues, self.eigenvectors, self.deltas)\n    ):\n        if len(ls) == 1:\n            Q, eigval, p = Qs[0], ls[0], len(ls[0])\n            ldelta_exp = torch.pow(eigval + delta, exponent).reshape(-1, 1)\n            W_p = W[:, cur_p : cur_p + p].T\n            SW.append((Q @ (ldelta_exp * (Q.T @ W_p))).T)\n            cur_p += p\n        elif len(ls) == 2:\n            Q1, Q2 = Qs\n            l1, l2 = ls\n            p = len(l1) * len(l2)\n            if self.damping:\n                l1d, l2d = l1 + torch.sqrt(delta), l2 + torch.sqrt(delta)\n                ldelta_exp = torch.pow(torch.outer(l1d, l2d), exponent).unsqueeze(0)\n            else:\n                ldelta_exp = torch.pow(\n                    torch.outer(l1, l2) + delta, exponent\n                ).unsqueeze(0)\n            p_in, p_out = len(l1), len(l2)\n            W_p = W[:, cur_p : cur_p + p].reshape(B * K, p_in, p_out)\n            W_p = (Q1.T @ W_p @ Q2) * ldelta_exp\n            W_p = Q1 @ W_p @ Q2.T\n            SW.append(W_p.reshape(B * K, p_in * p_out))\n            cur_p += p\n        else:\n            raise AttributeError(\"Shape mismatch\")\n    SW = torch.cat(SW, dim=1).reshape(B, K, P)\n    return SW\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.KronDecomposed._bmm(W)","title":"<code>W</code>","text":"(<code>Tensor</code>)           \u2013            <p>matrix <code>(batch, classes, params)</code></p>"},{"location":"api_reference/utils/#laplace.utils.KronDecomposed._bmm(exponent)","title":"<code>exponent</code>","text":"(<code>float</code>, default:                   <code>-1</code> )           \u2013            <p>exponent on <code>self</code></p>"},{"location":"api_reference/utils/#laplace.utils.KronDecomposed.bmm","title":"bmm","text":"<pre><code>bmm(W: Tensor, exponent: float = -1) -&gt; Tensor\n</code></pre> <p>Batched matrix multiplication with the decomposed Kronecker factors. This is useful for computing the predictive or a regularization loss. Compared to <code>Kron.bmm</code>, a prior can be added here in form of <code>deltas</code> and the exponent can be other than just 1. Computes \\(H^{exponent} W\\).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>SW</code> (              <code>Tensor</code> )          \u2013            <p>result <code>(batch, classes, params)</code></p> </li> </ul> Source code in <code>laplace/utils/matrix.py</code> <pre><code>def bmm(self, W: torch.Tensor, exponent: float = -1) -&gt; torch.Tensor:\n    \"\"\"Batched matrix multiplication with the decomposed Kronecker factors.\n    This is useful for computing the predictive or a regularization loss.\n    Compared to `Kron.bmm`, a prior can be added here in form of `deltas`\n    and the exponent can be other than just 1.\n    Computes \\\\(H^{exponent} W\\\\).\n\n    Parameters\n    ----------\n    W : torch.Tensor\n        matrix `(batch, classes, params)`\n    exponent: float, default=1\n\n    Returns\n    -------\n    SW : torch.Tensor\n        result `(batch, classes, params)`\n    \"\"\"\n    if W.ndim == 1:\n        return self._bmm(W.unsqueeze(0).unsqueeze(0), exponent).squeeze()\n    elif W.ndim == 2:\n        return self._bmm(W.unsqueeze(1), exponent).squeeze()\n    elif W.ndim == 3:\n        return self._bmm(W, exponent)\n    else:\n        raise ValueError(\"Invalid shape for W\")\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.KronDecomposed.bmm(W)","title":"<code>W</code>","text":"(<code>Tensor</code>)           \u2013            <p>matrix <code>(batch, classes, params)</code></p>"},{"location":"api_reference/utils/#laplace.utils.KronDecomposed.bmm(exponent)","title":"<code>exponent</code>","text":"(<code>float</code>, default:                   <code>-1</code> )           \u2013"},{"location":"api_reference/utils/#laplace.utils.KronDecomposed.diag","title":"diag","text":"<pre><code>diag(exponent: float = 1) -&gt; Tensor\n</code></pre> <p>Extract diagonal of the entire decomposed Kronecker factorization.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>diag</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/utils/matrix.py</code> <pre><code>def diag(self, exponent: float = 1) -&gt; torch.Tensor:\n    \"\"\"Extract diagonal of the entire decomposed Kronecker factorization.\n\n    Parameters\n    ----------\n    exponent: float, default=1\n        exponent of the Kronecker factorization\n\n    Returns\n    -------\n    diag : torch.Tensor\n    \"\"\"\n    diags = list()\n    for Qs, ls, delta in zip(self.eigenvectors, self.eigenvalues, self.deltas):\n        if len(ls) == 1:\n            Ql = Qs[0] * torch.pow(ls[0] + delta, exponent).reshape(1, -1)\n            d = torch.einsum(\n                \"mp,mp-&gt;m\", Ql, Qs[0]\n            )  # only compute inner products for diag\n            diags.append(d)\n        else:\n            Q1, Q2 = Qs\n            l1, l2 = ls\n            if self.damping:\n                delta_sqrt = torch.sqrt(delta)\n                eigval = torch.pow(\n                    torch.outer(l1 + delta_sqrt, l2 + delta_sqrt), exponent\n                )\n            else:\n                eigval = torch.pow(torch.outer(l1, l2) + delta, exponent)\n            d = oe.contract(\"mp,nq,pq,mp,nq-&gt;mn\", Q1, Q2, eigval, Q1, Q2).flatten()\n            diags.append(d)\n    return torch.cat(diags)\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.KronDecomposed.diag(exponent)","title":"<code>exponent</code>","text":"(<code>float</code>, default:                   <code>1</code> )           \u2013            <p>exponent of the Kronecker factorization</p>"},{"location":"api_reference/utils/#laplace.utils.KronDecomposed.to_matrix","title":"to_matrix","text":"<pre><code>to_matrix(exponent: float = 1) -&gt; Tensor\n</code></pre> <p>Make the Kronecker factorization dense by computing the kronecker product. Warning: this should only be used for testing purposes as it will allocate large amounts of memory for big architectures.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>block_diag</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/utils/matrix.py</code> <pre><code>def to_matrix(self, exponent: float = 1) -&gt; torch.Tensor:\n    \"\"\"Make the Kronecker factorization dense by computing the kronecker product.\n    Warning: this should only be used for testing purposes as it will allocate\n    large amounts of memory for big architectures.\n\n    Parameters\n    ----------\n    exponent: float, default=1\n        exponent of the Kronecker factorization\n\n    Returns\n    -------\n    block_diag : torch.Tensor\n    \"\"\"\n    blocks = list()\n    for Qs, ls, delta in zip(self.eigenvectors, self.eigenvalues, self.deltas):\n        if len(ls) == 1:\n            Q, eigval = Qs[0], ls[0]\n            blocks.append(Q @ torch.diag(torch.pow(eigval + delta, exponent)) @ Q.T)\n        else:\n            Q1, Q2 = Qs\n            l1, l2 = ls\n            Q = kron(Q1, Q2)\n            if self.damping:\n                delta_sqrt = torch.sqrt(delta)\n                eigval = torch.pow(\n                    torch.outer(l1 + delta_sqrt, l2 + delta_sqrt), exponent\n                )\n            else:\n                eigval = torch.pow(torch.outer(l1, l2) + delta, exponent)\n            L = torch.diag(eigval.flatten())\n            blocks.append(Q @ L @ Q.T)\n    return block_diag(blocks)\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.KronDecomposed.to_matrix(exponent)","title":"<code>exponent</code>","text":"(<code>float</code>, default:                   <code>1</code> )           \u2013            <p>exponent of the Kronecker factorization</p>"},{"location":"api_reference/utils/#laplace.utils.SubnetMask","title":"SubnetMask","text":"<pre><code>SubnetMask(model: Module)\n</code></pre> <p>Baseclass for all subnetwork masks in this library (for subnetwork Laplace).</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>convert_subnet_mask_to_indices</code>             \u2013              <p>Converts a subnetwork mask into subnetwork indices.</p> </li> <li> <code>select</code>             \u2013              <p>Select the subnetwork mask.</p> </li> <li> <code>get_subnet_mask</code>             \u2013              <p>Get the subnetwork mask.</p> </li> </ul> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def __init__(self, model: nn.Module) -&gt; None:\n    self.model: nn.Module = model\n    self.parameter_vector: torch.Tensor = parameters_to_vector(\n        self.model.parameters()\n    ).detach()\n    self._n_params: int = len(self.parameter_vector)\n    self._indices: torch.LongTensor | None = None\n    self._n_params_subnet: int | None = None\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.SubnetMask(model)","title":"<code>model</code>","text":"(<code>Module</code>)           \u2013"},{"location":"api_reference/utils/#laplace.utils.SubnetMask.convert_subnet_mask_to_indices","title":"convert_subnet_mask_to_indices","text":"<pre><code>convert_subnet_mask_to_indices(subnet_mask: Tensor) -&gt; LongTensor\n</code></pre> <p>Converts a subnetwork mask into subnetwork indices.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>subnet_mask_indices</code> (              <code>LongTensor</code> )          \u2013            <p>a vector of indices of the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>) that define the subnetwork</p> </li> </ul> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def convert_subnet_mask_to_indices(\n    self, subnet_mask: torch.Tensor\n) -&gt; torch.LongTensor:\n    \"\"\"Converts a subnetwork mask into subnetwork indices.\n\n    Parameters\n    ----------\n    subnet_mask : torch.Tensor\n        a binary vector of size (n_params) where 1s locate the subnetwork parameters\n        within the vectorized model parameters\n        (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)\n\n    Returns\n    -------\n    subnet_mask_indices : torch.LongTensor\n        a vector of indices of the vectorized model parameters\n        (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)\n        that define the subnetwork\n    \"\"\"\n    if not isinstance(subnet_mask, torch.Tensor):\n        raise ValueError(\"Subnetwork mask needs to be torch.Tensor!\")\n    elif (\n        subnet_mask.dtype\n        not in [\n            torch.int64,\n            torch.int32,\n            torch.int16,\n            torch.int8,\n            torch.uint8,\n            torch.bool,\n        ]\n        or len(subnet_mask.shape) != 1\n    ):\n        raise ValueError(\n            \"Subnetwork mask needs to be 1-dimensional integral or boolean tensor!\"\n        )\n    elif (\n        len(subnet_mask) != self._n_params\n        or len(subnet_mask[subnet_mask == 0]) + len(subnet_mask[subnet_mask == 1])\n        != self._n_params\n    ):\n        raise ValueError(\n            \"Subnetwork mask needs to be a binary vector of\"\n            \"size (n_params) where 1s locate the subnetwork\"\n            \"parameters within the vectorized model parameters\"\n            \"(i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)!\"\n        )\n\n    subnet_mask_indices = subnet_mask.nonzero(as_tuple=True)[0]\n    return subnet_mask_indices\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.SubnetMask.convert_subnet_mask_to_indices(subnet_mask)","title":"<code>subnet_mask</code>","text":"(<code>Tensor</code>)           \u2013            <p>a binary vector of size (n_params) where 1s locate the subnetwork parameters within the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>)</p>"},{"location":"api_reference/utils/#laplace.utils.SubnetMask.select","title":"select","text":"<pre><code>select(train_loader: DataLoader | None = None) -&gt; LongTensor\n</code></pre> <p>Select the subnetwork mask.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>subnet_mask_indices</code> (              <code>LongTensor</code> )          \u2013            <p>a vector of indices of the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>) that define the subnetwork</p> </li> </ul> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def select(self, train_loader: DataLoader | None = None) -&gt; torch.LongTensor:\n    \"\"\"Select the subnetwork mask.\n\n    Parameters\n    ----------\n    train_loader : torch.data.utils.DataLoader, default=None\n        each iterate is a training batch (X, y);\n        `train_loader.dataset` needs to be set to access \\\\(N\\\\), size of the data set\n\n    Returns\n    -------\n    subnet_mask_indices : torch.LongTensor\n        a vector of indices of the vectorized model parameters\n        (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)\n        that define the subnetwork\n    \"\"\"\n    if self._indices is not None:\n        raise ValueError(\"Subnetwork mask already selected.\")\n\n    subnet_mask = self.get_subnet_mask(train_loader)\n    self._indices = self.convert_subnet_mask_to_indices(subnet_mask)\n    return self._indices\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.SubnetMask.select(train_loader)","title":"<code>train_loader</code>","text":"(<code>DataLoader</code>, default:                   <code>None</code> )           \u2013            <p>each iterate is a training batch (X, y); <code>train_loader.dataset</code> needs to be set to access \\(N\\), size of the data set</p>"},{"location":"api_reference/utils/#laplace.utils.SubnetMask.get_subnet_mask","title":"get_subnet_mask","text":"<pre><code>get_subnet_mask(train_loader: DataLoader) -&gt; Tensor\n</code></pre> <p>Get the subnetwork mask.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>subnet_mask</code> (              <code>Tensor</code> )          \u2013            <p>a binary vector of size (n_params) where 1s locate the subnetwork parameters within the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>)</p> </li> </ul> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def get_subnet_mask(self, train_loader: DataLoader) -&gt; torch.Tensor:\n    \"\"\"Get the subnetwork mask.\n\n    Parameters\n    ----------\n    train_loader : torch.data.utils.DataLoader\n        each iterate is a training batch (X, y);\n        `train_loader.dataset` needs to be set to access \\\\(N\\\\), size of the data set\n\n    Returns\n    -------\n    subnet_mask: torch.Tensor\n        a binary vector of size (n_params) where 1s locate the subnetwork parameters\n        within the vectorized model parameters\n        (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.SubnetMask.get_subnet_mask(train_loader)","title":"<code>train_loader</code>","text":"(<code>DataLoader</code>)           \u2013            <p>each iterate is a training batch (X, y); <code>train_loader.dataset</code> needs to be set to access \\(N\\), size of the data set</p>"},{"location":"api_reference/utils/#laplace.utils.RandomSubnetMask","title":"RandomSubnetMask","text":"<pre><code>RandomSubnetMask(model: Module, n_params_subnet: int)\n</code></pre> <p>               Bases: <code>ScoreBasedSubnetMask</code></p> <p>Subnetwork mask of parameters sampled uniformly at random.</p> <p>Methods:</p> <ul> <li> <code>convert_subnet_mask_to_indices</code>             \u2013              <p>Converts a subnetwork mask into subnetwork indices.</p> </li> <li> <code>select</code>             \u2013              <p>Select the subnetwork mask.</p> </li> <li> <code>get_subnet_mask</code>             \u2013              <p>Get the subnetwork mask by (descendingly) ranking parameters based on their scores.</p> </li> </ul> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def __init__(self, model: nn.Module, n_params_subnet: int) -&gt; None:\n    super().__init__(model)\n\n    if n_params_subnet is None:\n        raise ValueError(\n            \"Need to pass number of subnetwork parameters when using subnetwork Laplace.\"\n        )\n    if n_params_subnet &gt; self._n_params:\n        raise ValueError(\n            f\"Subnetwork ({n_params_subnet}) cannot be larger than model ({self._n_params}).\"\n        )\n    self._n_params_subnet = n_params_subnet\n    self._param_scores: torch.Tensor | None = None\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.RandomSubnetMask.convert_subnet_mask_to_indices","title":"convert_subnet_mask_to_indices","text":"<pre><code>convert_subnet_mask_to_indices(subnet_mask: Tensor) -&gt; LongTensor\n</code></pre> <p>Converts a subnetwork mask into subnetwork indices.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>subnet_mask_indices</code> (              <code>LongTensor</code> )          \u2013            <p>a vector of indices of the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>) that define the subnetwork</p> </li> </ul> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def convert_subnet_mask_to_indices(\n    self, subnet_mask: torch.Tensor\n) -&gt; torch.LongTensor:\n    \"\"\"Converts a subnetwork mask into subnetwork indices.\n\n    Parameters\n    ----------\n    subnet_mask : torch.Tensor\n        a binary vector of size (n_params) where 1s locate the subnetwork parameters\n        within the vectorized model parameters\n        (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)\n\n    Returns\n    -------\n    subnet_mask_indices : torch.LongTensor\n        a vector of indices of the vectorized model parameters\n        (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)\n        that define the subnetwork\n    \"\"\"\n    if not isinstance(subnet_mask, torch.Tensor):\n        raise ValueError(\"Subnetwork mask needs to be torch.Tensor!\")\n    elif (\n        subnet_mask.dtype\n        not in [\n            torch.int64,\n            torch.int32,\n            torch.int16,\n            torch.int8,\n            torch.uint8,\n            torch.bool,\n        ]\n        or len(subnet_mask.shape) != 1\n    ):\n        raise ValueError(\n            \"Subnetwork mask needs to be 1-dimensional integral or boolean tensor!\"\n        )\n    elif (\n        len(subnet_mask) != self._n_params\n        or len(subnet_mask[subnet_mask == 0]) + len(subnet_mask[subnet_mask == 1])\n        != self._n_params\n    ):\n        raise ValueError(\n            \"Subnetwork mask needs to be a binary vector of\"\n            \"size (n_params) where 1s locate the subnetwork\"\n            \"parameters within the vectorized model parameters\"\n            \"(i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)!\"\n        )\n\n    subnet_mask_indices = subnet_mask.nonzero(as_tuple=True)[0]\n    return subnet_mask_indices\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.RandomSubnetMask.convert_subnet_mask_to_indices(subnet_mask)","title":"<code>subnet_mask</code>","text":"(<code>Tensor</code>)           \u2013            <p>a binary vector of size (n_params) where 1s locate the subnetwork parameters within the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>)</p>"},{"location":"api_reference/utils/#laplace.utils.RandomSubnetMask.select","title":"select","text":"<pre><code>select(train_loader: DataLoader | None = None) -&gt; LongTensor\n</code></pre> <p>Select the subnetwork mask.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>subnet_mask_indices</code> (              <code>LongTensor</code> )          \u2013            <p>a vector of indices of the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>) that define the subnetwork</p> </li> </ul> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def select(self, train_loader: DataLoader | None = None) -&gt; torch.LongTensor:\n    \"\"\"Select the subnetwork mask.\n\n    Parameters\n    ----------\n    train_loader : torch.data.utils.DataLoader, default=None\n        each iterate is a training batch (X, y);\n        `train_loader.dataset` needs to be set to access \\\\(N\\\\), size of the data set\n\n    Returns\n    -------\n    subnet_mask_indices : torch.LongTensor\n        a vector of indices of the vectorized model parameters\n        (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)\n        that define the subnetwork\n    \"\"\"\n    if self._indices is not None:\n        raise ValueError(\"Subnetwork mask already selected.\")\n\n    subnet_mask = self.get_subnet_mask(train_loader)\n    self._indices = self.convert_subnet_mask_to_indices(subnet_mask)\n    return self._indices\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.RandomSubnetMask.select(train_loader)","title":"<code>train_loader</code>","text":"(<code>DataLoader</code>, default:                   <code>None</code> )           \u2013            <p>each iterate is a training batch (X, y); <code>train_loader.dataset</code> needs to be set to access \\(N\\), size of the data set</p>"},{"location":"api_reference/utils/#laplace.utils.RandomSubnetMask.get_subnet_mask","title":"get_subnet_mask","text":"<pre><code>get_subnet_mask(train_loader)\n</code></pre> <p>Get the subnetwork mask by (descendingly) ranking parameters based on their scores.</p> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def get_subnet_mask(self, train_loader):\n    \"\"\"Get the subnetwork mask by (descendingly) ranking parameters based on their scores.\"\"\"\n    if self._param_scores is None:\n        self._param_scores = self.compute_param_scores(train_loader)\n    self._check_param_scores()\n\n    idx = torch.argsort(self._param_scores, descending=True)[\n        : self._n_params_subnet\n    ]\n    idx = idx.sort()[0]\n    subnet_mask = torch.zeros_like(self.parameter_vector).bool()\n    subnet_mask[idx] = 1\n    return subnet_mask\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.LargestMagnitudeSubnetMask","title":"LargestMagnitudeSubnetMask","text":"<pre><code>LargestMagnitudeSubnetMask(model: Module, n_params_subnet: int)\n</code></pre> <p>               Bases: <code>ScoreBasedSubnetMask</code></p> <p>Subnetwork mask identifying the parameters with the largest magnitude.</p> <p>Methods:</p> <ul> <li> <code>convert_subnet_mask_to_indices</code>             \u2013              <p>Converts a subnetwork mask into subnetwork indices.</p> </li> <li> <code>select</code>             \u2013              <p>Select the subnetwork mask.</p> </li> <li> <code>get_subnet_mask</code>             \u2013              <p>Get the subnetwork mask by (descendingly) ranking parameters based on their scores.</p> </li> </ul> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def __init__(self, model: nn.Module, n_params_subnet: int) -&gt; None:\n    super().__init__(model)\n\n    if n_params_subnet is None:\n        raise ValueError(\n            \"Need to pass number of subnetwork parameters when using subnetwork Laplace.\"\n        )\n    if n_params_subnet &gt; self._n_params:\n        raise ValueError(\n            f\"Subnetwork ({n_params_subnet}) cannot be larger than model ({self._n_params}).\"\n        )\n    self._n_params_subnet = n_params_subnet\n    self._param_scores: torch.Tensor | None = None\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.LargestMagnitudeSubnetMask.convert_subnet_mask_to_indices","title":"convert_subnet_mask_to_indices","text":"<pre><code>convert_subnet_mask_to_indices(subnet_mask: Tensor) -&gt; LongTensor\n</code></pre> <p>Converts a subnetwork mask into subnetwork indices.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>subnet_mask_indices</code> (              <code>LongTensor</code> )          \u2013            <p>a vector of indices of the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>) that define the subnetwork</p> </li> </ul> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def convert_subnet_mask_to_indices(\n    self, subnet_mask: torch.Tensor\n) -&gt; torch.LongTensor:\n    \"\"\"Converts a subnetwork mask into subnetwork indices.\n\n    Parameters\n    ----------\n    subnet_mask : torch.Tensor\n        a binary vector of size (n_params) where 1s locate the subnetwork parameters\n        within the vectorized model parameters\n        (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)\n\n    Returns\n    -------\n    subnet_mask_indices : torch.LongTensor\n        a vector of indices of the vectorized model parameters\n        (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)\n        that define the subnetwork\n    \"\"\"\n    if not isinstance(subnet_mask, torch.Tensor):\n        raise ValueError(\"Subnetwork mask needs to be torch.Tensor!\")\n    elif (\n        subnet_mask.dtype\n        not in [\n            torch.int64,\n            torch.int32,\n            torch.int16,\n            torch.int8,\n            torch.uint8,\n            torch.bool,\n        ]\n        or len(subnet_mask.shape) != 1\n    ):\n        raise ValueError(\n            \"Subnetwork mask needs to be 1-dimensional integral or boolean tensor!\"\n        )\n    elif (\n        len(subnet_mask) != self._n_params\n        or len(subnet_mask[subnet_mask == 0]) + len(subnet_mask[subnet_mask == 1])\n        != self._n_params\n    ):\n        raise ValueError(\n            \"Subnetwork mask needs to be a binary vector of\"\n            \"size (n_params) where 1s locate the subnetwork\"\n            \"parameters within the vectorized model parameters\"\n            \"(i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)!\"\n        )\n\n    subnet_mask_indices = subnet_mask.nonzero(as_tuple=True)[0]\n    return subnet_mask_indices\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.LargestMagnitudeSubnetMask.convert_subnet_mask_to_indices(subnet_mask)","title":"<code>subnet_mask</code>","text":"(<code>Tensor</code>)           \u2013            <p>a binary vector of size (n_params) where 1s locate the subnetwork parameters within the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>)</p>"},{"location":"api_reference/utils/#laplace.utils.LargestMagnitudeSubnetMask.select","title":"select","text":"<pre><code>select(train_loader: DataLoader | None = None) -&gt; LongTensor\n</code></pre> <p>Select the subnetwork mask.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>subnet_mask_indices</code> (              <code>LongTensor</code> )          \u2013            <p>a vector of indices of the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>) that define the subnetwork</p> </li> </ul> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def select(self, train_loader: DataLoader | None = None) -&gt; torch.LongTensor:\n    \"\"\"Select the subnetwork mask.\n\n    Parameters\n    ----------\n    train_loader : torch.data.utils.DataLoader, default=None\n        each iterate is a training batch (X, y);\n        `train_loader.dataset` needs to be set to access \\\\(N\\\\), size of the data set\n\n    Returns\n    -------\n    subnet_mask_indices : torch.LongTensor\n        a vector of indices of the vectorized model parameters\n        (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)\n        that define the subnetwork\n    \"\"\"\n    if self._indices is not None:\n        raise ValueError(\"Subnetwork mask already selected.\")\n\n    subnet_mask = self.get_subnet_mask(train_loader)\n    self._indices = self.convert_subnet_mask_to_indices(subnet_mask)\n    return self._indices\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.LargestMagnitudeSubnetMask.select(train_loader)","title":"<code>train_loader</code>","text":"(<code>DataLoader</code>, default:                   <code>None</code> )           \u2013            <p>each iterate is a training batch (X, y); <code>train_loader.dataset</code> needs to be set to access \\(N\\), size of the data set</p>"},{"location":"api_reference/utils/#laplace.utils.LargestMagnitudeSubnetMask.get_subnet_mask","title":"get_subnet_mask","text":"<pre><code>get_subnet_mask(train_loader)\n</code></pre> <p>Get the subnetwork mask by (descendingly) ranking parameters based on their scores.</p> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def get_subnet_mask(self, train_loader):\n    \"\"\"Get the subnetwork mask by (descendingly) ranking parameters based on their scores.\"\"\"\n    if self._param_scores is None:\n        self._param_scores = self.compute_param_scores(train_loader)\n    self._check_param_scores()\n\n    idx = torch.argsort(self._param_scores, descending=True)[\n        : self._n_params_subnet\n    ]\n    idx = idx.sort()[0]\n    subnet_mask = torch.zeros_like(self.parameter_vector).bool()\n    subnet_mask[idx] = 1\n    return subnet_mask\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.LargestVarianceDiagLaplaceSubnetMask","title":"LargestVarianceDiagLaplaceSubnetMask","text":"<pre><code>LargestVarianceDiagLaplaceSubnetMask(model: Module, n_params_subnet: int, diag_laplace_model: DiagLaplace)\n</code></pre> <p>               Bases: <code>ScoreBasedSubnetMask</code></p> <p>Subnetwork mask identifying the parameters with the largest marginal variances (estimated using a diagonal Laplace approximation over all model parameters).</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>convert_subnet_mask_to_indices</code>             \u2013              <p>Converts a subnetwork mask into subnetwork indices.</p> </li> <li> <code>select</code>             \u2013              <p>Select the subnetwork mask.</p> </li> <li> <code>get_subnet_mask</code>             \u2013              <p>Get the subnetwork mask by (descendingly) ranking parameters based on their scores.</p> </li> </ul> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    n_params_subnet: int,\n    diag_laplace_model: laplace.baselaplace.DiagLaplace,\n):\n    super().__init__(model, n_params_subnet)\n    self.diag_laplace_model: laplace.baselaplace.DiagLaplace = diag_laplace_model\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.LargestVarianceDiagLaplaceSubnetMask(model)","title":"<code>model</code>","text":"(<code>Module</code>)           \u2013"},{"location":"api_reference/utils/#laplace.utils.LargestVarianceDiagLaplaceSubnetMask(n_params_subnet)","title":"<code>n_params_subnet</code>","text":"(<code>int</code>)           \u2013            <p>number of parameters in the subnetwork (i.e. number of top-scoring parameters to select)</p>"},{"location":"api_reference/utils/#laplace.utils.LargestVarianceDiagLaplaceSubnetMask(diag_laplace_model)","title":"<code>diag_laplace_model</code>","text":"(<code>`laplace.baselaplace.DiagLaplace`</code>)           \u2013            <p>diagonal Laplace model to use for variance estimation</p>"},{"location":"api_reference/utils/#laplace.utils.LargestVarianceDiagLaplaceSubnetMask.convert_subnet_mask_to_indices","title":"convert_subnet_mask_to_indices","text":"<pre><code>convert_subnet_mask_to_indices(subnet_mask: Tensor) -&gt; LongTensor\n</code></pre> <p>Converts a subnetwork mask into subnetwork indices.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>subnet_mask_indices</code> (              <code>LongTensor</code> )          \u2013            <p>a vector of indices of the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>) that define the subnetwork</p> </li> </ul> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def convert_subnet_mask_to_indices(\n    self, subnet_mask: torch.Tensor\n) -&gt; torch.LongTensor:\n    \"\"\"Converts a subnetwork mask into subnetwork indices.\n\n    Parameters\n    ----------\n    subnet_mask : torch.Tensor\n        a binary vector of size (n_params) where 1s locate the subnetwork parameters\n        within the vectorized model parameters\n        (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)\n\n    Returns\n    -------\n    subnet_mask_indices : torch.LongTensor\n        a vector of indices of the vectorized model parameters\n        (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)\n        that define the subnetwork\n    \"\"\"\n    if not isinstance(subnet_mask, torch.Tensor):\n        raise ValueError(\"Subnetwork mask needs to be torch.Tensor!\")\n    elif (\n        subnet_mask.dtype\n        not in [\n            torch.int64,\n            torch.int32,\n            torch.int16,\n            torch.int8,\n            torch.uint8,\n            torch.bool,\n        ]\n        or len(subnet_mask.shape) != 1\n    ):\n        raise ValueError(\n            \"Subnetwork mask needs to be 1-dimensional integral or boolean tensor!\"\n        )\n    elif (\n        len(subnet_mask) != self._n_params\n        or len(subnet_mask[subnet_mask == 0]) + len(subnet_mask[subnet_mask == 1])\n        != self._n_params\n    ):\n        raise ValueError(\n            \"Subnetwork mask needs to be a binary vector of\"\n            \"size (n_params) where 1s locate the subnetwork\"\n            \"parameters within the vectorized model parameters\"\n            \"(i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)!\"\n        )\n\n    subnet_mask_indices = subnet_mask.nonzero(as_tuple=True)[0]\n    return subnet_mask_indices\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.LargestVarianceDiagLaplaceSubnetMask.convert_subnet_mask_to_indices(subnet_mask)","title":"<code>subnet_mask</code>","text":"(<code>Tensor</code>)           \u2013            <p>a binary vector of size (n_params) where 1s locate the subnetwork parameters within the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>)</p>"},{"location":"api_reference/utils/#laplace.utils.LargestVarianceDiagLaplaceSubnetMask.select","title":"select","text":"<pre><code>select(train_loader: DataLoader | None = None) -&gt; LongTensor\n</code></pre> <p>Select the subnetwork mask.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>subnet_mask_indices</code> (              <code>LongTensor</code> )          \u2013            <p>a vector of indices of the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>) that define the subnetwork</p> </li> </ul> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def select(self, train_loader: DataLoader | None = None) -&gt; torch.LongTensor:\n    \"\"\"Select the subnetwork mask.\n\n    Parameters\n    ----------\n    train_loader : torch.data.utils.DataLoader, default=None\n        each iterate is a training batch (X, y);\n        `train_loader.dataset` needs to be set to access \\\\(N\\\\), size of the data set\n\n    Returns\n    -------\n    subnet_mask_indices : torch.LongTensor\n        a vector of indices of the vectorized model parameters\n        (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)\n        that define the subnetwork\n    \"\"\"\n    if self._indices is not None:\n        raise ValueError(\"Subnetwork mask already selected.\")\n\n    subnet_mask = self.get_subnet_mask(train_loader)\n    self._indices = self.convert_subnet_mask_to_indices(subnet_mask)\n    return self._indices\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.LargestVarianceDiagLaplaceSubnetMask.select(train_loader)","title":"<code>train_loader</code>","text":"(<code>DataLoader</code>, default:                   <code>None</code> )           \u2013            <p>each iterate is a training batch (X, y); <code>train_loader.dataset</code> needs to be set to access \\(N\\), size of the data set</p>"},{"location":"api_reference/utils/#laplace.utils.LargestVarianceDiagLaplaceSubnetMask.get_subnet_mask","title":"get_subnet_mask","text":"<pre><code>get_subnet_mask(train_loader)\n</code></pre> <p>Get the subnetwork mask by (descendingly) ranking parameters based on their scores.</p> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def get_subnet_mask(self, train_loader):\n    \"\"\"Get the subnetwork mask by (descendingly) ranking parameters based on their scores.\"\"\"\n    if self._param_scores is None:\n        self._param_scores = self.compute_param_scores(train_loader)\n    self._check_param_scores()\n\n    idx = torch.argsort(self._param_scores, descending=True)[\n        : self._n_params_subnet\n    ]\n    idx = idx.sort()[0]\n    subnet_mask = torch.zeros_like(self.parameter_vector).bool()\n    subnet_mask[idx] = 1\n    return subnet_mask\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.LargestVarianceSWAGSubnetMask","title":"LargestVarianceSWAGSubnetMask","text":"<pre><code>LargestVarianceSWAGSubnetMask(model: Module, n_params_subnet: int, likelihood: Likelihood | str = CLASSIFICATION, swag_n_snapshots: int = 40, swag_snapshot_freq: int = 1, swag_lr: float = 0.01)\n</code></pre> <p>               Bases: <code>ScoreBasedSubnetMask</code></p> <p>Subnetwork mask identifying the parameters with the largest marginal variances (estimated using diagonal SWAG over all model parameters).</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>convert_subnet_mask_to_indices</code>             \u2013              <p>Converts a subnetwork mask into subnetwork indices.</p> </li> <li> <code>select</code>             \u2013              <p>Select the subnetwork mask.</p> </li> <li> <code>get_subnet_mask</code>             \u2013              <p>Get the subnetwork mask by (descendingly) ranking parameters based on their scores.</p> </li> </ul> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    n_params_subnet: int,\n    likelihood: Likelihood | str = Likelihood.CLASSIFICATION,\n    swag_n_snapshots: int = 40,\n    swag_snapshot_freq: int = 1,\n    swag_lr: float = 0.01,\n):\n    if likelihood not in [Likelihood.CLASSIFICATION, Likelihood.REGRESSION]:\n        raise ValueError(\"Only available for classification and regression!\")\n\n    super().__init__(model, n_params_subnet)\n\n    self.likelihood: Likelihood | str = likelihood\n    self.swag_n_snapshots: int = swag_n_snapshots\n    self.swag_snapshot_freq: int = swag_snapshot_freq\n    self.swag_lr: float = swag_lr\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.LargestVarianceSWAGSubnetMask(model)","title":"<code>model</code>","text":"(<code>Module</code>)           \u2013"},{"location":"api_reference/utils/#laplace.utils.LargestVarianceSWAGSubnetMask(n_params_subnet)","title":"<code>n_params_subnet</code>","text":"(<code>int</code>)           \u2013            <p>number of parameters in the subnetwork (i.e. number of top-scoring parameters to select)</p>"},{"location":"api_reference/utils/#laplace.utils.LargestVarianceSWAGSubnetMask(likelihood)","title":"<code>likelihood</code>","text":"(<code>str</code>, default:                   <code>CLASSIFICATION</code> )           \u2013            <p>'classification' or 'regression'</p>"},{"location":"api_reference/utils/#laplace.utils.LargestVarianceSWAGSubnetMask(swag_n_snapshots)","title":"<code>swag_n_snapshots</code>","text":"(<code>int</code>, default:                   <code>40</code> )           \u2013            <p>number of model snapshots to collect for SWAG</p>"},{"location":"api_reference/utils/#laplace.utils.LargestVarianceSWAGSubnetMask(swag_snapshot_freq)","title":"<code>swag_snapshot_freq</code>","text":"(<code>int</code>, default:                   <code>1</code> )           \u2013            <p>SWAG snapshot collection frequency (in epochs)</p>"},{"location":"api_reference/utils/#laplace.utils.LargestVarianceSWAGSubnetMask(swag_lr)","title":"<code>swag_lr</code>","text":"(<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>learning rate for SWAG snapshot collection</p>"},{"location":"api_reference/utils/#laplace.utils.LargestVarianceSWAGSubnetMask.convert_subnet_mask_to_indices","title":"convert_subnet_mask_to_indices","text":"<pre><code>convert_subnet_mask_to_indices(subnet_mask: Tensor) -&gt; LongTensor\n</code></pre> <p>Converts a subnetwork mask into subnetwork indices.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>subnet_mask_indices</code> (              <code>LongTensor</code> )          \u2013            <p>a vector of indices of the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>) that define the subnetwork</p> </li> </ul> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def convert_subnet_mask_to_indices(\n    self, subnet_mask: torch.Tensor\n) -&gt; torch.LongTensor:\n    \"\"\"Converts a subnetwork mask into subnetwork indices.\n\n    Parameters\n    ----------\n    subnet_mask : torch.Tensor\n        a binary vector of size (n_params) where 1s locate the subnetwork parameters\n        within the vectorized model parameters\n        (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)\n\n    Returns\n    -------\n    subnet_mask_indices : torch.LongTensor\n        a vector of indices of the vectorized model parameters\n        (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)\n        that define the subnetwork\n    \"\"\"\n    if not isinstance(subnet_mask, torch.Tensor):\n        raise ValueError(\"Subnetwork mask needs to be torch.Tensor!\")\n    elif (\n        subnet_mask.dtype\n        not in [\n            torch.int64,\n            torch.int32,\n            torch.int16,\n            torch.int8,\n            torch.uint8,\n            torch.bool,\n        ]\n        or len(subnet_mask.shape) != 1\n    ):\n        raise ValueError(\n            \"Subnetwork mask needs to be 1-dimensional integral or boolean tensor!\"\n        )\n    elif (\n        len(subnet_mask) != self._n_params\n        or len(subnet_mask[subnet_mask == 0]) + len(subnet_mask[subnet_mask == 1])\n        != self._n_params\n    ):\n        raise ValueError(\n            \"Subnetwork mask needs to be a binary vector of\"\n            \"size (n_params) where 1s locate the subnetwork\"\n            \"parameters within the vectorized model parameters\"\n            \"(i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)!\"\n        )\n\n    subnet_mask_indices = subnet_mask.nonzero(as_tuple=True)[0]\n    return subnet_mask_indices\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.LargestVarianceSWAGSubnetMask.convert_subnet_mask_to_indices(subnet_mask)","title":"<code>subnet_mask</code>","text":"(<code>Tensor</code>)           \u2013            <p>a binary vector of size (n_params) where 1s locate the subnetwork parameters within the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>)</p>"},{"location":"api_reference/utils/#laplace.utils.LargestVarianceSWAGSubnetMask.select","title":"select","text":"<pre><code>select(train_loader: DataLoader | None = None) -&gt; LongTensor\n</code></pre> <p>Select the subnetwork mask.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>subnet_mask_indices</code> (              <code>LongTensor</code> )          \u2013            <p>a vector of indices of the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>) that define the subnetwork</p> </li> </ul> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def select(self, train_loader: DataLoader | None = None) -&gt; torch.LongTensor:\n    \"\"\"Select the subnetwork mask.\n\n    Parameters\n    ----------\n    train_loader : torch.data.utils.DataLoader, default=None\n        each iterate is a training batch (X, y);\n        `train_loader.dataset` needs to be set to access \\\\(N\\\\), size of the data set\n\n    Returns\n    -------\n    subnet_mask_indices : torch.LongTensor\n        a vector of indices of the vectorized model parameters\n        (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)\n        that define the subnetwork\n    \"\"\"\n    if self._indices is not None:\n        raise ValueError(\"Subnetwork mask already selected.\")\n\n    subnet_mask = self.get_subnet_mask(train_loader)\n    self._indices = self.convert_subnet_mask_to_indices(subnet_mask)\n    return self._indices\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.LargestVarianceSWAGSubnetMask.select(train_loader)","title":"<code>train_loader</code>","text":"(<code>DataLoader</code>, default:                   <code>None</code> )           \u2013            <p>each iterate is a training batch (X, y); <code>train_loader.dataset</code> needs to be set to access \\(N\\), size of the data set</p>"},{"location":"api_reference/utils/#laplace.utils.LargestVarianceSWAGSubnetMask.get_subnet_mask","title":"get_subnet_mask","text":"<pre><code>get_subnet_mask(train_loader)\n</code></pre> <p>Get the subnetwork mask by (descendingly) ranking parameters based on their scores.</p> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def get_subnet_mask(self, train_loader):\n    \"\"\"Get the subnetwork mask by (descendingly) ranking parameters based on their scores.\"\"\"\n    if self._param_scores is None:\n        self._param_scores = self.compute_param_scores(train_loader)\n    self._check_param_scores()\n\n    idx = torch.argsort(self._param_scores, descending=True)[\n        : self._n_params_subnet\n    ]\n    idx = idx.sort()[0]\n    subnet_mask = torch.zeros_like(self.parameter_vector).bool()\n    subnet_mask[idx] = 1\n    return subnet_mask\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.ParamNameSubnetMask","title":"ParamNameSubnetMask","text":"<pre><code>ParamNameSubnetMask(model: Module, parameter_names: list[str])\n</code></pre> <p>               Bases: <code>SubnetMask</code></p> <p>Subnetwork mask corresponding to the specified parameters of the neural network.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>convert_subnet_mask_to_indices</code>             \u2013              <p>Converts a subnetwork mask into subnetwork indices.</p> </li> <li> <code>select</code>             \u2013              <p>Select the subnetwork mask.</p> </li> <li> <code>get_subnet_mask</code>             \u2013              <p>Get the subnetwork mask identifying the specified parameters.</p> </li> </ul> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def __init__(self, model: nn.Module, parameter_names: list[str]) -&gt; None:\n    super().__init__(model)\n    self._parameter_names: list[str] = parameter_names\n    self._n_params_subnet: int | None = None\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.ParamNameSubnetMask(model)","title":"<code>model</code>","text":"(<code>Module</code>)           \u2013"},{"location":"api_reference/utils/#laplace.utils.ParamNameSubnetMask(parameter_names)","title":"<code>parameter_names</code>","text":"(<code>list[str]</code>)           \u2013            <p>list of names of the parameters (as in <code>model.named_parameters()</code>) that define the subnetwork</p>"},{"location":"api_reference/utils/#laplace.utils.ParamNameSubnetMask.convert_subnet_mask_to_indices","title":"convert_subnet_mask_to_indices","text":"<pre><code>convert_subnet_mask_to_indices(subnet_mask: Tensor) -&gt; LongTensor\n</code></pre> <p>Converts a subnetwork mask into subnetwork indices.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>subnet_mask_indices</code> (              <code>LongTensor</code> )          \u2013            <p>a vector of indices of the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>) that define the subnetwork</p> </li> </ul> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def convert_subnet_mask_to_indices(\n    self, subnet_mask: torch.Tensor\n) -&gt; torch.LongTensor:\n    \"\"\"Converts a subnetwork mask into subnetwork indices.\n\n    Parameters\n    ----------\n    subnet_mask : torch.Tensor\n        a binary vector of size (n_params) where 1s locate the subnetwork parameters\n        within the vectorized model parameters\n        (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)\n\n    Returns\n    -------\n    subnet_mask_indices : torch.LongTensor\n        a vector of indices of the vectorized model parameters\n        (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)\n        that define the subnetwork\n    \"\"\"\n    if not isinstance(subnet_mask, torch.Tensor):\n        raise ValueError(\"Subnetwork mask needs to be torch.Tensor!\")\n    elif (\n        subnet_mask.dtype\n        not in [\n            torch.int64,\n            torch.int32,\n            torch.int16,\n            torch.int8,\n            torch.uint8,\n            torch.bool,\n        ]\n        or len(subnet_mask.shape) != 1\n    ):\n        raise ValueError(\n            \"Subnetwork mask needs to be 1-dimensional integral or boolean tensor!\"\n        )\n    elif (\n        len(subnet_mask) != self._n_params\n        or len(subnet_mask[subnet_mask == 0]) + len(subnet_mask[subnet_mask == 1])\n        != self._n_params\n    ):\n        raise ValueError(\n            \"Subnetwork mask needs to be a binary vector of\"\n            \"size (n_params) where 1s locate the subnetwork\"\n            \"parameters within the vectorized model parameters\"\n            \"(i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)!\"\n        )\n\n    subnet_mask_indices = subnet_mask.nonzero(as_tuple=True)[0]\n    return subnet_mask_indices\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.ParamNameSubnetMask.convert_subnet_mask_to_indices(subnet_mask)","title":"<code>subnet_mask</code>","text":"(<code>Tensor</code>)           \u2013            <p>a binary vector of size (n_params) where 1s locate the subnetwork parameters within the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>)</p>"},{"location":"api_reference/utils/#laplace.utils.ParamNameSubnetMask.select","title":"select","text":"<pre><code>select(train_loader: DataLoader | None = None) -&gt; LongTensor\n</code></pre> <p>Select the subnetwork mask.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>subnet_mask_indices</code> (              <code>LongTensor</code> )          \u2013            <p>a vector of indices of the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>) that define the subnetwork</p> </li> </ul> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def select(self, train_loader: DataLoader | None = None) -&gt; torch.LongTensor:\n    \"\"\"Select the subnetwork mask.\n\n    Parameters\n    ----------\n    train_loader : torch.data.utils.DataLoader, default=None\n        each iterate is a training batch (X, y);\n        `train_loader.dataset` needs to be set to access \\\\(N\\\\), size of the data set\n\n    Returns\n    -------\n    subnet_mask_indices : torch.LongTensor\n        a vector of indices of the vectorized model parameters\n        (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)\n        that define the subnetwork\n    \"\"\"\n    if self._indices is not None:\n        raise ValueError(\"Subnetwork mask already selected.\")\n\n    subnet_mask = self.get_subnet_mask(train_loader)\n    self._indices = self.convert_subnet_mask_to_indices(subnet_mask)\n    return self._indices\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.ParamNameSubnetMask.select(train_loader)","title":"<code>train_loader</code>","text":"(<code>DataLoader</code>, default:                   <code>None</code> )           \u2013            <p>each iterate is a training batch (X, y); <code>train_loader.dataset</code> needs to be set to access \\(N\\), size of the data set</p>"},{"location":"api_reference/utils/#laplace.utils.ParamNameSubnetMask.get_subnet_mask","title":"get_subnet_mask","text":"<pre><code>get_subnet_mask(train_loader: DataLoader) -&gt; Tensor\n</code></pre> <p>Get the subnetwork mask identifying the specified parameters.</p> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def get_subnet_mask(self, train_loader: DataLoader) -&gt; torch.Tensor:\n    \"\"\"Get the subnetwork mask identifying the specified parameters.\"\"\"\n\n    self._check_param_names()\n\n    subnet_mask_list = []\n    for name, param in self.model.named_parameters():\n        if name in self._parameter_names:\n            mask_method = torch.ones_like\n        else:\n            mask_method = torch.zeros_like\n        subnet_mask_list.append(mask_method(parameters_to_vector(param)))\n    subnet_mask = torch.cat(subnet_mask_list).bool()\n    return subnet_mask\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.ModuleNameSubnetMask","title":"ModuleNameSubnetMask","text":"<pre><code>ModuleNameSubnetMask(model: Module, module_names: list[str])\n</code></pre> <p>               Bases: <code>SubnetMask</code></p> <p>Subnetwork mask corresponding to the specified modules of the neural network.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>convert_subnet_mask_to_indices</code>             \u2013              <p>Converts a subnetwork mask into subnetwork indices.</p> </li> <li> <code>select</code>             \u2013              <p>Select the subnetwork mask.</p> </li> <li> <code>get_subnet_mask</code>             \u2013              <p>Get the subnetwork mask identifying the specified modules.</p> </li> </ul> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def __init__(self, model: nn.Module, module_names: list[str]):\n    super().__init__(model)\n    self._module_names: list[str] = module_names\n    self._n_params_subnet: int | None = None\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.ModuleNameSubnetMask(model)","title":"<code>model</code>","text":"(<code>Module</code>)           \u2013"},{"location":"api_reference/utils/#laplace.utils.ModuleNameSubnetMask(parameter_names)","title":"<code>parameter_names</code>","text":"\u2013            <p>list of names of the modules (as in <code>model.named_modules()</code>) that define the subnetwork; the modules cannot have children, i.e. need to be leaf modules</p>"},{"location":"api_reference/utils/#laplace.utils.ModuleNameSubnetMask.convert_subnet_mask_to_indices","title":"convert_subnet_mask_to_indices","text":"<pre><code>convert_subnet_mask_to_indices(subnet_mask: Tensor) -&gt; LongTensor\n</code></pre> <p>Converts a subnetwork mask into subnetwork indices.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>subnet_mask_indices</code> (              <code>LongTensor</code> )          \u2013            <p>a vector of indices of the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>) that define the subnetwork</p> </li> </ul> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def convert_subnet_mask_to_indices(\n    self, subnet_mask: torch.Tensor\n) -&gt; torch.LongTensor:\n    \"\"\"Converts a subnetwork mask into subnetwork indices.\n\n    Parameters\n    ----------\n    subnet_mask : torch.Tensor\n        a binary vector of size (n_params) where 1s locate the subnetwork parameters\n        within the vectorized model parameters\n        (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)\n\n    Returns\n    -------\n    subnet_mask_indices : torch.LongTensor\n        a vector of indices of the vectorized model parameters\n        (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)\n        that define the subnetwork\n    \"\"\"\n    if not isinstance(subnet_mask, torch.Tensor):\n        raise ValueError(\"Subnetwork mask needs to be torch.Tensor!\")\n    elif (\n        subnet_mask.dtype\n        not in [\n            torch.int64,\n            torch.int32,\n            torch.int16,\n            torch.int8,\n            torch.uint8,\n            torch.bool,\n        ]\n        or len(subnet_mask.shape) != 1\n    ):\n        raise ValueError(\n            \"Subnetwork mask needs to be 1-dimensional integral or boolean tensor!\"\n        )\n    elif (\n        len(subnet_mask) != self._n_params\n        or len(subnet_mask[subnet_mask == 0]) + len(subnet_mask[subnet_mask == 1])\n        != self._n_params\n    ):\n        raise ValueError(\n            \"Subnetwork mask needs to be a binary vector of\"\n            \"size (n_params) where 1s locate the subnetwork\"\n            \"parameters within the vectorized model parameters\"\n            \"(i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)!\"\n        )\n\n    subnet_mask_indices = subnet_mask.nonzero(as_tuple=True)[0]\n    return subnet_mask_indices\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.ModuleNameSubnetMask.convert_subnet_mask_to_indices(subnet_mask)","title":"<code>subnet_mask</code>","text":"(<code>Tensor</code>)           \u2013            <p>a binary vector of size (n_params) where 1s locate the subnetwork parameters within the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>)</p>"},{"location":"api_reference/utils/#laplace.utils.ModuleNameSubnetMask.select","title":"select","text":"<pre><code>select(train_loader: DataLoader | None = None) -&gt; LongTensor\n</code></pre> <p>Select the subnetwork mask.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>subnet_mask_indices</code> (              <code>LongTensor</code> )          \u2013            <p>a vector of indices of the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>) that define the subnetwork</p> </li> </ul> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def select(self, train_loader: DataLoader | None = None) -&gt; torch.LongTensor:\n    \"\"\"Select the subnetwork mask.\n\n    Parameters\n    ----------\n    train_loader : torch.data.utils.DataLoader, default=None\n        each iterate is a training batch (X, y);\n        `train_loader.dataset` needs to be set to access \\\\(N\\\\), size of the data set\n\n    Returns\n    -------\n    subnet_mask_indices : torch.LongTensor\n        a vector of indices of the vectorized model parameters\n        (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)\n        that define the subnetwork\n    \"\"\"\n    if self._indices is not None:\n        raise ValueError(\"Subnetwork mask already selected.\")\n\n    subnet_mask = self.get_subnet_mask(train_loader)\n    self._indices = self.convert_subnet_mask_to_indices(subnet_mask)\n    return self._indices\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.ModuleNameSubnetMask.select(train_loader)","title":"<code>train_loader</code>","text":"(<code>DataLoader</code>, default:                   <code>None</code> )           \u2013            <p>each iterate is a training batch (X, y); <code>train_loader.dataset</code> needs to be set to access \\(N\\), size of the data set</p>"},{"location":"api_reference/utils/#laplace.utils.ModuleNameSubnetMask.get_subnet_mask","title":"get_subnet_mask","text":"<pre><code>get_subnet_mask(train_loader: DataLoader) -&gt; Tensor\n</code></pre> <p>Get the subnetwork mask identifying the specified modules.</p> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def get_subnet_mask(self, train_loader: DataLoader) -&gt; torch.Tensor:\n    \"\"\"Get the subnetwork mask identifying the specified modules.\"\"\"\n\n    self._check_module_names()\n\n    subnet_mask_list = []\n    for name, module in self.model.named_modules():\n        if len(list(module.children())) &gt; 0 or len(list(module.parameters())) == 0:\n            continue\n        if name in self._module_names:\n            mask_method = torch.ones_like\n        else:\n            mask_method = torch.zeros_like\n        subnet_mask_list.append(\n            mask_method(parameters_to_vector(module.parameters()))\n        )\n    subnet_mask = torch.cat(subnet_mask_list).bool()\n    return subnet_mask\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.LastLayerSubnetMask","title":"LastLayerSubnetMask","text":"<pre><code>LastLayerSubnetMask(model: Module, last_layer_name: str | None = None)\n</code></pre> <p>               Bases: <code>ModuleNameSubnetMask</code></p> <p>Subnetwork mask corresponding to the last layer of the neural network.</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>convert_subnet_mask_to_indices</code>             \u2013              <p>Converts a subnetwork mask into subnetwork indices.</p> </li> <li> <code>select</code>             \u2013              <p>Select the subnetwork mask.</p> </li> <li> <code>get_subnet_mask</code>             \u2013              <p>Get the subnetwork mask identifying the last layer.</p> </li> </ul> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def __init__(self, model: nn.Module, last_layer_name: str | None = None):\n    super().__init__(model, [])\n    self._feature_extractor: FeatureExtractor = FeatureExtractor(\n        self.model, last_layer_name=last_layer_name\n    )\n    self._n_params_subnet: int | None = None\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.LastLayerSubnetMask(model)","title":"<code>model</code>","text":"(<code>Module</code>)           \u2013"},{"location":"api_reference/utils/#laplace.utils.LastLayerSubnetMask(last_layer_name)","title":"<code>last_layer_name</code>","text":"(<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>name of the model's last layer, if None it will be determined automatically</p>"},{"location":"api_reference/utils/#laplace.utils.LastLayerSubnetMask.convert_subnet_mask_to_indices","title":"convert_subnet_mask_to_indices","text":"<pre><code>convert_subnet_mask_to_indices(subnet_mask: Tensor) -&gt; LongTensor\n</code></pre> <p>Converts a subnetwork mask into subnetwork indices.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>subnet_mask_indices</code> (              <code>LongTensor</code> )          \u2013            <p>a vector of indices of the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>) that define the subnetwork</p> </li> </ul> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def convert_subnet_mask_to_indices(\n    self, subnet_mask: torch.Tensor\n) -&gt; torch.LongTensor:\n    \"\"\"Converts a subnetwork mask into subnetwork indices.\n\n    Parameters\n    ----------\n    subnet_mask : torch.Tensor\n        a binary vector of size (n_params) where 1s locate the subnetwork parameters\n        within the vectorized model parameters\n        (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)\n\n    Returns\n    -------\n    subnet_mask_indices : torch.LongTensor\n        a vector of indices of the vectorized model parameters\n        (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)\n        that define the subnetwork\n    \"\"\"\n    if not isinstance(subnet_mask, torch.Tensor):\n        raise ValueError(\"Subnetwork mask needs to be torch.Tensor!\")\n    elif (\n        subnet_mask.dtype\n        not in [\n            torch.int64,\n            torch.int32,\n            torch.int16,\n            torch.int8,\n            torch.uint8,\n            torch.bool,\n        ]\n        or len(subnet_mask.shape) != 1\n    ):\n        raise ValueError(\n            \"Subnetwork mask needs to be 1-dimensional integral or boolean tensor!\"\n        )\n    elif (\n        len(subnet_mask) != self._n_params\n        or len(subnet_mask[subnet_mask == 0]) + len(subnet_mask[subnet_mask == 1])\n        != self._n_params\n    ):\n        raise ValueError(\n            \"Subnetwork mask needs to be a binary vector of\"\n            \"size (n_params) where 1s locate the subnetwork\"\n            \"parameters within the vectorized model parameters\"\n            \"(i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)!\"\n        )\n\n    subnet_mask_indices = subnet_mask.nonzero(as_tuple=True)[0]\n    return subnet_mask_indices\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.LastLayerSubnetMask.convert_subnet_mask_to_indices(subnet_mask)","title":"<code>subnet_mask</code>","text":"(<code>Tensor</code>)           \u2013            <p>a binary vector of size (n_params) where 1s locate the subnetwork parameters within the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>)</p>"},{"location":"api_reference/utils/#laplace.utils.LastLayerSubnetMask.select","title":"select","text":"<pre><code>select(train_loader: DataLoader | None = None) -&gt; LongTensor\n</code></pre> <p>Select the subnetwork mask.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>subnet_mask_indices</code> (              <code>LongTensor</code> )          \u2013            <p>a vector of indices of the vectorized model parameters (i.e. <code>torch.nn.utils.parameters_to_vector(model.parameters())</code>) that define the subnetwork</p> </li> </ul> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def select(self, train_loader: DataLoader | None = None) -&gt; torch.LongTensor:\n    \"\"\"Select the subnetwork mask.\n\n    Parameters\n    ----------\n    train_loader : torch.data.utils.DataLoader, default=None\n        each iterate is a training batch (X, y);\n        `train_loader.dataset` needs to be set to access \\\\(N\\\\), size of the data set\n\n    Returns\n    -------\n    subnet_mask_indices : torch.LongTensor\n        a vector of indices of the vectorized model parameters\n        (i.e. `torch.nn.utils.parameters_to_vector(model.parameters())`)\n        that define the subnetwork\n    \"\"\"\n    if self._indices is not None:\n        raise ValueError(\"Subnetwork mask already selected.\")\n\n    subnet_mask = self.get_subnet_mask(train_loader)\n    self._indices = self.convert_subnet_mask_to_indices(subnet_mask)\n    return self._indices\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.LastLayerSubnetMask.select(train_loader)","title":"<code>train_loader</code>","text":"(<code>DataLoader</code>, default:                   <code>None</code> )           \u2013            <p>each iterate is a training batch (X, y); <code>train_loader.dataset</code> needs to be set to access \\(N\\), size of the data set</p>"},{"location":"api_reference/utils/#laplace.utils.LastLayerSubnetMask.get_subnet_mask","title":"get_subnet_mask","text":"<pre><code>get_subnet_mask(train_loader: DataLoader) -&gt; Tensor\n</code></pre> <p>Get the subnetwork mask identifying the last layer.</p> Source code in <code>laplace/utils/subnetmask.py</code> <pre><code>def get_subnet_mask(self, train_loader: DataLoader) -&gt; torch.Tensor:\n    \"\"\"Get the subnetwork mask identifying the last layer.\"\"\"\n    if train_loader is None:\n        raise ValueError(\"Need to pass train loader for subnet selection.\")\n\n    self._feature_extractor.eval()\n    if self._feature_extractor.last_layer is None:\n        X = next(iter(train_loader))[0]\n        with torch.no_grad():\n            self._feature_extractor.find_last_layer(X[:1].to(self._device))\n    self._module_names = [self._feature_extractor._last_layer_name]\n\n    return super().get_subnet_mask(train_loader)\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.RunningNLLMetric","title":"RunningNLLMetric","text":"<pre><code>RunningNLLMetric(ignore_index: int = -100)\n</code></pre> <p>               Bases: <code>Metric</code></p> <p>NLL metrics that</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>update</code>             \u2013              <p>Parameters</p> </li> </ul> Source code in <code>laplace/utils/metrics.py</code> <pre><code>def __init__(self, ignore_index: int = -100) -&gt; None:\n    super().__init__()\n    self.add_state(\"nll_sum\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n    self.add_state(\n        \"n_valid_labels\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\"\n    )\n    self.ignore_index: int = ignore_index\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.RunningNLLMetric(ignore_index)","title":"<code>ignore_index</code>","text":"(<code>int</code>, default:                   <code>-100</code> )           \u2013            <p>which class label to ignore when computing the NLL loss</p>"},{"location":"api_reference/utils/#laplace.utils.RunningNLLMetric.update","title":"update","text":"<pre><code>update(probs: Tensor, targets: Tensor) -&gt; None\n</code></pre> <p>Parameters:</p> Source code in <code>laplace/utils/metrics.py</code> <pre><code>def update(self, probs: torch.Tensor, targets: torch.Tensor) -&gt; None:\n    \"\"\"\n    Parameters\n    ----------\n    probs: torch.Tensor\n        probability tensor of shape (..., n_classes)\n\n    targets: torch.Tensor\n        integer tensor of shape (...)\n    \"\"\"\n    probs = probs.view(-1, probs.shape[-1])\n    targets = targets.view(-1)\n\n    self.nll_sum += F.nll_loss(\n        probs.log(), targets, ignore_index=self.ignore_index, reduction=\"sum\"\n    )\n    self.n_valid_labels += (targets != self.ignore_index).sum()\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.RunningNLLMetric.update(probs)","title":"<code>probs</code>","text":"(<code>Tensor</code>)           \u2013            <p>probability tensor of shape (..., n_classes)</p>"},{"location":"api_reference/utils/#laplace.utils.RunningNLLMetric.update(targets)","title":"<code>targets</code>","text":"(<code>Tensor</code>)           \u2013            <p>integer tensor of shape (...)</p>"},{"location":"api_reference/utils/#laplace.utils.get_nll","title":"get_nll","text":"<pre><code>get_nll(out_dist: Tensor, targets: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>laplace/utils/utils.py</code> <pre><code>def get_nll(out_dist: torch.Tensor, targets: torch.Tensor) -&gt; torch.Tensor:\n    return F.nll_loss(torch.log(out_dist), targets)\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.validate","title":"validate","text":"<pre><code>validate(laplace: BaseLaplace, val_loader: DataLoader, loss: Metric | Callable[[Tensor, Tensor], Tensor] | Callable[[Tensor, Tensor, Tensor], Tensor], pred_type: PredType | str = GLM, link_approx: LinkApprox | str = PROBIT, n_samples: int = 100, dict_key_y: str = 'labels') -&gt; float\n</code></pre> Source code in <code>laplace/utils/utils.py</code> <pre><code>@torch.no_grad()\ndef validate(\n    laplace: laplace.baselaplace.BaseLaplace,\n    val_loader: DataLoader,\n    loss: torchmetrics.Metric\n    | Callable[[torch.Tensor, torch.Tensor], torch.Tensor]\n    | Callable[[torch.Tensor, torch.Tensor, torch.Tensor], torch.Tensor],\n    pred_type: PredType | str = PredType.GLM,\n    link_approx: LinkApprox | str = LinkApprox.PROBIT,\n    n_samples: int = 100,\n    dict_key_y: str = \"labels\",\n) -&gt; float:\n    laplace.model.eval()\n    assert callable(loss) or isinstance(loss, Metric)\n    is_offline = not isinstance(loss, Metric)\n\n    if is_offline:\n        output_means, output_vars = list(), list()\n        targets = list()\n\n    for data in val_loader:\n        if isinstance(data, MutableMapping):\n            X, y = data, data[dict_key_y]\n        else:\n            X, y = data\n            X = X.to(laplace._device)\n        y = y.to(laplace._device)\n        out = laplace(\n            X,\n            pred_type=pred_type,\n            link_approx=link_approx,\n            n_samples=n_samples,\n            fitting=True,\n        )\n\n        if type(out) is tuple:\n            if is_offline:\n                output_means.append(out[0])\n                output_vars.append(out[1])\n                targets.append(y)\n            else:\n                try:\n                    loss.update(*out, y)\n                except TypeError:  # If the online loss only accepts 2 args\n                    loss.update(out[0], y)\n        else:\n            if is_offline:\n                output_means.append(out)\n                targets.append(y)\n            else:\n                loss.update(out, y)\n\n    if is_offline:\n        if len(output_vars) == 0:\n            preds, targets = torch.cat(output_means, dim=0), torch.cat(targets, dim=0)\n            return loss(preds, targets).item()\n\n        means, variances = torch.cat(output_means, dim=0), torch.cat(output_vars, dim=0)\n        targets = torch.cat(targets, dim=0)\n        return loss(means, variances, targets).item()\n    else:\n        # Aggregate since torchmetrics output n_classes values for the MSE metric\n        return loss.compute().sum().item()\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.parameters_per_layer","title":"parameters_per_layer","text":"<pre><code>parameters_per_layer(model: Module) -&gt; list[int]\n</code></pre> <p>Get number of parameters per layer.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>params_per_layer</code> (              <code>list[int]</code> )          \u2013            </li> </ul> Source code in <code>laplace/utils/utils.py</code> <pre><code>def parameters_per_layer(model: nn.Module) -&gt; list[int]:\n    \"\"\"Get number of parameters per layer.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n\n    Returns\n    -------\n    params_per_layer : list[int]\n    \"\"\"\n    return [np.prod(p.shape) for p in model.parameters()]\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.parameters_per_layer(model)","title":"<code>model</code>","text":"(<code>Module</code>)           \u2013"},{"location":"api_reference/utils/#laplace.utils.invsqrt_precision","title":"invsqrt_precision","text":"<pre><code>invsqrt_precision(M: Tensor) -&gt; Tensor\n</code></pre> <p>Compute <code>M^{-0.5}</code> as a tridiagonal matrix.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>M_invsqrt</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/utils/utils.py</code> <pre><code>def invsqrt_precision(M: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute ``M^{-0.5}`` as a tridiagonal matrix.\n\n    Parameters\n    ----------\n    M : torch.Tensor\n\n    Returns\n    -------\n    M_invsqrt : torch.Tensor\n    \"\"\"\n    return _precision_to_scale_tril(M)\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.invsqrt_precision(M)","title":"<code>M</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/utils/#laplace.utils.kron","title":"kron","text":"<pre><code>kron(t1: Tensor, t2: Tensor) -&gt; Tensor\n</code></pre> <p>Computes the Kronecker product between two tensors.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>kron_product</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/utils/utils.py</code> <pre><code>def kron(t1: torch.Tensor, t2: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Computes the Kronecker product between two tensors.\n\n    Parameters\n    ----------\n    t1 : torch.Tensor\n    t2 : torch.Tensor\n\n    Returns\n    -------\n    kron_product : torch.Tensor\n    \"\"\"\n    t1_height, t1_width = t1.size()\n    t2_height, t2_width = t2.size()\n    out_height = t1_height * t2_height\n    out_width = t1_width * t2_width\n\n    tiled_t2 = t2.repeat(t1_height, t1_width)\n    expanded_t1 = (\n        t1.unsqueeze(2)\n        .unsqueeze(3)\n        .repeat(1, t2_height, t2_width, 1)\n        .view(out_height, out_width)\n    )\n\n    return expanded_t1 * tiled_t2\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.kron(t1)","title":"<code>t1</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/utils/#laplace.utils.kron(t2)","title":"<code>t2</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/utils/#laplace.utils.diagonal_add_scalar","title":"diagonal_add_scalar","text":"<pre><code>diagonal_add_scalar(X: Tensor, value: Tensor) -&gt; Tensor\n</code></pre> <p>Add scalar value <code>value</code> to diagonal of <code>X</code>.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>X_add_scalar</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/utils/utils.py</code> <pre><code>def diagonal_add_scalar(X: torch.Tensor, value: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Add scalar value `value` to diagonal of `X`.\n\n    Parameters\n    ----------\n    X : torch.Tensor\n    value : torch.Tensor or float\n\n    Returns\n    -------\n    X_add_scalar : torch.Tensor\n    \"\"\"\n    indices = torch.LongTensor([[i, i] for i in range(X.shape[0])], device=X.device)\n    values = X.new_ones(X.shape[0]).mul(value)\n    return X.index_put(tuple(indices.t()), values, accumulate=True)\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.diagonal_add_scalar(X)","title":"<code>X</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/utils/#laplace.utils.diagonal_add_scalar(value)","title":"<code>value</code>","text":"(<code>Tensor or float</code>)           \u2013"},{"location":"api_reference/utils/#laplace.utils.symeig","title":"symeig","text":"<pre><code>symeig(M: Tensor) -&gt; tuple[Tensor, Tensor]\n</code></pre> <p>Symetric eigendecomposition avoiding failure cases by adding and removing jitter to the diagonal.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>L</code> (              <code>Tensor</code> )          \u2013            <p>eigenvalues</p> </li> <li> <code>W</code> (              <code>Tensor</code> )          \u2013            <p>eigenvectors</p> </li> </ul> Source code in <code>laplace/utils/utils.py</code> <pre><code>def symeig(M: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Symetric eigendecomposition avoiding failure cases by\n    adding and removing jitter to the diagonal.\n\n    Parameters\n    ----------\n    M : torch.Tensor\n\n    Returns\n    -------\n    L : torch.Tensor\n        eigenvalues\n    W : torch.Tensor\n        eigenvectors\n    \"\"\"\n    try:\n        L, W = torch.linalg.eigh(M, UPLO=\"U\")\n    except RuntimeError:  # did not converge\n        logging.info(\"SYMEIG: adding jitter, did not converge.\")\n        # use W L W^T + I = W (L + I) W^T\n        M = M + torch.eye(M.shape[0], device=M.device, dtype=M.dtype)\n\n        try:\n            L, W = torch.linalg.eigh(M, UPLO=\"U\")\n            L -= 1.0\n        except RuntimeError:\n            stats = f\"diag: {M.diagonal()}, max: {M.abs().max()}, \"\n            stats = stats + f\"min: {M.abs().min()}, mean: {M.abs().mean()}\"\n            logging.info(f\"SYMEIG: adding jitter failed. Stats: {stats}\")\n            exit()\n\n    # eigenvalues of symeig at least 0\n    L = L.clamp(min=0.0)\n    L = torch.nan_to_num(L)\n    W = torch.nan_to_num(W)\n    return L, W\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.symeig(M)","title":"<code>M</code>","text":"(<code>Tensor</code>)           \u2013"},{"location":"api_reference/utils/#laplace.utils.block_diag","title":"block_diag","text":"<pre><code>block_diag(blocks: list[Tensor]) -&gt; Tensor\n</code></pre> <p>Compose block-diagonal matrix of individual blocks.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>M</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/utils/utils.py</code> <pre><code>def block_diag(blocks: list[torch.Tensor]) -&gt; torch.Tensor:\n    \"\"\"Compose block-diagonal matrix of individual blocks.\n\n    Parameters\n    ----------\n    blocks : list[torch.Tensor]\n\n    Returns\n    -------\n    M : torch.Tensor\n    \"\"\"\n    P = sum([b.shape[0] for b in blocks])\n    M = torch.zeros(P, P, dtype=blocks[0].dtype, device=blocks[0].device)\n    p_cur = 0\n    for block in blocks:\n        p_block = block.shape[0]\n        M[p_cur : p_cur + p_block, p_cur : p_cur + p_block] = block\n        p_cur += p_block\n    return M\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.block_diag(blocks)","title":"<code>blocks</code>","text":"(<code>list[Tensor]</code>)           \u2013"},{"location":"api_reference/utils/#laplace.utils.normal_samples","title":"normal_samples","text":"<pre><code>normal_samples(mean: Tensor, var: Tensor, n_samples: int, generator: Generator | None = None) -&gt; Tensor\n</code></pre> <p>Produce samples from a batch of Normal distributions either parameterized by a diagonal or full covariance given by <code>var</code>.</p> <p>Parameters:</p> Source code in <code>laplace/utils/utils.py</code> <pre><code>def normal_samples(\n    mean: torch.Tensor,\n    var: torch.Tensor,\n    n_samples: int,\n    generator: torch.Generator | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Produce samples from a batch of Normal distributions either parameterized\n    by a diagonal or full covariance given by `var`.\n\n    Parameters\n    ----------\n    mean : torch.Tensor\n        `(batch_size, output_dim)`\n    var : torch.Tensor\n        (co)variance of the Normal distribution\n        `(batch_size, output_dim, output_dim)` or `(batch_size, output_dim)`\n    generator : torch.Generator\n        random number generator\n    \"\"\"\n    assert mean.ndim == 2, \"Invalid input shape of mean, should be 2-dimensional.\"\n    _, output_dim = mean.shape\n    randn_samples = torch.randn(\n        (output_dim, n_samples),\n        device=mean.device,\n        dtype=mean.dtype,\n        generator=generator,\n    )\n\n    if mean.shape == var.shape:\n        # diagonal covariance\n        scaled_samples = var.sqrt().unsqueeze(-1) * randn_samples.unsqueeze(0)\n        return (mean.unsqueeze(-1) + scaled_samples).permute((2, 0, 1))\n    elif mean.shape == var.shape[:2] and var.shape[-1] == mean.shape[1]:\n        # full covariance\n        scale = torch.linalg.cholesky(var)\n        scaled_samples = torch.matmul(\n            scale, randn_samples.unsqueeze(0)\n        )  # expand batch dim\n        return (mean.unsqueeze(-1) + scaled_samples).permute((2, 0, 1))\n    else:\n        raise ValueError(\"Invalid input shapes.\")\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.normal_samples(mean)","title":"<code>mean</code>","text":"(<code>Tensor</code>)           \u2013            <p><code>(batch_size, output_dim)</code></p>"},{"location":"api_reference/utils/#laplace.utils.normal_samples(var)","title":"<code>var</code>","text":"(<code>Tensor</code>)           \u2013            <p>(co)variance of the Normal distribution <code>(batch_size, output_dim, output_dim)</code> or <code>(batch_size, output_dim)</code></p>"},{"location":"api_reference/utils/#laplace.utils.normal_samples(generator)","title":"<code>generator</code>","text":"(<code>Generator</code>, default:                   <code>None</code> )           \u2013            <p>random number generator</p>"},{"location":"api_reference/utils/#laplace.utils._is_batchnorm","title":"_is_batchnorm","text":"<pre><code>_is_batchnorm(module: Module) -&gt; bool\n</code></pre> Source code in <code>laplace/utils/utils.py</code> <pre><code>def _is_batchnorm(module: nn.Module) -&gt; bool:\n    if isinstance(module, (BatchNorm1d, BatchNorm2d, BatchNorm3d)):\n        return True\n    return False\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils._is_valid_scalar","title":"_is_valid_scalar","text":"<pre><code>_is_valid_scalar(scalar: float | int | Tensor) -&gt; bool\n</code></pre> Source code in <code>laplace/utils/utils.py</code> <pre><code>def _is_valid_scalar(scalar: float | int | torch.Tensor) -&gt; bool:\n    if np.isscalar(scalar) and np.isreal(scalar):\n        return True\n    elif torch.is_tensor(scalar) and scalar.ndim &lt;= 1:\n        if scalar.ndim == 1 and len(scalar) != 1:\n            return False\n        return True\n    return False\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.expand_prior_precision","title":"expand_prior_precision","text":"<pre><code>expand_prior_precision(prior_prec: Tensor, model: Module) -&gt; Tensor\n</code></pre> <p>Expand prior precision to match the shape of the model parameters.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>expanded_prior_prec</code> (              <code>Tensor</code> )          \u2013            <p>expanded prior precision has the same shape as model parameters</p> </li> </ul> Source code in <code>laplace/utils/utils.py</code> <pre><code>def expand_prior_precision(prior_prec: torch.Tensor, model: nn.Module) -&gt; torch.Tensor:\n    \"\"\"Expand prior precision to match the shape of the model parameters.\n\n    Parameters\n    ----------\n    prior_prec : torch.Tensor 1-dimensional\n        prior precision\n    model : torch.nn.Module\n        torch model with parameters that are regularized by prior_prec\n\n    Returns\n    -------\n    expanded_prior_prec : torch.Tensor\n        expanded prior precision has the same shape as model parameters\n    \"\"\"\n    trainable_params = [p for p in model.parameters() if p.requires_grad]\n    theta = parameters_to_vector(trainable_params)\n    device, dtype, P = theta.device, theta.dtype, len(theta)\n    assert prior_prec.ndim == 1\n    if len(prior_prec) == 1:  # scalar\n        return torch.ones(P, device=device, dtype=dtype) * prior_prec\n    elif len(prior_prec) == P:  # full diagonal\n        return prior_prec.to(device)\n    else:\n        return torch.cat(\n            [\n                delta * torch.ones_like(m).flatten()\n                for delta, m in zip(prior_prec, trainable_params)\n            ]\n        )\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.expand_prior_precision(prior_prec)","title":"<code>prior_prec</code>","text":"(<code>torch.Tensor 1-dimensional</code>)           \u2013            <p>prior precision</p>"},{"location":"api_reference/utils/#laplace.utils.expand_prior_precision(model)","title":"<code>model</code>","text":"(<code>Module</code>)           \u2013            <p>torch model with parameters that are regularized by prior_prec</p>"},{"location":"api_reference/utils/#laplace.utils.fix_prior_prec_structure","title":"fix_prior_prec_structure","text":"<pre><code>fix_prior_prec_structure(prior_prec_init: Tensor, prior_structure: PriorStructure | str, n_layers: int, n_params: int, device: device, dtype: dtype) -&gt; Tensor\n</code></pre> <p>Create a tensor of prior precision with the correct shape, depending on the choice of the prior structure type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>correct_prior_precision</code> (              <code>Tensor</code> )          \u2013            </li> </ul> Source code in <code>laplace/utils/utils.py</code> <pre><code>def fix_prior_prec_structure(\n    prior_prec_init: torch.Tensor,\n    prior_structure: PriorStructure | str,\n    n_layers: int,\n    n_params: int,\n    device: torch.device,\n    dtype: torch.dtype,\n) -&gt; torch.Tensor:\n    \"\"\"Create a tensor of prior precision with the correct shape, depending on the\n    choice of the prior structure type.\n\n    Parameters\n    ----------\n    prior_prec_init: torch.Tensor\n        the initial prior precision tensor (could be scalar)\n    prior_structure: PriorStructure | str\n        the choice of the prior structure type\n    n_layers: int\n    n_params: int\n    device: torch.device\n    dtype: torch.dtype\n\n    Returns\n    -------\n    correct_prior_precision: torch.Tensor\n    \"\"\"\n    if prior_structure == PriorStructure.SCALAR:\n        prior_prec_init = torch.full((1,), prior_prec_init, device=device, dtype=dtype)\n    elif prior_structure == PriorStructure.LAYERWISE:\n        prior_prec_init = torch.full(\n            (n_layers,), prior_prec_init, device=device, dtype=dtype\n        )\n    elif prior_structure == PriorStructure.DIAG:\n        prior_prec_init = torch.full(\n            (n_params,), prior_prec_init, device=device, dtype=dtype\n        )\n    else:\n        raise ValueError(f\"Invalid prior structure {prior_structure}.\")\n    return prior_prec_init\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.fix_prior_prec_structure(prior_prec_init)","title":"<code>prior_prec_init</code>","text":"(<code>Tensor</code>)           \u2013            <p>the initial prior precision tensor (could be scalar)</p>"},{"location":"api_reference/utils/#laplace.utils.fix_prior_prec_structure(prior_structure)","title":"<code>prior_structure</code>","text":"(<code>PriorStructure | str</code>)           \u2013            <p>the choice of the prior structure type</p>"},{"location":"api_reference/utils/#laplace.utils.fix_prior_prec_structure(n_layers)","title":"<code>n_layers</code>","text":"(<code>int</code>)           \u2013"},{"location":"api_reference/utils/#laplace.utils.fix_prior_prec_structure(n_params)","title":"<code>n_params</code>","text":"(<code>int</code>)           \u2013"},{"location":"api_reference/utils/#laplace.utils.fix_prior_prec_structure(device)","title":"<code>device</code>","text":"(<code>device</code>)           \u2013"},{"location":"api_reference/utils/#laplace.utils.fix_prior_prec_structure(dtype)","title":"<code>dtype</code>","text":"(<code>dtype</code>)           \u2013"},{"location":"api_reference/utils/#laplace.utils.fit_diagonal_swag_var","title":"fit_diagonal_swag_var","text":"<pre><code>fit_diagonal_swag_var(model: Module, train_loader: DataLoader, criterion: CrossEntropyLoss | MSELoss, n_snapshots_total: int = 40, snapshot_freq: int = 1, lr: float = 0.01, momentum: float = 0.9, weight_decay: float = 0.0003, min_var: float = 1e-30) -&gt; Tensor\n</code></pre> <p>Fit diagonal SWAG [1], which estimates marginal variances of model parameters by computing the first and second moment of SGD iterates with a large learning rate.</p> <p>Implementation partly adapted from: - https://github.com/wjmaddox/swa_gaussian/blob/master/swag/posteriors/swag.py - https://github.com/wjmaddox/swa_gaussian/blob/master/experiments/train/run_swag.py</p> References <p>[1] Maddox, W., Garipov, T., Izmailov, P., Vetrov, D., Wilson, AG. A Simple Baseline for Bayesian Uncertainty in Deep Learning. NeurIPS 2019.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>param_variances</code> (              <code>Tensor</code> )          \u2013            <p>vector of marginal variances for each model parameter</p> </li> </ul> Source code in <code>laplace/utils/swag.py</code> <pre><code>def fit_diagonal_swag_var(\n    model: nn.Module,\n    train_loader: DataLoader,\n    criterion: nn.CrossEntropyLoss | nn.MSELoss,\n    n_snapshots_total: int = 40,\n    snapshot_freq: int = 1,\n    lr: float = 0.01,\n    momentum: float = 0.9,\n    weight_decay: float = 3e-4,\n    min_var: float = 1e-30,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Fit diagonal SWAG [1], which estimates marginal variances of model parameters by\n    computing the first and second moment of SGD iterates with a large learning rate.\n\n    Implementation partly adapted from:\n    - https://github.com/wjmaddox/swa_gaussian/blob/master/swag/posteriors/swag.py\n    - https://github.com/wjmaddox/swa_gaussian/blob/master/experiments/train/run_swag.py\n\n    References\n    ----------\n    [1] Maddox, W., Garipov, T., Izmailov, P., Vetrov, D., Wilson, AG.\n    [*A Simple Baseline for Bayesian Uncertainty in Deep Learning*](https://arxiv.org/abs/1902.02476).\n    NeurIPS 2019.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n    train_loader : torch.data.utils.DataLoader\n        training data loader to use for snapshot collection\n    criterion : torch.nn.CrossEntropyLoss or torch.nn.MSELoss\n        loss function to use for snapshot collection\n    n_snapshots_total : int\n        total number of model snapshots to collect\n    snapshot_freq : int\n        snapshot collection frequency (in epochs)\n    lr : float\n        SGD learning rate for collecting snapshots\n    momentum : float\n        SGD momentum\n    weight_decay : float\n        SGD weight decay\n    min_var : float\n        minimum parameter variance to clamp to (for numerical stability)\n\n    Returns\n    -------\n    param_variances : torch.Tensor\n        vector of marginal variances for each model parameter\n    \"\"\"\n\n    # create a copy of the model to avoid undesired changes to the original model parameters\n    _model: nn.Module = deepcopy(model)\n    _model.train()\n    device: torch.device = next(_model.parameters()).device\n\n    # initialize running estimates of first and second moment of model parameters\n    mean: torch.Tensor = torch.zeros_like(_param_vector(_model))\n    sq_mean: torch.Tensor = torch.zeros_like(_param_vector(_model))\n    n_snapshots: int = 0\n\n    # run SGD to collect model snapshots\n    optimizer: Optimizer = torch.optim.SGD(\n        _model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay\n    )\n    n_epochs: int = snapshot_freq * n_snapshots_total\n\n    for epoch in range(n_epochs):\n        for inputs, targets in train_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad()\n            loss = criterion(_model(inputs), targets)\n            loss.backward()\n            optimizer.step()\n\n        if epoch % snapshot_freq == 0:\n            # update running estimates of first and second moment of model parameters\n            old_fac, new_fac = n_snapshots / (n_snapshots + 1), 1 / (n_snapshots + 1)\n            mean = mean * old_fac + _param_vector(_model) * new_fac\n            sq_mean = sq_mean * old_fac + _param_vector(_model) ** 2 * new_fac\n            n_snapshots += 1\n\n    # compute marginal parameter variances, Var[P] = E[P^2] - E[P]^2\n    param_variances: torch.Tensor = torch.clamp(sq_mean - mean**2, min_var)\n    return param_variances\n</code></pre>"},{"location":"api_reference/utils/#laplace.utils.fit_diagonal_swag_var(model)","title":"<code>model</code>","text":"(<code>Module</code>)           \u2013"},{"location":"api_reference/utils/#laplace.utils.fit_diagonal_swag_var(train_loader)","title":"<code>train_loader</code>","text":"(<code>DataLoader</code>)           \u2013            <p>training data loader to use for snapshot collection</p>"},{"location":"api_reference/utils/#laplace.utils.fit_diagonal_swag_var(criterion)","title":"<code>criterion</code>","text":"(<code>CrossEntropyLoss or MSELoss</code>)           \u2013            <p>loss function to use for snapshot collection</p>"},{"location":"api_reference/utils/#laplace.utils.fit_diagonal_swag_var(n_snapshots_total)","title":"<code>n_snapshots_total</code>","text":"(<code>int</code>, default:                   <code>40</code> )           \u2013            <p>total number of model snapshots to collect</p>"},{"location":"api_reference/utils/#laplace.utils.fit_diagonal_swag_var(snapshot_freq)","title":"<code>snapshot_freq</code>","text":"(<code>int</code>, default:                   <code>1</code> )           \u2013            <p>snapshot collection frequency (in epochs)</p>"},{"location":"api_reference/utils/#laplace.utils.fit_diagonal_swag_var(lr)","title":"<code>lr</code>","text":"(<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>SGD learning rate for collecting snapshots</p>"},{"location":"api_reference/utils/#laplace.utils.fit_diagonal_swag_var(momentum)","title":"<code>momentum</code>","text":"(<code>float</code>, default:                   <code>0.9</code> )           \u2013            <p>SGD momentum</p>"},{"location":"api_reference/utils/#laplace.utils.fit_diagonal_swag_var(weight_decay)","title":"<code>weight_decay</code>","text":"(<code>float</code>, default:                   <code>0.0003</code> )           \u2013            <p>SGD weight decay</p>"},{"location":"api_reference/utils/#laplace.utils.fit_diagonal_swag_var(min_var)","title":"<code>min_var</code>","text":"(<code>float</code>, default:                   <code>1e-30</code> )           \u2013            <p>minimum parameter variance to clamp to (for numerical stability)</p>"}]}
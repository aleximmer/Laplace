
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="wiseodd">
      
      
        <link rel="canonical" href="https://aleximmer.github.io/Laplace/api_reference/functionallaplace/">
      
      
        <link rel="prev" href="../parametriclaplace/">
      
      
        <link rel="next" href="../lllaplace/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.46">
    
    
      
        <title>Functional Laplace - laplace-torch</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.6f8fc17f.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../css/mkdocstrings.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#laplace.baselaplace" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="laplace-torch" class="md-header__button md-logo" aria-label="laplace-torch" data-md-component="logo">
      
  <img src="../../assets/laplace_logo_inv.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            laplace-torch
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Functional Laplace
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/aleximmer/laplace" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    laplace
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="laplace-torch" class="md-nav__button md-logo" aria-label="laplace-torch" data-md-component="logo">
      
  <img src="../../assets/laplace_logo_inv.png" alt="logo">

    </a>
    laplace-torch
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/aleximmer/laplace" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    laplace
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../devs_guide/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Developer's Guide
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../regression_example/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Example: Regression
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../calibration_example/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Example: Calibration
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../calibration_gp_example/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Example: GP Inference
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../huggingface_example/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Example: Huggingface LLMs
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../reward_modeling_example/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Example: Reward Modeling
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" checked>
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    API Reference
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            API Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../laplace/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Laplace Frontend
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../enums/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Laplace Options
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../baselaplace/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Base Laplace
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../parametriclaplace/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Parametric Laplace
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Functional Laplace
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Functional Laplace
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;FunctionalLaplace
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" FunctionalLaplace">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace(num_data)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;num_data
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace(diagonal_kernel)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;diagonal_kernel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace(See)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;See
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.log_likelihood" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-attribute"></code>&nbsp;log_likelihood
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.prior_precision_diag" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-attribute"></code>&nbsp;prior_precision_diag
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.log_det_ratio" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-attribute"></code>&nbsp;log_det_ratio
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.scatter" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-attribute"></code>&nbsp;scatter
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_glm_forward_call
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" _glm_forward_call">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call(x)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;x
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call(likelihood)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;likelihood
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call(link_approx)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;link_approx
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call(joint)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;joint
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call(n_samples)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;n_samples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call(diagonal_output)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;diagonal_output
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_functional_samples" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_glm_functional_samples
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" _glm_functional_samples">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_functional_samples(f_mu)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;f_mu
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_functional_samples(f_var)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;f_var
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_functional_samples(n_samples)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;n_samples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_functional_samples(diagonal_output)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;diagonal_output
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_functional_samples(generator)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;generator
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_glm_predictive_samples
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" _glm_predictive_samples">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(f_mu)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;f_mu
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(f_var)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;f_var
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(n_samples)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;n_samples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(diagonal_output)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;diagonal_output
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(generator)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;generator
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._check_prior_precision" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_check_prior_precision
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._init_K_MM" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_init_K_MM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._init_Sigma_inv" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_init_Sigma_inv
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._store_K_batch" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_store_K_batch
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._build_L" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_build_L
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" _build_L">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._build_L(lambdas)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;lambdas
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._build_Sigma_inv" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_build_Sigma_inv
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._get_SoD_data_loader" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_get_SoD_data_loader
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.fit" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;fit
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" fit">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.fit(train_loader)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;train_loader
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.fit(progress_bar)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;progress_bar
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__call__
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" __call__">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.__call__(x)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;x
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.__call__(pred_type)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;pred_type
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.__call__(link_approx)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;link_approx
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.__call__(joint)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;joint
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.__call__(n_samples)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;n_samples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.__call__(diagonal_output)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;diagonal_output
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.__call__(generator)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;generator
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.__call__(fitting)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;fitting
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.functional_samples" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;functional_samples
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" functional_samples">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.functional_samples(x)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;x
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.functional_samples(pred_type)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;pred_type
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.functional_samples(n_samples)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;n_samples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.functional_samples(diagonal_output)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;diagonal_output
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.functional_samples(generator)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;generator
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.predictive_samples" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;predictive_samples
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" predictive_samples">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.predictive_samples(x)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;x
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.predictive_samples(pred_type)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;pred_type
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.predictive_samples(n_samples)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;n_samples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.predictive_samples(diagonal_output)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;diagonal_output
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.predictive_samples(generator)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;generator
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.functional_variance" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;functional_variance
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" functional_variance">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.functional_variance(Js_star)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;Js_star
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.functional_covariance" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;functional_covariance
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" functional_covariance">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.functional_covariance(Js_star)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;Js_star
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._build_K_star_M" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_build_K_star_M
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" _build_K_star_M">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._build_K_star_M(K_M_star)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;K_M_star
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._build_K_star_M(joint)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;joint
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.optimize_prior_precision" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;optimize_prior_precision
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._kernel_batch" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_kernel_batch
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" _kernel_batch">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._kernel_batch(jacobians)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;jacobians
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._kernel_batch(batch)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;batch
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._kernel_star" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_kernel_star
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" _kernel_star">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._kernel_star(jacobians)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;jacobians
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._kernel_batch_star" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_kernel_batch_star
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" _kernel_batch_star">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._kernel_batch_star(jacobians)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;jacobians
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._kernel_batch_star(batch)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;batch
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._jacobians" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_jacobians
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._mean_scatter_term_batch" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_mean_scatter_term_batch
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" _mean_scatter_term_batch">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._mean_scatter_term_batch(Js)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;Js
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._mean_scatter_term_batch(f)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;f
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._mean_scatter_term_batch(y)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;y
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.log_marginal_likelihood" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;log_marginal_likelihood
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" log_marginal_likelihood">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.log_marginal_likelihood(prior_precision)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;prior_precision
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.log_marginal_likelihood(sigma_noise)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;sigma_noise
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lllaplace/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Last-Layer Laplace
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../subnetlaplace/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Subnet Laplace
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../curvatures/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Curvatures
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../marglik_training/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Marglik Training Utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utilities
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-class"></code>&nbsp;FunctionalLaplace
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" FunctionalLaplace">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace(num_data)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;num_data
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace(diagonal_kernel)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;diagonal_kernel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace(See)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;See
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.log_likelihood" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-attribute"></code>&nbsp;log_likelihood
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.prior_precision_diag" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-attribute"></code>&nbsp;prior_precision_diag
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.log_det_ratio" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-attribute"></code>&nbsp;log_det_ratio
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.scatter" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-attribute"></code>&nbsp;scatter
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_glm_forward_call
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" _glm_forward_call">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call(x)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;x
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call(likelihood)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;likelihood
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call(link_approx)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;link_approx
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call(joint)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;joint
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call(n_samples)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;n_samples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call(diagonal_output)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;diagonal_output
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_functional_samples" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_glm_functional_samples
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" _glm_functional_samples">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_functional_samples(f_mu)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;f_mu
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_functional_samples(f_var)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;f_var
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_functional_samples(n_samples)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;n_samples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_functional_samples(diagonal_output)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;diagonal_output
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_functional_samples(generator)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;generator
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_glm_predictive_samples
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" _glm_predictive_samples">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(f_mu)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;f_mu
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(f_var)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;f_var
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(n_samples)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;n_samples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(diagonal_output)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;diagonal_output
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(generator)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;generator
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._check_prior_precision" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_check_prior_precision
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._init_K_MM" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_init_K_MM
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._init_Sigma_inv" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_init_Sigma_inv
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._store_K_batch" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_store_K_batch
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._build_L" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_build_L
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" _build_L">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._build_L(lambdas)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;lambdas
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._build_Sigma_inv" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_build_Sigma_inv
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._get_SoD_data_loader" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_get_SoD_data_loader
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.fit" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;fit
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" fit">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.fit(train_loader)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;train_loader
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.fit(progress_bar)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;progress_bar
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;__call__
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" __call__">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.__call__(x)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;x
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.__call__(pred_type)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;pred_type
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.__call__(link_approx)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;link_approx
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.__call__(joint)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;joint
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.__call__(n_samples)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;n_samples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.__call__(diagonal_output)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;diagonal_output
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.__call__(generator)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;generator
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.__call__(fitting)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;fitting
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.functional_samples" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;functional_samples
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" functional_samples">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.functional_samples(x)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;x
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.functional_samples(pred_type)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;pred_type
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.functional_samples(n_samples)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;n_samples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.functional_samples(diagonal_output)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;diagonal_output
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.functional_samples(generator)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;generator
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.predictive_samples" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;predictive_samples
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" predictive_samples">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.predictive_samples(x)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;x
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.predictive_samples(pred_type)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;pred_type
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.predictive_samples(n_samples)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;n_samples
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.predictive_samples(diagonal_output)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;diagonal_output
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.predictive_samples(generator)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;generator
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.functional_variance" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;functional_variance
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" functional_variance">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.functional_variance(Js_star)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;Js_star
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.functional_covariance" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;functional_covariance
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" functional_covariance">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.functional_covariance(Js_star)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;Js_star
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._build_K_star_M" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_build_K_star_M
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" _build_K_star_M">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._build_K_star_M(K_M_star)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;K_M_star
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._build_K_star_M(joint)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;joint
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.optimize_prior_precision" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;optimize_prior_precision
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._kernel_batch" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_kernel_batch
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" _kernel_batch">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._kernel_batch(jacobians)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;jacobians
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._kernel_batch(batch)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;batch
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._kernel_star" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_kernel_star
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" _kernel_star">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._kernel_star(jacobians)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;jacobians
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._kernel_batch_star" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_kernel_batch_star
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" _kernel_batch_star">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._kernel_batch_star(jacobians)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;jacobians
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._kernel_batch_star(batch)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;batch
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._jacobians" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_jacobians
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._mean_scatter_term_batch" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;_mean_scatter_term_batch
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" _mean_scatter_term_batch">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._mean_scatter_term_batch(Js)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;Js
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._mean_scatter_term_batch(f)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;f
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace._mean_scatter_term_batch(y)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;y
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.log_marginal_likelihood" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-method"></code>&nbsp;log_marginal_likelihood
    </span>
  </a>
  
    <nav class="md-nav" aria-label=" log_marginal_likelihood">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.log_marginal_likelihood(prior_precision)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;prior_precision
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#laplace.baselaplace.FunctionalLaplace.log_marginal_likelihood(sigma_noise)" class="md-nav__link">
    <span class="md-ellipsis">
      <code class="doc-symbol doc-symbol-toc doc-symbol-parameter"></code>&nbsp;sigma_noise
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<div class="doc doc-object doc-module">



<h1 id="laplace.baselaplace" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-module"></code>            <span class="doc doc-object-name doc-module-name">laplace.baselaplace</span>


<a href="#laplace.baselaplace" class="headerlink" title="Permanent link">#</a></h1>

    <div class="doc doc-contents first">







<p><span class="doc-section-title">Classes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><a class="autorefs autorefs-internal" title="laplace.baselaplace.FunctionalLaplace" href="#laplace.baselaplace.FunctionalLaplace">FunctionalLaplace</a></code></b>
          –
          <div class="doc-md-description">
            <p>Applying the GGN (Generalized Gauss-Newton) approximation for the Hessian in the Laplace approximation of the posterior</p>
          </div>
        </li>
    </ul>







  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="laplace.baselaplace.FunctionalLaplace" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>            <span class="doc doc-object-name doc-class-name">FunctionalLaplace</span>


<a href="#laplace.baselaplace.FunctionalLaplace" class="headerlink" title="Permanent link">#</a></h2>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nf">FunctionalLaplace</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n"><span title="torch.nn.Module">Module</span></span><span class="p">,</span> <span class="n">likelihood</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="laplace.utils.enums.Likelihood" href="../enums/#laplace.utils.enums.Likelihood">Likelihood</a></span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span><span class="p">,</span> <span class="n">n_subset</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span> <span class="n">sigma_noise</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span> <span class="o">|</span> <span class="n"><span title="torch.Tensor">Tensor</span></span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">prior_precision</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span> <span class="o">|</span> <span class="n"><span title="torch.Tensor">Tensor</span></span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">prior_mean</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span> <span class="o">|</span> <span class="n"><span title="torch.Tensor">Tensor</span></span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">temperature</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">enable_backprop</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">dict_key_x</span><span class="o">=</span><span class="s1">&#39;input_ids&#39;</span><span class="p">,</span> <span class="n">dict_key_y</span><span class="o">=</span><span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="n">backend</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#type">type</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-internal" title="laplace.curvature.curvature.CurvatureInterface" href="../curvatures/#laplace.curvature.CurvatureInterface">CurvatureInterface</a></span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n"><a class="autorefs autorefs-internal" title="laplace.curvature.backpack.BackPackGGN" href="../curvatures/#laplace.curvature.BackPackGGN">BackPackGGN</a></span><span class="p">,</span> <span class="n">backend_kwargs</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#dict">dict</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Any" href="https://docs.python.org/3/library/typing.html#typing.Any">Any</a></span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">independent_outputs</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="laplace.baselaplace.BaseLaplace" href="../baselaplace/#laplace.baselaplace.BaseLaplace">BaseLaplace</a></code></p>


        <p>Applying the GGN (Generalized Gauss-Newton) approximation for the Hessian in the Laplace approximation of the posterior
turns the underlying probabilistic model from a BNN into a GLM (generalized linear model).
This GLM (in the weight space) is equivalent to a GP (in the function space), see
<a href="https://arxiv.org/abs/1906.01930">Approximate Inference Turns Deep Networks into Gaussian Processes (Khan et al., 2019)</a></p>
<p>This class implements the (approximate) GP inference through which
we obtain the desired quantities (posterior predictive, marginal log-likelihood).
See <a href="https://arxiv.org/abs/2008.08400">Improving predictions of Bayesian neural nets via local linearization (Immer et al., 2021)</a>
for more details.</p>
<p>Note that for <code>likelihood='classification'</code>, we approximate <span class="arithmatex">\( L_{NN} \)</span> with a diagonal matrix
( <span class="arithmatex">\( L_{NN} \)</span> is a block-diagonal matrix, where blocks represent Hessians of per-data-point log-likelihood w.r.t.
neural network output <span class="arithmatex">\( f \)</span>, See Appendix <a href="https://arxiv.org/abs/2008.08400">A.2.1</a> for exact definition). We
resort to such an approximation because of the (possible) errors found in Laplace approximation for
multiclass GP classification in Chapter 3.5 of <a href="http://www.gaussianprocess.org/gpml/">R&amp;W 2006 GP book</a>,
see the question
<a href="https://stats.stackexchange.com/questions/555183/gaussian-processes-multi-class-laplace-approximation">here</a>
for more details. Alternatively, one could also resort to <em>one-vs-one</em> or <em>one-vs-rest</em> implementations
for multiclass classification, however, that is not (yet) supported here.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<h3 id="laplace.baselaplace.FunctionalLaplace(num_data)" class="doc doc-heading doc-heading-parameter">              <b><code>num_data</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace(num_data)" class="headerlink" title="Permanent link">#</a></h3>              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>)
          –
          <div class="doc-md-description">
            <p>number of data points for Subset-of-Data (SOD) approximate GP inference.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h3 id="laplace.baselaplace.FunctionalLaplace(diagonal_kernel)" class="doc doc-heading doc-heading-parameter">              <b><code>diagonal_kernel</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace(diagonal_kernel)" class="headerlink" title="Permanent link">#</a></h3>              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>)
          –
          <div class="doc-md-description">
            <p>GP kernel here is product of Jacobians, which results in a <span class="arithmatex">\( C \times C\)</span> matrix where <span class="arithmatex">\(C\)</span> is the output
dimension. If <code>diagonal_kernel=True</code>, only a diagonal of a GP kernel is used. This is (somewhat) equivalent to
assuming independent GPs across output channels.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h3 id="laplace.baselaplace.FunctionalLaplace(See)" class="doc doc-heading doc-heading-parameter">              <b><code>See</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace(See)" class="headerlink" title="Permanent link">#</a></h3>          –
          <div class="doc-md-description">
            
          </div>
        </li>
    </ul>









<p><span class="doc-section-title">Methods:</span></p>
    <ul>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="laplace.baselaplace.FunctionalLaplace.fit" href="#laplace.baselaplace.FunctionalLaplace.fit">fit</a></code></b>
            –
            <div class="doc-md-description">
              <p>Fit the Laplace approximation of a GP posterior.</p>
            </div>
          </li>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="laplace.baselaplace.FunctionalLaplace.__call__" href="#laplace.baselaplace.FunctionalLaplace.__call__">__call__</a></code></b>
            –
            <div class="doc-md-description">
              <p>Compute the posterior predictive on input data <code>x</code>.</p>
            </div>
          </li>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="laplace.baselaplace.FunctionalLaplace.functional_samples" href="#laplace.baselaplace.FunctionalLaplace.functional_samples">functional_samples</a></code></b>
            –
            <div class="doc-md-description">
              <p>Sample from the functional posterior on input data <code>x</code>.</p>
            </div>
          </li>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="laplace.baselaplace.FunctionalLaplace.predictive_samples" href="#laplace.baselaplace.FunctionalLaplace.predictive_samples">predictive_samples</a></code></b>
            –
            <div class="doc-md-description">
              <p>Sample from the posterior predictive on input data <code>x</code>.</p>
            </div>
          </li>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="laplace.baselaplace.FunctionalLaplace.functional_variance" href="#laplace.baselaplace.FunctionalLaplace.functional_variance">functional_variance</a></code></b>
            –
            <div class="doc-md-description">
              <p>GP posterior variance:</p>
            </div>
          </li>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="laplace.baselaplace.FunctionalLaplace.functional_covariance" href="#laplace.baselaplace.FunctionalLaplace.functional_covariance">functional_covariance</a></code></b>
            –
            <div class="doc-md-description">
              <p>GP posterior covariance:</p>
            </div>
          </li>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="laplace.baselaplace.FunctionalLaplace.optimize_prior_precision" href="#laplace.baselaplace.FunctionalLaplace.optimize_prior_precision">optimize_prior_precision</a></code></b>
            –
            <div class="doc-md-description">
              <p><code>optimize_prior_precision_base</code> from <code>BaseLaplace</code> with <code>pred_type='gp'</code></p>
            </div>
          </li>
          <li class="doc-section-item field-body">
            <b><code><a class="autorefs autorefs-internal" title="laplace.baselaplace.FunctionalLaplace.log_marginal_likelihood" href="#laplace.baselaplace.FunctionalLaplace.log_marginal_likelihood">log_marginal_likelihood</a></code></b>
            –
            <div class="doc-md-description">
              <p>Compute the Laplace approximation to the log marginal likelihood.</p>
            </div>
          </li>
    </ul>




<p><span class="doc-section-title">Attributes:</span></p>
    <ul>
        <li class="doc-section-item field-body">
          <b><code><a class="autorefs autorefs-internal" title="laplace.baselaplace.FunctionalLaplace.log_likelihood" href="#laplace.baselaplace.FunctionalLaplace.log_likelihood">log_likelihood</a></code></b>
              (<code><span title="torch.Tensor">Tensor</span></code>)
          –
          <div class="doc-md-description">
            <p>Compute log likelihood on the training data after <code>.fit()</code> has been called.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><a class="autorefs autorefs-internal" title="laplace.baselaplace.FunctionalLaplace.prior_precision_diag" href="#laplace.baselaplace.FunctionalLaplace.prior_precision_diag">prior_precision_diag</a></code></b>
              (<code><span title="torch.Tensor">Tensor</span></code>)
          –
          <div class="doc-md-description">
            <p>Obtain the diagonal prior precision <span class="arithmatex">\(p_0\)</span> constructed from either</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><a class="autorefs autorefs-internal" title="laplace.baselaplace.FunctionalLaplace.log_det_ratio" href="#laplace.baselaplace.FunctionalLaplace.log_det_ratio">log_det_ratio</a></code></b>
              (<code><span title="torch.Tensor">Tensor</span></code>)
          –
          <div class="doc-md-description">
            <p>Computes log determinant term in GP marginal likelihood</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
          <b><code><a class="autorefs autorefs-internal" title="laplace.baselaplace.FunctionalLaplace.scatter" href="#laplace.baselaplace.FunctionalLaplace.scatter">scatter</a></code></b>
              (<code><span title="torch.Tensor">Tensor</span></code>)
          –
          <div class="doc-md-description">
            <p>Compute scatter term in GP log marginal likelihood.</p>
          </div>
        </li>
    </ul>

                  <details class="quote">
                    <summary>Source code in <code>laplace/baselaplace.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-2118">2118</a></span>
<span class="normal"><a href="#__codelineno-0-2119">2119</a></span>
<span class="normal"><a href="#__codelineno-0-2120">2120</a></span>
<span class="normal"><a href="#__codelineno-0-2121">2121</a></span>
<span class="normal"><a href="#__codelineno-0-2122">2122</a></span>
<span class="normal"><a href="#__codelineno-0-2123">2123</a></span>
<span class="normal"><a href="#__codelineno-0-2124">2124</a></span>
<span class="normal"><a href="#__codelineno-0-2125">2125</a></span>
<span class="normal"><a href="#__codelineno-0-2126">2126</a></span>
<span class="normal"><a href="#__codelineno-0-2127">2127</a></span>
<span class="normal"><a href="#__codelineno-0-2128">2128</a></span>
<span class="normal"><a href="#__codelineno-0-2129">2129</a></span>
<span class="normal"><a href="#__codelineno-0-2130">2130</a></span>
<span class="normal"><a href="#__codelineno-0-2131">2131</a></span>
<span class="normal"><a href="#__codelineno-0-2132">2132</a></span>
<span class="normal"><a href="#__codelineno-0-2133">2133</a></span>
<span class="normal"><a href="#__codelineno-0-2134">2134</a></span>
<span class="normal"><a href="#__codelineno-0-2135">2135</a></span>
<span class="normal"><a href="#__codelineno-0-2136">2136</a></span>
<span class="normal"><a href="#__codelineno-0-2137">2137</a></span>
<span class="normal"><a href="#__codelineno-0-2138">2138</a></span>
<span class="normal"><a href="#__codelineno-0-2139">2139</a></span>
<span class="normal"><a href="#__codelineno-0-2140">2140</a></span>
<span class="normal"><a href="#__codelineno-0-2141">2141</a></span>
<span class="normal"><a href="#__codelineno-0-2142">2142</a></span>
<span class="normal"><a href="#__codelineno-0-2143">2143</a></span>
<span class="normal"><a href="#__codelineno-0-2144">2144</a></span>
<span class="normal"><a href="#__codelineno-0-2145">2145</a></span>
<span class="normal"><a href="#__codelineno-0-2146">2146</a></span>
<span class="normal"><a href="#__codelineno-0-2147">2147</a></span>
<span class="normal"><a href="#__codelineno-0-2148">2148</a></span>
<span class="normal"><a href="#__codelineno-0-2149">2149</a></span>
<span class="normal"><a href="#__codelineno-0-2150">2150</a></span>
<span class="normal"><a href="#__codelineno-0-2151">2151</a></span>
<span class="normal"><a href="#__codelineno-0-2152">2152</a></span>
<span class="normal"><a href="#__codelineno-0-2153">2153</a></span>
<span class="normal"><a href="#__codelineno-0-2154">2154</a></span>
<span class="normal"><a href="#__codelineno-0-2155">2155</a></span>
<span class="normal"><a href="#__codelineno-0-2156">2156</a></span>
<span class="normal"><a href="#__codelineno-0-2157">2157</a></span>
<span class="normal"><a href="#__codelineno-0-2158">2158</a></span>
<span class="normal"><a href="#__codelineno-0-2159">2159</a></span>
<span class="normal"><a href="#__codelineno-0-2160">2160</a></span>
<span class="normal"><a href="#__codelineno-0-2161">2161</a></span>
<span class="normal"><a href="#__codelineno-0-2162">2162</a></span>
<span class="normal"><a href="#__codelineno-0-2163">2163</a></span>
<span class="normal"><a href="#__codelineno-0-2164">2164</a></span>
<span class="normal"><a href="#__codelineno-0-2165">2165</a></span>
<span class="normal"><a href="#__codelineno-0-2166">2166</a></span>
<span class="normal"><a href="#__codelineno-0-2167">2167</a></span>
<span class="normal"><a href="#__codelineno-0-2168">2168</a></span>
<span class="normal"><a href="#__codelineno-0-2169">2169</a></span>
<span class="normal"><a href="#__codelineno-0-2170">2170</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-2118" name="__codelineno-0-2118"></a><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-0-2119" name="__codelineno-0-2119"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-2120" name="__codelineno-0-2120"></a>    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
<a id="__codelineno-0-2121" name="__codelineno-0-2121"></a>    <span class="n">likelihood</span><span class="p">:</span> <span class="n">Likelihood</span> <span class="o">|</span> <span class="nb">str</span><span class="p">,</span>
<a id="__codelineno-0-2122" name="__codelineno-0-2122"></a>    <span class="n">n_subset</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<a id="__codelineno-0-2123" name="__codelineno-0-2123"></a>    <span class="n">sigma_noise</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
<a id="__codelineno-0-2124" name="__codelineno-0-2124"></a>    <span class="n">prior_precision</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
<a id="__codelineno-0-2125" name="__codelineno-0-2125"></a>    <span class="n">prior_mean</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
<a id="__codelineno-0-2126" name="__codelineno-0-2126"></a>    <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
<a id="__codelineno-0-2127" name="__codelineno-0-2127"></a>    <span class="n">enable_backprop</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-0-2128" name="__codelineno-0-2128"></a>    <span class="n">dict_key_x</span><span class="o">=</span><span class="s2">&quot;input_ids&quot;</span><span class="p">,</span>
<a id="__codelineno-0-2129" name="__codelineno-0-2129"></a>    <span class="n">dict_key_y</span><span class="o">=</span><span class="s2">&quot;labels&quot;</span><span class="p">,</span>
<a id="__codelineno-0-2130" name="__codelineno-0-2130"></a>    <span class="n">backend</span><span class="p">:</span> <span class="nb">type</span><span class="p">[</span><span class="n">CurvatureInterface</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="n">BackPackGGN</span><span class="p">,</span>
<a id="__codelineno-0-2131" name="__codelineno-0-2131"></a>    <span class="n">backend_kwargs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-2132" name="__codelineno-0-2132"></a>    <span class="n">independent_outputs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-0-2133" name="__codelineno-0-2133"></a>    <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
<a id="__codelineno-0-2134" name="__codelineno-0-2134"></a><span class="p">):</span>
<a id="__codelineno-0-2135" name="__codelineno-0-2135"></a>    <span class="k">assert</span> <span class="n">backend</span> <span class="ow">in</span> <span class="p">[</span><span class="n">BackPackGGN</span><span class="p">,</span> <span class="n">AsdlGGN</span><span class="p">,</span> <span class="n">CurvlinopsGGN</span><span class="p">]</span>
<a id="__codelineno-0-2136" name="__codelineno-0-2136"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">_check_prior_precision</span><span class="p">(</span><span class="n">prior_precision</span><span class="p">)</span>
<a id="__codelineno-0-2137" name="__codelineno-0-2137"></a>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-0-2138" name="__codelineno-0-2138"></a>        <span class="n">model</span><span class="p">,</span>
<a id="__codelineno-0-2139" name="__codelineno-0-2139"></a>        <span class="n">likelihood</span><span class="p">,</span>
<a id="__codelineno-0-2140" name="__codelineno-0-2140"></a>        <span class="n">sigma_noise</span><span class="p">,</span>
<a id="__codelineno-0-2141" name="__codelineno-0-2141"></a>        <span class="n">prior_precision</span><span class="p">,</span>
<a id="__codelineno-0-2142" name="__codelineno-0-2142"></a>        <span class="n">prior_mean</span><span class="p">,</span>
<a id="__codelineno-0-2143" name="__codelineno-0-2143"></a>        <span class="n">temperature</span><span class="p">,</span>
<a id="__codelineno-0-2144" name="__codelineno-0-2144"></a>        <span class="n">enable_backprop</span><span class="p">,</span>
<a id="__codelineno-0-2145" name="__codelineno-0-2145"></a>        <span class="n">dict_key_x</span><span class="p">,</span>
<a id="__codelineno-0-2146" name="__codelineno-0-2146"></a>        <span class="n">dict_key_y</span><span class="p">,</span>
<a id="__codelineno-0-2147" name="__codelineno-0-2147"></a>        <span class="n">backend</span><span class="p">,</span>
<a id="__codelineno-0-2148" name="__codelineno-0-2148"></a>        <span class="n">backend_kwargs</span><span class="p">,</span>
<a id="__codelineno-0-2149" name="__codelineno-0-2149"></a>    <span class="p">)</span>
<a id="__codelineno-0-2150" name="__codelineno-0-2150"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">enable_backprop</span> <span class="o">=</span> <span class="n">enable_backprop</span>
<a id="__codelineno-0-2151" name="__codelineno-0-2151"></a>
<a id="__codelineno-0-2152" name="__codelineno-0-2152"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">n_subset</span> <span class="o">=</span> <span class="n">n_subset</span>
<a id="__codelineno-0-2153" name="__codelineno-0-2153"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">independent_outputs</span> <span class="o">=</span> <span class="n">independent_outputs</span>
<a id="__codelineno-0-2154" name="__codelineno-0-2154"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span>
<a id="__codelineno-0-2155" name="__codelineno-0-2155"></a>
<a id="__codelineno-0-2156" name="__codelineno-0-2156"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">K_MM</span> <span class="o">=</span> <span class="kc">None</span>
<a id="__codelineno-0-2157" name="__codelineno-0-2157"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">Sigma_inv</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># (K_{MM} + L_MM_inv)^{-1}</span>
<a id="__codelineno-0-2158" name="__codelineno-0-2158"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span> <span class="o">=</span> <span class="p">(</span>
<a id="__codelineno-0-2159" name="__codelineno-0-2159"></a>        <span class="kc">None</span>  <span class="c1"># needed in functional variance and marginal log likelihood</span>
<a id="__codelineno-0-2160" name="__codelineno-0-2160"></a>    <span class="p">)</span>
<a id="__codelineno-0-2161" name="__codelineno-0-2161"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="kc">None</span>
<a id="__codelineno-0-2162" name="__codelineno-0-2162"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">_prior_factor_sod</span> <span class="o">=</span> <span class="kc">None</span>
<a id="__codelineno-0-2163" name="__codelineno-0-2163"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># mean in the scatter term of the log marginal likelihood</span>
<a id="__codelineno-0-2164" name="__codelineno-0-2164"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">L</span> <span class="o">=</span> <span class="kc">None</span>
<a id="__codelineno-0-2165" name="__codelineno-0-2165"></a>
<a id="__codelineno-0-2166" name="__codelineno-0-2166"></a>    <span class="c1"># Posterior mean (used in regression marginal likelihood)</span>
<a id="__codelineno-0-2167" name="__codelineno-0-2167"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">parameters_to_vector</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<a id="__codelineno-0-2168" name="__codelineno-0-2168"></a>
<a id="__codelineno-0-2169" name="__codelineno-0-2169"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">_fitted</span> <span class="o">=</span> <span class="kc">False</span>
<a id="__codelineno-0-2170" name="__codelineno-0-2170"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">_recompute_Sigma</span> <span class="o">=</span> <span class="kc">True</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="laplace.baselaplace.FunctionalLaplace.log_likelihood" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <span class="doc doc-object-name doc-attribute-name">log_likelihood</span>


<a href="#laplace.baselaplace.FunctionalLaplace.log_likelihood" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">log_likelihood</span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Compute log likelihood on the training data after <code>.fit()</code> has been called.
The log likelihood is computed on-demand based on the loss and, for example,
the observation noise which makes it differentiable in the latter for
iterative updates.</p>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>log_likelihood</code></b> (              <code><span title="torch.Tensor">Tensor</span></code>
)          –
          <div class="doc-md-description">
            
          </div>
        </li>
    </ul>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="laplace.baselaplace.FunctionalLaplace.prior_precision_diag" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <span class="doc doc-object-name doc-attribute-name">prior_precision_diag</span>


<a href="#laplace.baselaplace.FunctionalLaplace.prior_precision_diag" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">prior_precision_diag</span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Obtain the diagonal prior precision <span class="arithmatex">\(p_0\)</span> constructed from either
a scalar, layer-wise, or diagonal prior precision.</p>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>prior_precision_diag</code></b> (              <code><span title="torch.Tensor">Tensor</span></code>
)          –
          <div class="doc-md-description">
            
          </div>
        </li>
    </ul>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="laplace.baselaplace.FunctionalLaplace.log_det_ratio" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <span class="doc doc-object-name doc-attribute-name">log_det_ratio</span>


<a href="#laplace.baselaplace.FunctionalLaplace.log_det_ratio" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">log_det_ratio</span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Computes log determinant term in GP marginal likelihood</p>
<p>For <code>classification</code> we use eq. (3.44) from Chapter 3.5 from
<a href="http://www.gaussianprocess.org/gpml/chapters/">GP book R&amp;W 2006</a> with
(note that we always use diagonal approximation <span class="arithmatex">\(D\)</span> of the Hessian of log likelihood w.r.t. <span class="arithmatex">\(f\)</span>):</p>
<p>log determinant term := <span class="arithmatex">\( \log | I + D^{1/2}K D^{1/2} | \)</span></p>
<p>For <code>regression</code>, we use <a href="https://stats.stackexchange.com/questions/280105/log-marginal-likelihood-for-gaussian-process">"standard" GP marginal likelihood</a>:</p>
<p>log determinant term := <span class="arithmatex">\( \log | K + \sigma_2 I | \)</span></p>
    </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="laplace.baselaplace.FunctionalLaplace.scatter" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>            <span class="doc doc-object-name doc-attribute-name">scatter</span>


<a href="#laplace.baselaplace.FunctionalLaplace.scatter" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">scatter</span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Compute scatter term in GP log marginal likelihood.</p>
<p>For <code>classification</code> we use eq. (3.44) from Chapter 3.5 from
<a href="http://www.gaussianprocess.org/gpml/chapters/">GP book R&amp;W 2006</a> with <span class="arithmatex">\(\hat{f} = f \)</span>:</p>
<p>scatter term := <span class="arithmatex">\( f K^{-1} f^{T} \)</span></p>
<p>For <code>regression</code>, we use <a href="https://stats.stackexchange.com/questions/280105/log-marginal-likelihood-for-gaussian-process">"standard" GP marginal likelihood</a>:</p>
<p>scatter term := <span class="arithmatex">\( (y - m)K^{-1}(y -m )^T \)</span>,
where <span class="arithmatex">\( m \)</span> is the mean of the GP prior, which in our case corresponds to
<span class="arithmatex">\( m := f + J (\theta - \theta_{MAP}) \)</span></p>
    </div>

</div>



<div class="doc doc-object doc-function">


<h3 id="laplace.baselaplace.FunctionalLaplace._glm_forward_call" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">_glm_forward_call</span>


<a href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nf">_glm_forward_call</span><span class="p">(</span><span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call(x)">x</a></span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" title="collections.abc.MutableMapping" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.MutableMapping">MutableMapping</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call(likelihood)">likelihood</a></span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="laplace.utils.enums.Likelihood" href="../enums/#laplace.utils.enums.Likelihood">Likelihood</a></span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call(joint)">joint</a></span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call(link_approx)">link_approx</a></span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="laplace.utils.enums.LinkApprox" href="../enums/#laplace.utils.enums.LinkApprox">LinkApprox</a></span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span> <span class="o">=</span> <span class="n"><a class="autorefs autorefs-internal" title="laplace.utils.enums.LinkApprox.PROBIT" href="../enums/#laplace.utils.enums.LinkApprox.PROBIT">PROBIT</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call(n_samples)">n_samples</a></span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call(diagonal_output)">diagonal_output</a></span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="torch.Tensor">Tensor</span></span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#tuple">tuple</a></span><span class="p">[</span><span class="n"><span title="torch.Tensor">Tensor</span></span><span class="p">,</span> <span class="n"><span title="torch.Tensor">Tensor</span></span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Compute the posterior predictive on input data <code>x</code> for "glm" pred type.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._glm_forward_call(x)" class="doc doc-heading doc-heading-parameter">              <b><code>x</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call(x)" class="headerlink" title="Permanent link">#</a></h4>              (<code><span title="torch.Tensor">Tensor</span> or <a class="autorefs autorefs-external" title="collections.abc.MutableMapping" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.MutableMapping">MutableMapping</a></code>)
          –
          <div class="doc-md-description">
            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain
the said tensor.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._glm_forward_call(likelihood)" class="doc doc-heading doc-heading-parameter">              <b><code>likelihood</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call(likelihood)" class="headerlink" title="Permanent link">#</a></h4>              (<code><a class="autorefs autorefs-internal" title="laplace.utils.enums.Likelihood" href="../enums/#laplace.utils.enums.Likelihood">Likelihood</a> or <a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a> in {&#39;classification&#39;, &#39;regression&#39;, &#39;reward_modeling&#39;}</code>)
          –
          <div class="doc-md-description">
            <p>determines the log likelihood Hessian approximation.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._glm_forward_call(link_approx)" class="doc doc-heading doc-heading-parameter">              <b><code>link_approx</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call(link_approx)" class="headerlink" title="Permanent link">#</a></h4>              (<code>(&#39;mc&#39;, &#39;probit&#39;, &#39;bridge&#39;, &#39;bridge_norm&#39;)</code>, default:
                  <code>&#39;mc&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>how to approximate the classification link function for the <code>'glm'</code>.
For <code>pred_type='nn'</code>, only 'mc' is possible.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._glm_forward_call(joint)" class="doc doc-heading doc-heading-parameter">              <b><code>joint</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call(joint)" class="headerlink" title="Permanent link">#</a></h4>              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Whether to output a joint predictive distribution in regression with
<code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution
has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).
If <code>False</code>, then only outputs the marginal predictive distribution.
Only available for regression and GLM predictive.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._glm_forward_call(n_samples)" class="doc doc-heading doc-heading-parameter">              <b><code>n_samples</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call(n_samples)" class="headerlink" title="Permanent link">#</a></h4>              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>, default:
                  <code>100</code>
)
          –
          <div class="doc-md-description">
            <p>number of samples for <code>link_approx='mc'</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._glm_forward_call(diagonal_output)" class="doc doc-heading doc-heading-parameter">              <b><code>diagonal_output</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._glm_forward_call(diagonal_output)" class="headerlink" title="Permanent link">#</a></h4>              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>whether to use a diagonalized posterior predictive on the outputs.
Only works for <code>pred_type='glm'</code> and <code>link_approx='mc'</code>.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>predictive</code></b> (              <code><span title="torch.Tensor">Tensor</span> or <a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#tuple">tuple</a>[<span title="torch.Tensor">Tensor</span>]</code>
)          –
          <div class="doc-md-description">
            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with
a distribution over classes (similar to a Softmax).
For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned
with the mean and the predictive variance.
For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor
is returned with the mean and the predictive covariance.</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>laplace/baselaplace.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-598">598</a></span>
<span class="normal"><a href="#__codelineno-0-599">599</a></span>
<span class="normal"><a href="#__codelineno-0-600">600</a></span>
<span class="normal"><a href="#__codelineno-0-601">601</a></span>
<span class="normal"><a href="#__codelineno-0-602">602</a></span>
<span class="normal"><a href="#__codelineno-0-603">603</a></span>
<span class="normal"><a href="#__codelineno-0-604">604</a></span>
<span class="normal"><a href="#__codelineno-0-605">605</a></span>
<span class="normal"><a href="#__codelineno-0-606">606</a></span>
<span class="normal"><a href="#__codelineno-0-607">607</a></span>
<span class="normal"><a href="#__codelineno-0-608">608</a></span>
<span class="normal"><a href="#__codelineno-0-609">609</a></span>
<span class="normal"><a href="#__codelineno-0-610">610</a></span>
<span class="normal"><a href="#__codelineno-0-611">611</a></span>
<span class="normal"><a href="#__codelineno-0-612">612</a></span>
<span class="normal"><a href="#__codelineno-0-613">613</a></span>
<span class="normal"><a href="#__codelineno-0-614">614</a></span>
<span class="normal"><a href="#__codelineno-0-615">615</a></span>
<span class="normal"><a href="#__codelineno-0-616">616</a></span>
<span class="normal"><a href="#__codelineno-0-617">617</a></span>
<span class="normal"><a href="#__codelineno-0-618">618</a></span>
<span class="normal"><a href="#__codelineno-0-619">619</a></span>
<span class="normal"><a href="#__codelineno-0-620">620</a></span>
<span class="normal"><a href="#__codelineno-0-621">621</a></span>
<span class="normal"><a href="#__codelineno-0-622">622</a></span>
<span class="normal"><a href="#__codelineno-0-623">623</a></span>
<span class="normal"><a href="#__codelineno-0-624">624</a></span>
<span class="normal"><a href="#__codelineno-0-625">625</a></span>
<span class="normal"><a href="#__codelineno-0-626">626</a></span>
<span class="normal"><a href="#__codelineno-0-627">627</a></span>
<span class="normal"><a href="#__codelineno-0-628">628</a></span>
<span class="normal"><a href="#__codelineno-0-629">629</a></span>
<span class="normal"><a href="#__codelineno-0-630">630</a></span>
<span class="normal"><a href="#__codelineno-0-631">631</a></span>
<span class="normal"><a href="#__codelineno-0-632">632</a></span>
<span class="normal"><a href="#__codelineno-0-633">633</a></span>
<span class="normal"><a href="#__codelineno-0-634">634</a></span>
<span class="normal"><a href="#__codelineno-0-635">635</a></span>
<span class="normal"><a href="#__codelineno-0-636">636</a></span>
<span class="normal"><a href="#__codelineno-0-637">637</a></span>
<span class="normal"><a href="#__codelineno-0-638">638</a></span>
<span class="normal"><a href="#__codelineno-0-639">639</a></span>
<span class="normal"><a href="#__codelineno-0-640">640</a></span>
<span class="normal"><a href="#__codelineno-0-641">641</a></span>
<span class="normal"><a href="#__codelineno-0-642">642</a></span>
<span class="normal"><a href="#__codelineno-0-643">643</a></span>
<span class="normal"><a href="#__codelineno-0-644">644</a></span>
<span class="normal"><a href="#__codelineno-0-645">645</a></span>
<span class="normal"><a href="#__codelineno-0-646">646</a></span>
<span class="normal"><a href="#__codelineno-0-647">647</a></span>
<span class="normal"><a href="#__codelineno-0-648">648</a></span>
<span class="normal"><a href="#__codelineno-0-649">649</a></span>
<span class="normal"><a href="#__codelineno-0-650">650</a></span>
<span class="normal"><a href="#__codelineno-0-651">651</a></span>
<span class="normal"><a href="#__codelineno-0-652">652</a></span>
<span class="normal"><a href="#__codelineno-0-653">653</a></span>
<span class="normal"><a href="#__codelineno-0-654">654</a></span>
<span class="normal"><a href="#__codelineno-0-655">655</a></span>
<span class="normal"><a href="#__codelineno-0-656">656</a></span>
<span class="normal"><a href="#__codelineno-0-657">657</a></span>
<span class="normal"><a href="#__codelineno-0-658">658</a></span>
<span class="normal"><a href="#__codelineno-0-659">659</a></span>
<span class="normal"><a href="#__codelineno-0-660">660</a></span>
<span class="normal"><a href="#__codelineno-0-661">661</a></span>
<span class="normal"><a href="#__codelineno-0-662">662</a></span>
<span class="normal"><a href="#__codelineno-0-663">663</a></span>
<span class="normal"><a href="#__codelineno-0-664">664</a></span>
<span class="normal"><a href="#__codelineno-0-665">665</a></span>
<span class="normal"><a href="#__codelineno-0-666">666</a></span>
<span class="normal"><a href="#__codelineno-0-667">667</a></span>
<span class="normal"><a href="#__codelineno-0-668">668</a></span>
<span class="normal"><a href="#__codelineno-0-669">669</a></span>
<span class="normal"><a href="#__codelineno-0-670">670</a></span>
<span class="normal"><a href="#__codelineno-0-671">671</a></span>
<span class="normal"><a href="#__codelineno-0-672">672</a></span>
<span class="normal"><a href="#__codelineno-0-673">673</a></span>
<span class="normal"><a href="#__codelineno-0-674">674</a></span>
<span class="normal"><a href="#__codelineno-0-675">675</a></span>
<span class="normal"><a href="#__codelineno-0-676">676</a></span>
<span class="normal"><a href="#__codelineno-0-677">677</a></span>
<span class="normal"><a href="#__codelineno-0-678">678</a></span>
<span class="normal"><a href="#__codelineno-0-679">679</a></span>
<span class="normal"><a href="#__codelineno-0-680">680</a></span>
<span class="normal"><a href="#__codelineno-0-681">681</a></span>
<span class="normal"><a href="#__codelineno-0-682">682</a></span>
<span class="normal"><a href="#__codelineno-0-683">683</a></span>
<span class="normal"><a href="#__codelineno-0-684">684</a></span>
<span class="normal"><a href="#__codelineno-0-685">685</a></span>
<span class="normal"><a href="#__codelineno-0-686">686</a></span>
<span class="normal"><a href="#__codelineno-0-687">687</a></span>
<span class="normal"><a href="#__codelineno-0-688">688</a></span>
<span class="normal"><a href="#__codelineno-0-689">689</a></span>
<span class="normal"><a href="#__codelineno-0-690">690</a></span>
<span class="normal"><a href="#__codelineno-0-691">691</a></span>
<span class="normal"><a href="#__codelineno-0-692">692</a></span>
<span class="normal"><a href="#__codelineno-0-693">693</a></span>
<span class="normal"><a href="#__codelineno-0-694">694</a></span>
<span class="normal"><a href="#__codelineno-0-695">695</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-598" name="__codelineno-0-598"></a><span class="k">def</span> <span class="nf">_glm_forward_call</span><span class="p">(</span>
<a id="__codelineno-0-599" name="__codelineno-0-599"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-600" name="__codelineno-0-600"></a>    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">MutableMapping</span><span class="p">,</span>
<a id="__codelineno-0-601" name="__codelineno-0-601"></a>    <span class="n">likelihood</span><span class="p">:</span> <span class="n">Likelihood</span> <span class="o">|</span> <span class="nb">str</span><span class="p">,</span>
<a id="__codelineno-0-602" name="__codelineno-0-602"></a>    <span class="n">joint</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-0-603" name="__codelineno-0-603"></a>    <span class="n">link_approx</span><span class="p">:</span> <span class="n">LinkApprox</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">LinkApprox</span><span class="o">.</span><span class="n">PROBIT</span><span class="p">,</span>
<a id="__codelineno-0-604" name="__codelineno-0-604"></a>    <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-0-605" name="__codelineno-0-605"></a>    <span class="n">diagonal_output</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-0-606" name="__codelineno-0-606"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<a id="__codelineno-0-607" name="__codelineno-0-607"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the posterior predictive on input data `x` for &quot;glm&quot; pred type.</span>
<a id="__codelineno-0-608" name="__codelineno-0-608"></a>
<a id="__codelineno-0-609" name="__codelineno-0-609"></a><span class="sd">    Parameters</span>
<a id="__codelineno-0-610" name="__codelineno-0-610"></a><span class="sd">    ----------</span>
<a id="__codelineno-0-611" name="__codelineno-0-611"></a><span class="sd">    x : torch.Tensor or MutableMapping</span>
<a id="__codelineno-0-612" name="__codelineno-0-612"></a><span class="sd">        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain</span>
<a id="__codelineno-0-613" name="__codelineno-0-613"></a><span class="sd">        the said tensor.</span>
<a id="__codelineno-0-614" name="__codelineno-0-614"></a>
<a id="__codelineno-0-615" name="__codelineno-0-615"></a><span class="sd">    likelihood : Likelihood or str in {&#39;classification&#39;, &#39;regression&#39;, &#39;reward_modeling&#39;}</span>
<a id="__codelineno-0-616" name="__codelineno-0-616"></a><span class="sd">        determines the log likelihood Hessian approximation.</span>
<a id="__codelineno-0-617" name="__codelineno-0-617"></a>
<a id="__codelineno-0-618" name="__codelineno-0-618"></a><span class="sd">    link_approx : {&#39;mc&#39;, &#39;probit&#39;, &#39;bridge&#39;, &#39;bridge_norm&#39;}</span>
<a id="__codelineno-0-619" name="__codelineno-0-619"></a><span class="sd">        how to approximate the classification link function for the `&#39;glm&#39;`.</span>
<a id="__codelineno-0-620" name="__codelineno-0-620"></a><span class="sd">        For `pred_type=&#39;nn&#39;`, only &#39;mc&#39; is possible.</span>
<a id="__codelineno-0-621" name="__codelineno-0-621"></a>
<a id="__codelineno-0-622" name="__codelineno-0-622"></a><span class="sd">    joint : bool</span>
<a id="__codelineno-0-623" name="__codelineno-0-623"></a><span class="sd">        Whether to output a joint predictive distribution in regression with</span>
<a id="__codelineno-0-624" name="__codelineno-0-624"></a><span class="sd">        `pred_type=&#39;glm&#39;`. If set to `True`, the predictive distribution</span>
<a id="__codelineno-0-625" name="__codelineno-0-625"></a><span class="sd">        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).</span>
<a id="__codelineno-0-626" name="__codelineno-0-626"></a><span class="sd">        If `False`, then only outputs the marginal predictive distribution.</span>
<a id="__codelineno-0-627" name="__codelineno-0-627"></a><span class="sd">        Only available for regression and GLM predictive.</span>
<a id="__codelineno-0-628" name="__codelineno-0-628"></a>
<a id="__codelineno-0-629" name="__codelineno-0-629"></a><span class="sd">    n_samples : int</span>
<a id="__codelineno-0-630" name="__codelineno-0-630"></a><span class="sd">        number of samples for `link_approx=&#39;mc&#39;`.</span>
<a id="__codelineno-0-631" name="__codelineno-0-631"></a>
<a id="__codelineno-0-632" name="__codelineno-0-632"></a><span class="sd">    diagonal_output : bool</span>
<a id="__codelineno-0-633" name="__codelineno-0-633"></a><span class="sd">        whether to use a diagonalized posterior predictive on the outputs.</span>
<a id="__codelineno-0-634" name="__codelineno-0-634"></a><span class="sd">        Only works for `pred_type=&#39;glm&#39;` and `link_approx=&#39;mc&#39;`.</span>
<a id="__codelineno-0-635" name="__codelineno-0-635"></a>
<a id="__codelineno-0-636" name="__codelineno-0-636"></a><span class="sd">    Returns</span>
<a id="__codelineno-0-637" name="__codelineno-0-637"></a><span class="sd">    -------</span>
<a id="__codelineno-0-638" name="__codelineno-0-638"></a><span class="sd">    predictive: torch.Tensor or tuple[torch.Tensor]</span>
<a id="__codelineno-0-639" name="__codelineno-0-639"></a><span class="sd">        For `likelihood=&#39;classification&#39;`, a torch.Tensor is returned with</span>
<a id="__codelineno-0-640" name="__codelineno-0-640"></a><span class="sd">        a distribution over classes (similar to a Softmax).</span>
<a id="__codelineno-0-641" name="__codelineno-0-641"></a><span class="sd">        For `likelihood=&#39;regression&#39;`, a tuple of torch.Tensor is returned</span>
<a id="__codelineno-0-642" name="__codelineno-0-642"></a><span class="sd">        with the mean and the predictive variance.</span>
<a id="__codelineno-0-643" name="__codelineno-0-643"></a><span class="sd">        For `likelihood=&#39;regression&#39;` and `joint=True`, a tuple of torch.Tensor</span>
<a id="__codelineno-0-644" name="__codelineno-0-644"></a><span class="sd">        is returned with the mean and the predictive covariance.</span>
<a id="__codelineno-0-645" name="__codelineno-0-645"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-646" name="__codelineno-0-646"></a>    <span class="n">f_mu</span><span class="p">,</span> <span class="n">f_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_glm_predictive_distribution</span><span class="p">(</span>
<a id="__codelineno-0-647" name="__codelineno-0-647"></a>        <span class="n">x</span><span class="p">,</span> <span class="n">joint</span><span class="o">=</span><span class="n">joint</span> <span class="ow">and</span> <span class="n">likelihood</span> <span class="o">==</span> <span class="n">Likelihood</span><span class="o">.</span><span class="n">REGRESSION</span>
<a id="__codelineno-0-648" name="__codelineno-0-648"></a>    <span class="p">)</span>
<a id="__codelineno-0-649" name="__codelineno-0-649"></a>
<a id="__codelineno-0-650" name="__codelineno-0-650"></a>    <span class="k">if</span> <span class="n">likelihood</span> <span class="o">==</span> <span class="n">Likelihood</span><span class="o">.</span><span class="n">REGRESSION</span><span class="p">:</span>
<a id="__codelineno-0-651" name="__codelineno-0-651"></a>        <span class="k">if</span> <span class="n">diagonal_output</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">joint</span><span class="p">:</span>
<a id="__codelineno-0-652" name="__codelineno-0-652"></a>            <span class="n">f_var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span><span class="n">f_var</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim2</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-653" name="__codelineno-0-653"></a>        <span class="k">return</span> <span class="n">f_mu</span><span class="p">,</span> <span class="n">f_var</span>
<a id="__codelineno-0-654" name="__codelineno-0-654"></a>
<a id="__codelineno-0-655" name="__codelineno-0-655"></a>    <span class="k">if</span> <span class="n">link_approx</span> <span class="o">==</span> <span class="n">LinkApprox</span><span class="o">.</span><span class="n">MC</span><span class="p">:</span>
<a id="__codelineno-0-656" name="__codelineno-0-656"></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_glm_predictive_samples</span><span class="p">(</span>
<a id="__codelineno-0-657" name="__codelineno-0-657"></a>            <span class="n">f_mu</span><span class="p">,</span>
<a id="__codelineno-0-658" name="__codelineno-0-658"></a>            <span class="n">f_var</span><span class="p">,</span>
<a id="__codelineno-0-659" name="__codelineno-0-659"></a>            <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
<a id="__codelineno-0-660" name="__codelineno-0-660"></a>            <span class="n">diagonal_output</span><span class="o">=</span><span class="n">diagonal_output</span><span class="p">,</span>
<a id="__codelineno-0-661" name="__codelineno-0-661"></a>        <span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<a id="__codelineno-0-662" name="__codelineno-0-662"></a>    <span class="k">elif</span> <span class="n">link_approx</span> <span class="o">==</span> <span class="n">LinkApprox</span><span class="o">.</span><span class="n">PROBIT</span><span class="p">:</span>
<a id="__codelineno-0-663" name="__codelineno-0-663"></a>        <span class="n">kappa</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">8</span> <span class="o">*</span> <span class="n">f_var</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span><span class="n">dim1</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim2</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<a id="__codelineno-0-664" name="__codelineno-0-664"></a>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">kappa</span> <span class="o">*</span> <span class="n">f_mu</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-665" name="__codelineno-0-665"></a>    <span class="k">elif</span> <span class="s2">&quot;bridge&quot;</span> <span class="ow">in</span> <span class="n">link_approx</span><span class="p">:</span>
<a id="__codelineno-0-666" name="__codelineno-0-666"></a>        <span class="c1"># zero mean correction</span>
<a id="__codelineno-0-667" name="__codelineno-0-667"></a>        <span class="n">f_mu</span> <span class="o">-=</span> <span class="p">(</span>
<a id="__codelineno-0-668" name="__codelineno-0-668"></a>            <span class="n">f_var</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-669" name="__codelineno-0-669"></a>            <span class="o">*</span> <span class="n">f_mu</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-670" name="__codelineno-0-670"></a>            <span class="o">/</span> <span class="n">f_var</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-671" name="__codelineno-0-671"></a>        <span class="p">)</span>
<a id="__codelineno-0-672" name="__codelineno-0-672"></a>        <span class="n">f_var</span> <span class="o">-=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
<a id="__codelineno-0-673" name="__codelineno-0-673"></a>            <span class="s2">&quot;bi,bj-&gt;bij&quot;</span><span class="p">,</span> <span class="n">f_var</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">f_var</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-0-674" name="__codelineno-0-674"></a>        <span class="p">)</span> <span class="o">/</span> <span class="n">f_var</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-675" name="__codelineno-0-675"></a>
<a id="__codelineno-0-676" name="__codelineno-0-676"></a>        <span class="c1"># Laplace Bridge</span>
<a id="__codelineno-0-677" name="__codelineno-0-677"></a>        <span class="n">_</span><span class="p">,</span> <span class="n">K</span> <span class="o">=</span> <span class="n">f_mu</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">f_mu</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-678" name="__codelineno-0-678"></a>        <span class="n">f_var_diag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span><span class="n">f_var</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim2</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-0-679" name="__codelineno-0-679"></a>
<a id="__codelineno-0-680" name="__codelineno-0-680"></a>        <span class="c1"># optional: variance correction</span>
<a id="__codelineno-0-681" name="__codelineno-0-681"></a>        <span class="k">if</span> <span class="n">link_approx</span> <span class="o">==</span> <span class="n">LinkApprox</span><span class="o">.</span><span class="n">BRIDGE_NORM</span><span class="p">:</span>
<a id="__codelineno-0-682" name="__codelineno-0-682"></a>            <span class="n">f_var_diag_mean</span> <span class="o">=</span> <span class="n">f_var_diag</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-683" name="__codelineno-0-683"></a>            <span class="n">f_var_diag_mean</span> <span class="o">/=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span>
<a id="__codelineno-0-684" name="__codelineno-0-684"></a>                <span class="p">[</span><span class="n">K</span> <span class="o">/</span> <span class="mi">2</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>
<a id="__codelineno-0-685" name="__codelineno-0-685"></a>            <span class="p">)</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
<a id="__codelineno-0-686" name="__codelineno-0-686"></a>            <span class="n">f_mu</span> <span class="o">/=</span> <span class="n">f_var_diag_mean</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-687" name="__codelineno-0-687"></a>            <span class="n">f_var_diag</span> <span class="o">/=</span> <span class="n">f_var_diag_mean</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-688" name="__codelineno-0-688"></a>
<a id="__codelineno-0-689" name="__codelineno-0-689"></a>        <span class="n">sum_exp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">f_mu</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-690" name="__codelineno-0-690"></a>        <span class="n">alpha</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">K</span> <span class="o">+</span> <span class="n">f_mu</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="o">/</span> <span class="n">K</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sum_exp</span><span class="p">)</span> <span class="o">/</span> <span class="n">f_var_diag</span>
<a id="__codelineno-0-691" name="__codelineno-0-691"></a>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">alpha</span> <span class="o">/</span> <span class="n">alpha</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">nan</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<a id="__codelineno-0-692" name="__codelineno-0-692"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-693" name="__codelineno-0-693"></a>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<a id="__codelineno-0-694" name="__codelineno-0-694"></a>            <span class="s2">&quot;Prediction path invalid. Check the likelihood, pred_type, link_approx combination!&quot;</span>
<a id="__codelineno-0-695" name="__codelineno-0-695"></a>        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="laplace.baselaplace.FunctionalLaplace._glm_functional_samples" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">_glm_functional_samples</span>


<a href="#laplace.baselaplace.FunctionalLaplace._glm_functional_samples" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nf">_glm_functional_samples</span><span class="p">(</span><span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._glm_functional_samples(f_mu)">f_mu</a></span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._glm_functional_samples(f_var)">f_var</a></span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._glm_functional_samples(n_samples)">n_samples</a></span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._glm_functional_samples(diagonal_output)">diagonal_output</a></span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._glm_functional_samples(generator)">generator</a></span><span class="p">:</span> <span class="n"><span title="torch.Generator">Generator</span></span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="torch.Tensor">Tensor</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Sample from the posterior functional on input data <code>x</code> using "glm" prediction
type.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._glm_functional_samples(f_mu)" class="doc doc-heading doc-heading-parameter">              <b><code>f_mu</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._glm_functional_samples(f_mu)" class="headerlink" title="Permanent link">#</a></h4>              (<code><span title="torch.Tensor">Tensor</span> or <a class="autorefs autorefs-external" title="collections.abc.MutableMapping" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.MutableMapping">MutableMapping</a></code>)
          –
          <div class="doc-md-description">
            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._glm_functional_samples(f_var)" class="doc doc-heading doc-heading-parameter">              <b><code>f_var</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._glm_functional_samples(f_var)" class="headerlink" title="Permanent link">#</a></h4>              (<code><span title="torch.Tensor">Tensor</span> or <a class="autorefs autorefs-external" title="collections.abc.MutableMapping" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.MutableMapping">MutableMapping</a></code>)
          –
          <div class="doc-md-description">
            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._glm_functional_samples(n_samples)" class="doc doc-heading doc-heading-parameter">              <b><code>n_samples</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._glm_functional_samples(n_samples)" class="headerlink" title="Permanent link">#</a></h4>              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>)
          –
          <div class="doc-md-description">
            <p>number of samples</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._glm_functional_samples(diagonal_output)" class="doc doc-heading doc-heading-parameter">              <b><code>diagonal_output</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._glm_functional_samples(diagonal_output)" class="headerlink" title="Permanent link">#</a></h4>              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._glm_functional_samples(generator)" class="doc doc-heading doc-heading-parameter">              <b><code>generator</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._glm_functional_samples(generator)" class="headerlink" title="Permanent link">#</a></h4>              (<code><span title="torch.Generator">Generator</span></code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>random number generator to control the samples (if sampling used)</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>samples</code></b> (              <code><span title="torch.Tensor">Tensor</span></code>
)          –
          <div class="doc-md-description">
            <p>samples <code>(n_samples, batch_size, output_shape)</code></p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>laplace/baselaplace.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-697">697</a></span>
<span class="normal"><a href="#__codelineno-0-698">698</a></span>
<span class="normal"><a href="#__codelineno-0-699">699</a></span>
<span class="normal"><a href="#__codelineno-0-700">700</a></span>
<span class="normal"><a href="#__codelineno-0-701">701</a></span>
<span class="normal"><a href="#__codelineno-0-702">702</a></span>
<span class="normal"><a href="#__codelineno-0-703">703</a></span>
<span class="normal"><a href="#__codelineno-0-704">704</a></span>
<span class="normal"><a href="#__codelineno-0-705">705</a></span>
<span class="normal"><a href="#__codelineno-0-706">706</a></span>
<span class="normal"><a href="#__codelineno-0-707">707</a></span>
<span class="normal"><a href="#__codelineno-0-708">708</a></span>
<span class="normal"><a href="#__codelineno-0-709">709</a></span>
<span class="normal"><a href="#__codelineno-0-710">710</a></span>
<span class="normal"><a href="#__codelineno-0-711">711</a></span>
<span class="normal"><a href="#__codelineno-0-712">712</a></span>
<span class="normal"><a href="#__codelineno-0-713">713</a></span>
<span class="normal"><a href="#__codelineno-0-714">714</a></span>
<span class="normal"><a href="#__codelineno-0-715">715</a></span>
<span class="normal"><a href="#__codelineno-0-716">716</a></span>
<span class="normal"><a href="#__codelineno-0-717">717</a></span>
<span class="normal"><a href="#__codelineno-0-718">718</a></span>
<span class="normal"><a href="#__codelineno-0-719">719</a></span>
<span class="normal"><a href="#__codelineno-0-720">720</a></span>
<span class="normal"><a href="#__codelineno-0-721">721</a></span>
<span class="normal"><a href="#__codelineno-0-722">722</a></span>
<span class="normal"><a href="#__codelineno-0-723">723</a></span>
<span class="normal"><a href="#__codelineno-0-724">724</a></span>
<span class="normal"><a href="#__codelineno-0-725">725</a></span>
<span class="normal"><a href="#__codelineno-0-726">726</a></span>
<span class="normal"><a href="#__codelineno-0-727">727</a></span>
<span class="normal"><a href="#__codelineno-0-728">728</a></span>
<span class="normal"><a href="#__codelineno-0-729">729</a></span>
<span class="normal"><a href="#__codelineno-0-730">730</a></span>
<span class="normal"><a href="#__codelineno-0-731">731</a></span>
<span class="normal"><a href="#__codelineno-0-732">732</a></span>
<span class="normal"><a href="#__codelineno-0-733">733</a></span>
<span class="normal"><a href="#__codelineno-0-734">734</a></span>
<span class="normal"><a href="#__codelineno-0-735">735</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-697" name="__codelineno-0-697"></a><span class="k">def</span> <span class="nf">_glm_functional_samples</span><span class="p">(</span>
<a id="__codelineno-0-698" name="__codelineno-0-698"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-699" name="__codelineno-0-699"></a>    <span class="n">f_mu</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-0-700" name="__codelineno-0-700"></a>    <span class="n">f_var</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-0-701" name="__codelineno-0-701"></a>    <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<a id="__codelineno-0-702" name="__codelineno-0-702"></a>    <span class="n">diagonal_output</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-0-703" name="__codelineno-0-703"></a>    <span class="n">generator</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-704" name="__codelineno-0-704"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-705" name="__codelineno-0-705"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Sample from the posterior functional on input data `x` using &quot;glm&quot; prediction</span>
<a id="__codelineno-0-706" name="__codelineno-0-706"></a><span class="sd">    type.</span>
<a id="__codelineno-0-707" name="__codelineno-0-707"></a>
<a id="__codelineno-0-708" name="__codelineno-0-708"></a><span class="sd">    Parameters</span>
<a id="__codelineno-0-709" name="__codelineno-0-709"></a><span class="sd">    ----------</span>
<a id="__codelineno-0-710" name="__codelineno-0-710"></a><span class="sd">    f_mu : torch.Tensor or MutableMapping</span>
<a id="__codelineno-0-711" name="__codelineno-0-711"></a><span class="sd">        glm predictive mean `(batch_size, output_shape)`</span>
<a id="__codelineno-0-712" name="__codelineno-0-712"></a>
<a id="__codelineno-0-713" name="__codelineno-0-713"></a><span class="sd">    f_var : torch.Tensor or MutableMapping</span>
<a id="__codelineno-0-714" name="__codelineno-0-714"></a><span class="sd">        glm predictive covariances `(batch_size, output_shape, output_shape)`</span>
<a id="__codelineno-0-715" name="__codelineno-0-715"></a>
<a id="__codelineno-0-716" name="__codelineno-0-716"></a><span class="sd">    n_samples : int</span>
<a id="__codelineno-0-717" name="__codelineno-0-717"></a><span class="sd">        number of samples</span>
<a id="__codelineno-0-718" name="__codelineno-0-718"></a>
<a id="__codelineno-0-719" name="__codelineno-0-719"></a><span class="sd">    diagonal_output : bool</span>
<a id="__codelineno-0-720" name="__codelineno-0-720"></a><span class="sd">        whether to use a diagonalized glm posterior predictive on the outputs.</span>
<a id="__codelineno-0-721" name="__codelineno-0-721"></a>
<a id="__codelineno-0-722" name="__codelineno-0-722"></a><span class="sd">    generator : torch.Generator, optional</span>
<a id="__codelineno-0-723" name="__codelineno-0-723"></a><span class="sd">        random number generator to control the samples (if sampling used)</span>
<a id="__codelineno-0-724" name="__codelineno-0-724"></a>
<a id="__codelineno-0-725" name="__codelineno-0-725"></a><span class="sd">    Returns</span>
<a id="__codelineno-0-726" name="__codelineno-0-726"></a><span class="sd">    -------</span>
<a id="__codelineno-0-727" name="__codelineno-0-727"></a><span class="sd">    samples : torch.Tensor</span>
<a id="__codelineno-0-728" name="__codelineno-0-728"></a><span class="sd">        samples `(n_samples, batch_size, output_shape)`</span>
<a id="__codelineno-0-729" name="__codelineno-0-729"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-730" name="__codelineno-0-730"></a>    <span class="k">assert</span> <span class="n">f_var</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">f_mu</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">f_mu</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">f_mu</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
<a id="__codelineno-0-731" name="__codelineno-0-731"></a>
<a id="__codelineno-0-732" name="__codelineno-0-732"></a>    <span class="k">if</span> <span class="n">diagonal_output</span><span class="p">:</span>
<a id="__codelineno-0-733" name="__codelineno-0-733"></a>        <span class="n">f_var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span><span class="n">f_var</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim2</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-0-734" name="__codelineno-0-734"></a>
<a id="__codelineno-0-735" name="__codelineno-0-735"></a>    <span class="k">return</span> <span class="n">normal_samples</span><span class="p">(</span><span class="n">f_mu</span><span class="p">,</span> <span class="n">f_var</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">generator</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="laplace.baselaplace.FunctionalLaplace._glm_predictive_samples" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">_glm_predictive_samples</span>


<a href="#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nf">_glm_predictive_samples</span><span class="p">(</span><span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(f_mu)">f_mu</a></span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(f_var)">f_var</a></span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(n_samples)">n_samples</a></span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(diagonal_output)">diagonal_output</a></span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(generator)">generator</a></span><span class="p">:</span> <span class="n"><span title="torch.Generator">Generator</span></span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="torch.Tensor">Tensor</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Sample from the posterior predictive on input data <code>x</code> using "glm" prediction
type. I.e., the inverse-link function correponding to the likelihood is applied
on top of the functional sample.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(f_mu)" class="doc doc-heading doc-heading-parameter">              <b><code>f_mu</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(f_mu)" class="headerlink" title="Permanent link">#</a></h4>              (<code><span title="torch.Tensor">Tensor</span> or <a class="autorefs autorefs-external" title="collections.abc.MutableMapping" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.MutableMapping">MutableMapping</a></code>)
          –
          <div class="doc-md-description">
            <p>glm predictive mean <code>(batch_size, output_shape)</code></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(f_var)" class="doc doc-heading doc-heading-parameter">              <b><code>f_var</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(f_var)" class="headerlink" title="Permanent link">#</a></h4>              (<code><span title="torch.Tensor">Tensor</span> or <a class="autorefs autorefs-external" title="collections.abc.MutableMapping" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.MutableMapping">MutableMapping</a></code>)
          –
          <div class="doc-md-description">
            <p>glm predictive covariances <code>(batch_size, output_shape, output_shape)</code></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(n_samples)" class="doc doc-heading doc-heading-parameter">              <b><code>n_samples</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(n_samples)" class="headerlink" title="Permanent link">#</a></h4>              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>)
          –
          <div class="doc-md-description">
            <p>number of samples</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(diagonal_output)" class="doc doc-heading doc-heading-parameter">              <b><code>diagonal_output</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(diagonal_output)" class="headerlink" title="Permanent link">#</a></h4>              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>whether to use a diagonalized glm posterior predictive on the outputs.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(generator)" class="doc doc-heading doc-heading-parameter">              <b><code>generator</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._glm_predictive_samples(generator)" class="headerlink" title="Permanent link">#</a></h4>              (<code><span title="torch.Generator">Generator</span></code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>random number generator to control the samples (if sampling used)</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>samples</code></b> (              <code><span title="torch.Tensor">Tensor</span></code>
)          –
          <div class="doc-md-description">
            <p>samples <code>(n_samples, batch_size, output_shape)</code></p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>laplace/baselaplace.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-737">737</a></span>
<span class="normal"><a href="#__codelineno-0-738">738</a></span>
<span class="normal"><a href="#__codelineno-0-739">739</a></span>
<span class="normal"><a href="#__codelineno-0-740">740</a></span>
<span class="normal"><a href="#__codelineno-0-741">741</a></span>
<span class="normal"><a href="#__codelineno-0-742">742</a></span>
<span class="normal"><a href="#__codelineno-0-743">743</a></span>
<span class="normal"><a href="#__codelineno-0-744">744</a></span>
<span class="normal"><a href="#__codelineno-0-745">745</a></span>
<span class="normal"><a href="#__codelineno-0-746">746</a></span>
<span class="normal"><a href="#__codelineno-0-747">747</a></span>
<span class="normal"><a href="#__codelineno-0-748">748</a></span>
<span class="normal"><a href="#__codelineno-0-749">749</a></span>
<span class="normal"><a href="#__codelineno-0-750">750</a></span>
<span class="normal"><a href="#__codelineno-0-751">751</a></span>
<span class="normal"><a href="#__codelineno-0-752">752</a></span>
<span class="normal"><a href="#__codelineno-0-753">753</a></span>
<span class="normal"><a href="#__codelineno-0-754">754</a></span>
<span class="normal"><a href="#__codelineno-0-755">755</a></span>
<span class="normal"><a href="#__codelineno-0-756">756</a></span>
<span class="normal"><a href="#__codelineno-0-757">757</a></span>
<span class="normal"><a href="#__codelineno-0-758">758</a></span>
<span class="normal"><a href="#__codelineno-0-759">759</a></span>
<span class="normal"><a href="#__codelineno-0-760">760</a></span>
<span class="normal"><a href="#__codelineno-0-761">761</a></span>
<span class="normal"><a href="#__codelineno-0-762">762</a></span>
<span class="normal"><a href="#__codelineno-0-763">763</a></span>
<span class="normal"><a href="#__codelineno-0-764">764</a></span>
<span class="normal"><a href="#__codelineno-0-765">765</a></span>
<span class="normal"><a href="#__codelineno-0-766">766</a></span>
<span class="normal"><a href="#__codelineno-0-767">767</a></span>
<span class="normal"><a href="#__codelineno-0-768">768</a></span>
<span class="normal"><a href="#__codelineno-0-769">769</a></span>
<span class="normal"><a href="#__codelineno-0-770">770</a></span>
<span class="normal"><a href="#__codelineno-0-771">771</a></span>
<span class="normal"><a href="#__codelineno-0-772">772</a></span>
<span class="normal"><a href="#__codelineno-0-773">773</a></span>
<span class="normal"><a href="#__codelineno-0-774">774</a></span>
<span class="normal"><a href="#__codelineno-0-775">775</a></span>
<span class="normal"><a href="#__codelineno-0-776">776</a></span>
<span class="normal"><a href="#__codelineno-0-777">777</a></span>
<span class="normal"><a href="#__codelineno-0-778">778</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-737" name="__codelineno-0-737"></a><span class="k">def</span> <span class="nf">_glm_predictive_samples</span><span class="p">(</span>
<a id="__codelineno-0-738" name="__codelineno-0-738"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-739" name="__codelineno-0-739"></a>    <span class="n">f_mu</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-0-740" name="__codelineno-0-740"></a>    <span class="n">f_var</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-0-741" name="__codelineno-0-741"></a>    <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<a id="__codelineno-0-742" name="__codelineno-0-742"></a>    <span class="n">diagonal_output</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-0-743" name="__codelineno-0-743"></a>    <span class="n">generator</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-744" name="__codelineno-0-744"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-745" name="__codelineno-0-745"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Sample from the posterior predictive on input data `x` using &quot;glm&quot; prediction</span>
<a id="__codelineno-0-746" name="__codelineno-0-746"></a><span class="sd">    type. I.e., the inverse-link function correponding to the likelihood is applied</span>
<a id="__codelineno-0-747" name="__codelineno-0-747"></a><span class="sd">    on top of the functional sample.</span>
<a id="__codelineno-0-748" name="__codelineno-0-748"></a>
<a id="__codelineno-0-749" name="__codelineno-0-749"></a><span class="sd">    Parameters</span>
<a id="__codelineno-0-750" name="__codelineno-0-750"></a><span class="sd">    ----------</span>
<a id="__codelineno-0-751" name="__codelineno-0-751"></a><span class="sd">    f_mu : torch.Tensor or MutableMapping</span>
<a id="__codelineno-0-752" name="__codelineno-0-752"></a><span class="sd">        glm predictive mean `(batch_size, output_shape)`</span>
<a id="__codelineno-0-753" name="__codelineno-0-753"></a>
<a id="__codelineno-0-754" name="__codelineno-0-754"></a><span class="sd">    f_var : torch.Tensor or MutableMapping</span>
<a id="__codelineno-0-755" name="__codelineno-0-755"></a><span class="sd">        glm predictive covariances `(batch_size, output_shape, output_shape)`</span>
<a id="__codelineno-0-756" name="__codelineno-0-756"></a>
<a id="__codelineno-0-757" name="__codelineno-0-757"></a><span class="sd">    n_samples : int</span>
<a id="__codelineno-0-758" name="__codelineno-0-758"></a><span class="sd">        number of samples</span>
<a id="__codelineno-0-759" name="__codelineno-0-759"></a>
<a id="__codelineno-0-760" name="__codelineno-0-760"></a><span class="sd">    diagonal_output : bool</span>
<a id="__codelineno-0-761" name="__codelineno-0-761"></a><span class="sd">        whether to use a diagonalized glm posterior predictive on the outputs.</span>
<a id="__codelineno-0-762" name="__codelineno-0-762"></a>
<a id="__codelineno-0-763" name="__codelineno-0-763"></a><span class="sd">    generator : torch.Generator, optional</span>
<a id="__codelineno-0-764" name="__codelineno-0-764"></a><span class="sd">        random number generator to control the samples (if sampling used)</span>
<a id="__codelineno-0-765" name="__codelineno-0-765"></a>
<a id="__codelineno-0-766" name="__codelineno-0-766"></a><span class="sd">    Returns</span>
<a id="__codelineno-0-767" name="__codelineno-0-767"></a><span class="sd">    -------</span>
<a id="__codelineno-0-768" name="__codelineno-0-768"></a><span class="sd">    samples : torch.Tensor</span>
<a id="__codelineno-0-769" name="__codelineno-0-769"></a><span class="sd">        samples `(n_samples, batch_size, output_shape)`</span>
<a id="__codelineno-0-770" name="__codelineno-0-770"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-771" name="__codelineno-0-771"></a>    <span class="n">f_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_glm_functional_samples</span><span class="p">(</span>
<a id="__codelineno-0-772" name="__codelineno-0-772"></a>        <span class="n">f_mu</span><span class="p">,</span> <span class="n">f_var</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">diagonal_output</span><span class="p">,</span> <span class="n">generator</span>
<a id="__codelineno-0-773" name="__codelineno-0-773"></a>    <span class="p">)</span>
<a id="__codelineno-0-774" name="__codelineno-0-774"></a>
<a id="__codelineno-0-775" name="__codelineno-0-775"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span> <span class="o">==</span> <span class="n">Likelihood</span><span class="o">.</span><span class="n">REGRESSION</span><span class="p">:</span>
<a id="__codelineno-0-776" name="__codelineno-0-776"></a>        <span class="k">return</span> <span class="n">f_samples</span>
<a id="__codelineno-0-777" name="__codelineno-0-777"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-778" name="__codelineno-0-778"></a>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">f_samples</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="laplace.baselaplace.FunctionalLaplace._check_prior_precision" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">_check_prior_precision</span>


<a href="#laplace.baselaplace.FunctionalLaplace._check_prior_precision" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nf">_check_prior_precision</span><span class="p">(</span><span class="n">prior_precision</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span> <span class="o">|</span> <span class="n"><span title="torch.Tensor">Tensor</span></span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Checks if the given prior precision is suitable for the GP interpretation of LLA.
As such, only single value priors, i.e., isotropic priors are suitable.</p>

            <details class="quote">
              <summary>Source code in <code>laplace/baselaplace.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-2172">2172</a></span>
<span class="normal"><a href="#__codelineno-0-2173">2173</a></span>
<span class="normal"><a href="#__codelineno-0-2174">2174</a></span>
<span class="normal"><a href="#__codelineno-0-2175">2175</a></span>
<span class="normal"><a href="#__codelineno-0-2176">2176</a></span>
<span class="normal"><a href="#__codelineno-0-2177">2177</a></span>
<span class="normal"><a href="#__codelineno-0-2178">2178</a></span>
<span class="normal"><a href="#__codelineno-0-2179">2179</a></span>
<span class="normal"><a href="#__codelineno-0-2180">2180</a></span>
<span class="normal"><a href="#__codelineno-0-2181">2181</a></span>
<span class="normal"><a href="#__codelineno-0-2182">2182</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-2172" name="__codelineno-0-2172"></a><span class="nd">@staticmethod</span>
<a id="__codelineno-0-2173" name="__codelineno-0-2173"></a><span class="k">def</span> <span class="nf">_check_prior_precision</span><span class="p">(</span><span class="n">prior_precision</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<a id="__codelineno-0-2174" name="__codelineno-0-2174"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Checks if the given prior precision is suitable for the GP interpretation of LLA.</span>
<a id="__codelineno-0-2175" name="__codelineno-0-2175"></a><span class="sd">    As such, only single value priors, i.e., isotropic priors are suitable.</span>
<a id="__codelineno-0-2176" name="__codelineno-0-2176"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-2177" name="__codelineno-0-2177"></a>    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">prior_precision</span><span class="p">):</span>
<a id="__codelineno-0-2178" name="__codelineno-0-2178"></a>        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
<a id="__codelineno-0-2179" name="__codelineno-0-2179"></a>            <span class="n">prior_precision</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span>
<a id="__codelineno-0-2180" name="__codelineno-0-2180"></a>            <span class="ow">or</span> <span class="p">(</span><span class="n">prior_precision</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">prior_precision</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-2181" name="__codelineno-0-2181"></a>        <span class="p">):</span>
<a id="__codelineno-0-2182" name="__codelineno-0-2182"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Only isotropic priors supported in FunctionalLaplace&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="laplace.baselaplace.FunctionalLaplace._init_K_MM" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">_init_K_MM</span>


<a href="#laplace.baselaplace.FunctionalLaplace._init_K_MM" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nf">_init_K_MM</span><span class="p">()</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Allocates memory for the kernel matrix evaluated at the subset of the training
data points. If the subset is of size <span class="arithmatex">\(M\)</span> and the problem has <span class="arithmatex">\(C\)</span> outputs,
this is a list of C <span class="arithmatex">\((M,M\)</span>) tensors for diagonal kernel and <span class="arithmatex">\((M x C, M x C)\)</span>
otherwise.</p>

            <details class="quote">
              <summary>Source code in <code>laplace/baselaplace.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-2184">2184</a></span>
<span class="normal"><a href="#__codelineno-0-2185">2185</a></span>
<span class="normal"><a href="#__codelineno-0-2186">2186</a></span>
<span class="normal"><a href="#__codelineno-0-2187">2187</a></span>
<span class="normal"><a href="#__codelineno-0-2188">2188</a></span>
<span class="normal"><a href="#__codelineno-0-2189">2189</a></span>
<span class="normal"><a href="#__codelineno-0-2190">2190</a></span>
<span class="normal"><a href="#__codelineno-0-2191">2191</a></span>
<span class="normal"><a href="#__codelineno-0-2192">2192</a></span>
<span class="normal"><a href="#__codelineno-0-2193">2193</a></span>
<span class="normal"><a href="#__codelineno-0-2194">2194</a></span>
<span class="normal"><a href="#__codelineno-0-2195">2195</a></span>
<span class="normal"><a href="#__codelineno-0-2196">2196</a></span>
<span class="normal"><a href="#__codelineno-0-2197">2197</a></span>
<span class="normal"><a href="#__codelineno-0-2198">2198</a></span>
<span class="normal"><a href="#__codelineno-0-2199">2199</a></span>
<span class="normal"><a href="#__codelineno-0-2200">2200</a></span>
<span class="normal"><a href="#__codelineno-0-2201">2201</a></span>
<span class="normal"><a href="#__codelineno-0-2202">2202</a></span>
<span class="normal"><a href="#__codelineno-0-2203">2203</a></span>
<span class="normal"><a href="#__codelineno-0-2204">2204</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-2184" name="__codelineno-0-2184"></a><span class="k">def</span> <span class="nf">_init_K_MM</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<a id="__codelineno-0-2185" name="__codelineno-0-2185"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Allocates memory for the kernel matrix evaluated at the subset of the training</span>
<a id="__codelineno-0-2186" name="__codelineno-0-2186"></a><span class="sd">    data points. If the subset is of size \\(M\\) and the problem has \\(C\\) outputs,</span>
<a id="__codelineno-0-2187" name="__codelineno-0-2187"></a><span class="sd">    this is a list of C \\((M,M\\)) tensors for diagonal kernel and \\((M x C, M x C)\\)</span>
<a id="__codelineno-0-2188" name="__codelineno-0-2188"></a><span class="sd">    otherwise.</span>
<a id="__codelineno-0-2189" name="__codelineno-0-2189"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-2190" name="__codelineno-0-2190"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">independent_outputs</span><span class="p">:</span>
<a id="__codelineno-0-2191" name="__codelineno-0-2191"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">K_MM</span> <span class="o">=</span> <span class="p">[</span>
<a id="__codelineno-0-2192" name="__codelineno-0-2192"></a>            <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
<a id="__codelineno-0-2193" name="__codelineno-0-2193"></a>                <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_subset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_subset</span><span class="p">),</span>
<a id="__codelineno-0-2194" name="__codelineno-0-2194"></a>                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
<a id="__codelineno-0-2195" name="__codelineno-0-2195"></a>                <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span>
<a id="__codelineno-0-2196" name="__codelineno-0-2196"></a>            <span class="p">)</span>
<a id="__codelineno-0-2197" name="__codelineno-0-2197"></a>            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">)</span>
<a id="__codelineno-0-2198" name="__codelineno-0-2198"></a>        <span class="p">]</span>
<a id="__codelineno-0-2199" name="__codelineno-0-2199"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-2200" name="__codelineno-0-2200"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">K_MM</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
<a id="__codelineno-0-2201" name="__codelineno-0-2201"></a>            <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_subset</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_subset</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">),</span>
<a id="__codelineno-0-2202" name="__codelineno-0-2202"></a>            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
<a id="__codelineno-0-2203" name="__codelineno-0-2203"></a>            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span>
<a id="__codelineno-0-2204" name="__codelineno-0-2204"></a>        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="laplace.baselaplace.FunctionalLaplace._init_Sigma_inv" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">_init_Sigma_inv</span>


<a href="#laplace.baselaplace.FunctionalLaplace._init_Sigma_inv" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nf">_init_Sigma_inv</span><span class="p">()</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Allocates memory for the cholesky decomposition of
[
    K_{MM} + \Lambda_{MM}^{-1}.
]
See See <a href="https://arxiv.org/abs/2008.08400">Improving predictions of Bayesian neural nets via local linearization (Immer et al., 2021)</a>
Equation 15 for more information.</p>

            <details class="quote">
              <summary>Source code in <code>laplace/baselaplace.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-2206">2206</a></span>
<span class="normal"><a href="#__codelineno-0-2207">2207</a></span>
<span class="normal"><a href="#__codelineno-0-2208">2208</a></span>
<span class="normal"><a href="#__codelineno-0-2209">2209</a></span>
<span class="normal"><a href="#__codelineno-0-2210">2210</a></span>
<span class="normal"><a href="#__codelineno-0-2211">2211</a></span>
<span class="normal"><a href="#__codelineno-0-2212">2212</a></span>
<span class="normal"><a href="#__codelineno-0-2213">2213</a></span>
<span class="normal"><a href="#__codelineno-0-2214">2214</a></span>
<span class="normal"><a href="#__codelineno-0-2215">2215</a></span>
<span class="normal"><a href="#__codelineno-0-2216">2216</a></span>
<span class="normal"><a href="#__codelineno-0-2217">2217</a></span>
<span class="normal"><a href="#__codelineno-0-2218">2218</a></span>
<span class="normal"><a href="#__codelineno-0-2219">2219</a></span>
<span class="normal"><a href="#__codelineno-0-2220">2220</a></span>
<span class="normal"><a href="#__codelineno-0-2221">2221</a></span>
<span class="normal"><a href="#__codelineno-0-2222">2222</a></span>
<span class="normal"><a href="#__codelineno-0-2223">2223</a></span>
<span class="normal"><a href="#__codelineno-0-2224">2224</a></span>
<span class="normal"><a href="#__codelineno-0-2225">2225</a></span>
<span class="normal"><a href="#__codelineno-0-2226">2226</a></span>
<span class="normal"><a href="#__codelineno-0-2227">2227</a></span>
<span class="normal"><a href="#__codelineno-0-2228">2228</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-2206" name="__codelineno-0-2206"></a><span class="k">def</span> <span class="nf">_init_Sigma_inv</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<a id="__codelineno-0-2207" name="__codelineno-0-2207"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Allocates memory for the cholesky decomposition of</span>
<a id="__codelineno-0-2208" name="__codelineno-0-2208"></a><span class="sd">    \\[</span>
<a id="__codelineno-0-2209" name="__codelineno-0-2209"></a><span class="sd">        K_{MM} + \\Lambda_{MM}^{-1}.</span>
<a id="__codelineno-0-2210" name="__codelineno-0-2210"></a><span class="sd">    \\]</span>
<a id="__codelineno-0-2211" name="__codelineno-0-2211"></a><span class="sd">    See See [Improving predictions of Bayesian neural nets via local linearization (Immer et al., 2021)](https://arxiv.org/abs/2008.08400)</span>
<a id="__codelineno-0-2212" name="__codelineno-0-2212"></a><span class="sd">    Equation 15 for more information.</span>
<a id="__codelineno-0-2213" name="__codelineno-0-2213"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-2214" name="__codelineno-0-2214"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">independent_outputs</span><span class="p">:</span>
<a id="__codelineno-0-2215" name="__codelineno-0-2215"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">Sigma_inv</span> <span class="o">=</span> <span class="p">[</span>
<a id="__codelineno-0-2216" name="__codelineno-0-2216"></a>            <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
<a id="__codelineno-0-2217" name="__codelineno-0-2217"></a>                <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_subset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_subset</span><span class="p">),</span>
<a id="__codelineno-0-2218" name="__codelineno-0-2218"></a>                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
<a id="__codelineno-0-2219" name="__codelineno-0-2219"></a>                <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span>
<a id="__codelineno-0-2220" name="__codelineno-0-2220"></a>            <span class="p">)</span>
<a id="__codelineno-0-2221" name="__codelineno-0-2221"></a>            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">)</span>
<a id="__codelineno-0-2222" name="__codelineno-0-2222"></a>        <span class="p">]</span>
<a id="__codelineno-0-2223" name="__codelineno-0-2223"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-2224" name="__codelineno-0-2224"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">Sigma_inv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
<a id="__codelineno-0-2225" name="__codelineno-0-2225"></a>            <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_subset</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_subset</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">),</span>
<a id="__codelineno-0-2226" name="__codelineno-0-2226"></a>            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span>
<a id="__codelineno-0-2227" name="__codelineno-0-2227"></a>            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span>
<a id="__codelineno-0-2228" name="__codelineno-0-2228"></a>        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="laplace.baselaplace.FunctionalLaplace._store_K_batch" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">_store_K_batch</span>


<a href="#laplace.baselaplace.FunctionalLaplace._store_K_batch" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nf">_store_K_batch</span><span class="p">(</span><span class="n">K_batch</span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span><span class="p">,</span> <span class="n">i</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Given the kernel matrix between the i-th and the j-th batch, stores it in the
corresponding position in self.K_MM.</p>

            <details class="quote">
              <summary>Source code in <code>laplace/baselaplace.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-2230">2230</a></span>
<span class="normal"><a href="#__codelineno-0-2231">2231</a></span>
<span class="normal"><a href="#__codelineno-0-2232">2232</a></span>
<span class="normal"><a href="#__codelineno-0-2233">2233</a></span>
<span class="normal"><a href="#__codelineno-0-2234">2234</a></span>
<span class="normal"><a href="#__codelineno-0-2235">2235</a></span>
<span class="normal"><a href="#__codelineno-0-2236">2236</a></span>
<span class="normal"><a href="#__codelineno-0-2237">2237</a></span>
<span class="normal"><a href="#__codelineno-0-2238">2238</a></span>
<span class="normal"><a href="#__codelineno-0-2239">2239</a></span>
<span class="normal"><a href="#__codelineno-0-2240">2240</a></span>
<span class="normal"><a href="#__codelineno-0-2241">2241</a></span>
<span class="normal"><a href="#__codelineno-0-2242">2242</a></span>
<span class="normal"><a href="#__codelineno-0-2243">2243</a></span>
<span class="normal"><a href="#__codelineno-0-2244">2244</a></span>
<span class="normal"><a href="#__codelineno-0-2245">2245</a></span>
<span class="normal"><a href="#__codelineno-0-2246">2246</a></span>
<span class="normal"><a href="#__codelineno-0-2247">2247</a></span>
<span class="normal"><a href="#__codelineno-0-2248">2248</a></span>
<span class="normal"><a href="#__codelineno-0-2249">2249</a></span>
<span class="normal"><a href="#__codelineno-0-2250">2250</a></span>
<span class="normal"><a href="#__codelineno-0-2251">2251</a></span>
<span class="normal"><a href="#__codelineno-0-2252">2252</a></span>
<span class="normal"><a href="#__codelineno-0-2253">2253</a></span>
<span class="normal"><a href="#__codelineno-0-2254">2254</a></span>
<span class="normal"><a href="#__codelineno-0-2255">2255</a></span>
<span class="normal"><a href="#__codelineno-0-2256">2256</a></span>
<span class="normal"><a href="#__codelineno-0-2257">2257</a></span>
<span class="normal"><a href="#__codelineno-0-2258">2258</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-2230" name="__codelineno-0-2230"></a><span class="k">def</span> <span class="nf">_store_K_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">K_batch</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">i</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<a id="__codelineno-0-2231" name="__codelineno-0-2231"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Given the kernel matrix between the i-th and the j-th batch, stores it in the</span>
<a id="__codelineno-0-2232" name="__codelineno-0-2232"></a><span class="sd">    corresponding position in self.K_MM.</span>
<a id="__codelineno-0-2233" name="__codelineno-0-2233"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-2234" name="__codelineno-0-2234"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">independent_outputs</span><span class="p">:</span>
<a id="__codelineno-0-2235" name="__codelineno-0-2235"></a>        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">):</span>
<a id="__codelineno-0-2236" name="__codelineno-0-2236"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">K_MM</span><span class="p">[</span><span class="n">c</span><span class="p">][</span>
<a id="__codelineno-0-2237" name="__codelineno-0-2237"></a>                <span class="n">i</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="p">:</span> <span class="nb">min</span><span class="p">((</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_subset</span><span class="p">),</span>
<a id="__codelineno-0-2238" name="__codelineno-0-2238"></a>                <span class="n">j</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="p">:</span> <span class="nb">min</span><span class="p">((</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_subset</span><span class="p">),</span>
<a id="__codelineno-0-2239" name="__codelineno-0-2239"></a>            <span class="p">]</span> <span class="o">=</span> <span class="n">K_batch</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">c</span><span class="p">]</span>
<a id="__codelineno-0-2240" name="__codelineno-0-2240"></a>            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">j</span><span class="p">:</span>
<a id="__codelineno-0-2241" name="__codelineno-0-2241"></a>                <span class="bp">self</span><span class="o">.</span><span class="n">K_MM</span><span class="p">[</span><span class="n">c</span><span class="p">][</span>
<a id="__codelineno-0-2242" name="__codelineno-0-2242"></a>                    <span class="n">j</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="p">:</span> <span class="nb">min</span><span class="p">(</span>
<a id="__codelineno-0-2243" name="__codelineno-0-2243"></a>                        <span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_subset</span>
<a id="__codelineno-0-2244" name="__codelineno-0-2244"></a>                    <span class="p">),</span>
<a id="__codelineno-0-2245" name="__codelineno-0-2245"></a>                    <span class="n">i</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="p">:</span> <span class="nb">min</span><span class="p">(</span>
<a id="__codelineno-0-2246" name="__codelineno-0-2246"></a>                        <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_subset</span>
<a id="__codelineno-0-2247" name="__codelineno-0-2247"></a>                    <span class="p">),</span>
<a id="__codelineno-0-2248" name="__codelineno-0-2248"></a>                <span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">K_batch</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">c</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-2249" name="__codelineno-0-2249"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-2250" name="__codelineno-0-2250"></a>        <span class="n">bC</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span>
<a id="__codelineno-0-2251" name="__codelineno-0-2251"></a>        <span class="n">MC</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_subset</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span>
<a id="__codelineno-0-2252" name="__codelineno-0-2252"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">K_MM</span><span class="p">[</span>
<a id="__codelineno-0-2253" name="__codelineno-0-2253"></a>            <span class="n">i</span> <span class="o">*</span> <span class="n">bC</span> <span class="p">:</span> <span class="nb">min</span><span class="p">((</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">bC</span><span class="p">,</span> <span class="n">MC</span><span class="p">),</span> <span class="n">j</span> <span class="o">*</span> <span class="n">bC</span> <span class="p">:</span> <span class="nb">min</span><span class="p">((</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">bC</span><span class="p">,</span> <span class="n">MC</span><span class="p">)</span>
<a id="__codelineno-0-2254" name="__codelineno-0-2254"></a>        <span class="p">]</span> <span class="o">=</span> <span class="n">K_batch</span>
<a id="__codelineno-0-2255" name="__codelineno-0-2255"></a>        <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">j</span><span class="p">:</span>
<a id="__codelineno-0-2256" name="__codelineno-0-2256"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">K_MM</span><span class="p">[</span>
<a id="__codelineno-0-2257" name="__codelineno-0-2257"></a>                <span class="n">j</span> <span class="o">*</span> <span class="n">bC</span> <span class="p">:</span> <span class="nb">min</span><span class="p">((</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">bC</span><span class="p">,</span> <span class="n">MC</span><span class="p">),</span> <span class="n">i</span> <span class="o">*</span> <span class="n">bC</span> <span class="p">:</span> <span class="nb">min</span><span class="p">((</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">bC</span><span class="p">,</span> <span class="n">MC</span><span class="p">)</span>
<a id="__codelineno-0-2258" name="__codelineno-0-2258"></a>            <span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">K_batch</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="laplace.baselaplace.FunctionalLaplace._build_L" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">_build_L</span>


<a href="#laplace.baselaplace.FunctionalLaplace._build_L" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nf">_build_L</span><span class="p">(</span><span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._build_L(lambdas)">lambdas</a></span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#list">list</a></span><span class="p">[</span><span class="n"><span title="torch.Tensor">Tensor</span></span><span class="p">])</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Given a list of the Hessians of per-batch log-likelihood w.r.t. neural network output <span class="arithmatex">\( f \)</span>,
returns the contatenation of these hessians in a suitable format for the used kernel
(diagonal or not).</p>
<p>In this function the diagonal approximation is performed. Please refer to the introduction of the
class for more details.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._build_L(lambdas)" class="doc doc-heading doc-heading-parameter">              <b><code>lambdas</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._build_L(lambdas)" class="headerlink" title="Permanent link">#</a></h4>              (<code>list of torch.Tensor of shape (C, C)</code>)
          –
          <div class="doc-md-description">
            <div class="highlight"><pre><span></span><code>  Contains per-batch log-likelihood w.r.t. neural network output \( f \).
</code></pre></div>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>L</code></b> (              <code>list with length C of tensors with shape M or tensor (MxC)</code>
)          –
          <div class="doc-md-description">
            <p>Contains the given Hessians in a suitable format.</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>laplace/baselaplace.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-2260">2260</a></span>
<span class="normal"><a href="#__codelineno-0-2261">2261</a></span>
<span class="normal"><a href="#__codelineno-0-2262">2262</a></span>
<span class="normal"><a href="#__codelineno-0-2263">2263</a></span>
<span class="normal"><a href="#__codelineno-0-2264">2264</a></span>
<span class="normal"><a href="#__codelineno-0-2265">2265</a></span>
<span class="normal"><a href="#__codelineno-0-2266">2266</a></span>
<span class="normal"><a href="#__codelineno-0-2267">2267</a></span>
<span class="normal"><a href="#__codelineno-0-2268">2268</a></span>
<span class="normal"><a href="#__codelineno-0-2269">2269</a></span>
<span class="normal"><a href="#__codelineno-0-2270">2270</a></span>
<span class="normal"><a href="#__codelineno-0-2271">2271</a></span>
<span class="normal"><a href="#__codelineno-0-2272">2272</a></span>
<span class="normal"><a href="#__codelineno-0-2273">2273</a></span>
<span class="normal"><a href="#__codelineno-0-2274">2274</a></span>
<span class="normal"><a href="#__codelineno-0-2275">2275</a></span>
<span class="normal"><a href="#__codelineno-0-2276">2276</a></span>
<span class="normal"><a href="#__codelineno-0-2277">2277</a></span>
<span class="normal"><a href="#__codelineno-0-2278">2278</a></span>
<span class="normal"><a href="#__codelineno-0-2279">2279</a></span>
<span class="normal"><a href="#__codelineno-0-2280">2280</a></span>
<span class="normal"><a href="#__codelineno-0-2281">2281</a></span>
<span class="normal"><a href="#__codelineno-0-2282">2282</a></span>
<span class="normal"><a href="#__codelineno-0-2283">2283</a></span>
<span class="normal"><a href="#__codelineno-0-2284">2284</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-2260" name="__codelineno-0-2260"></a><span class="k">def</span> <span class="nf">_build_L</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lambdas</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]):</span>
<a id="__codelineno-0-2261" name="__codelineno-0-2261"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Given a list of the Hessians of per-batch log-likelihood w.r.t. neural network output \\( f \\),</span>
<a id="__codelineno-0-2262" name="__codelineno-0-2262"></a><span class="sd">    returns the contatenation of these hessians in a suitable format for the used kernel</span>
<a id="__codelineno-0-2263" name="__codelineno-0-2263"></a><span class="sd">    (diagonal or not).</span>
<a id="__codelineno-0-2264" name="__codelineno-0-2264"></a>
<a id="__codelineno-0-2265" name="__codelineno-0-2265"></a><span class="sd">    In this function the diagonal approximation is performed. Please refer to the introduction of the</span>
<a id="__codelineno-0-2266" name="__codelineno-0-2266"></a><span class="sd">    class for more details.</span>
<a id="__codelineno-0-2267" name="__codelineno-0-2267"></a>
<a id="__codelineno-0-2268" name="__codelineno-0-2268"></a><span class="sd">    Parameters</span>
<a id="__codelineno-0-2269" name="__codelineno-0-2269"></a><span class="sd">    ----------</span>
<a id="__codelineno-0-2270" name="__codelineno-0-2270"></a><span class="sd">    lambdas : list of torch.Tensor of shape (C, C)</span>
<a id="__codelineno-0-2271" name="__codelineno-0-2271"></a><span class="sd">              Contains per-batch log-likelihood w.r.t. neural network output \\( f \\).</span>
<a id="__codelineno-0-2272" name="__codelineno-0-2272"></a>
<a id="__codelineno-0-2273" name="__codelineno-0-2273"></a><span class="sd">    Returns</span>
<a id="__codelineno-0-2274" name="__codelineno-0-2274"></a><span class="sd">    -------</span>
<a id="__codelineno-0-2275" name="__codelineno-0-2275"></a><span class="sd">    L : list with length C of tensors with shape M or tensor (MxC)</span>
<a id="__codelineno-0-2276" name="__codelineno-0-2276"></a><span class="sd">        Contains the given Hessians in a suitable format.</span>
<a id="__codelineno-0-2277" name="__codelineno-0-2277"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-2278" name="__codelineno-0-2278"></a>    <span class="c1"># Concatenate batch dimension and discard non-diagonal entries.</span>
<a id="__codelineno-0-2279" name="__codelineno-0-2279"></a>    <span class="n">L_diag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">lambdas</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">dim1</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim2</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-2280" name="__codelineno-0-2280"></a>
<a id="__codelineno-0-2281" name="__codelineno-0-2281"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">independent_outputs</span><span class="p">:</span>
<a id="__codelineno-0-2282" name="__codelineno-0-2282"></a>        <span class="k">return</span> <span class="p">[</span><span class="n">L_diag</span><span class="p">[</span><span class="n">i</span> <span class="p">::</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">)]</span>
<a id="__codelineno-0-2283" name="__codelineno-0-2283"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-2284" name="__codelineno-0-2284"></a>        <span class="k">return</span> <span class="n">L_diag</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="laplace.baselaplace.FunctionalLaplace._build_Sigma_inv" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">_build_Sigma_inv</span>


<a href="#laplace.baselaplace.FunctionalLaplace._build_Sigma_inv" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nf">_build_Sigma_inv</span><span class="p">()</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Computes the cholesky decomposition of
        [
            K_{MM} + \Lambda_{MM}^{-1}.
        ]
        See See <a href="https://arxiv.org/abs/2008.08400">Improving predictions of Bayesian neural nets via local linearization (Immer et al., 2021)</a>
        Equation 15 for more information.</p>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD
        As the diagonal approximation is performed with \Lambda_{MM} (which is stored in self.L),
=======
        As the diagonal approximation is performed with <span class="arithmatex">\(\Lambda_{MM}\)</span> (which is stored in self.L),</p>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>main
        the code is greatly simplified.</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>

            <details class="quote">
              <summary>Source code in <code>laplace/baselaplace.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-2286">2286</a></span>
<span class="normal"><a href="#__codelineno-0-2287">2287</a></span>
<span class="normal"><a href="#__codelineno-0-2288">2288</a></span>
<span class="normal"><a href="#__codelineno-0-2289">2289</a></span>
<span class="normal"><a href="#__codelineno-0-2290">2290</a></span>
<span class="normal"><a href="#__codelineno-0-2291">2291</a></span>
<span class="normal"><a href="#__codelineno-0-2292">2292</a></span>
<span class="normal"><a href="#__codelineno-0-2293">2293</a></span>
<span class="normal"><a href="#__codelineno-0-2294">2294</a></span>
<span class="normal"><a href="#__codelineno-0-2295">2295</a></span>
<span class="normal"><a href="#__codelineno-0-2296">2296</a></span>
<span class="normal"><a href="#__codelineno-0-2297">2297</a></span>
<span class="normal"><a href="#__codelineno-0-2298">2298</a></span>
<span class="normal"><a href="#__codelineno-0-2299">2299</a></span>
<span class="normal"><a href="#__codelineno-0-2300">2300</a></span>
<span class="normal"><a href="#__codelineno-0-2301">2301</a></span>
<span class="normal"><a href="#__codelineno-0-2302">2302</a></span>
<span class="normal"><a href="#__codelineno-0-2303">2303</a></span>
<span class="normal"><a href="#__codelineno-0-2304">2304</a></span>
<span class="normal"><a href="#__codelineno-0-2305">2305</a></span>
<span class="normal"><a href="#__codelineno-0-2306">2306</a></span>
<span class="normal"><a href="#__codelineno-0-2307">2307</a></span>
<span class="normal"><a href="#__codelineno-0-2308">2308</a></span>
<span class="normal"><a href="#__codelineno-0-2309">2309</a></span>
<span class="normal"><a href="#__codelineno-0-2310">2310</a></span>
<span class="normal"><a href="#__codelineno-0-2311">2311</a></span>
<span class="normal"><a href="#__codelineno-0-2312">2312</a></span>
<span class="normal"><a href="#__codelineno-0-2313">2313</a></span>
<span class="normal"><a href="#__codelineno-0-2314">2314</a></span>
<span class="normal"><a href="#__codelineno-0-2315">2315</a></span>
<span class="normal"><a href="#__codelineno-0-2316">2316</a></span>
<span class="normal"><a href="#__codelineno-0-2317">2317</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-2286" name="__codelineno-0-2286"></a><span class="k">def</span> <span class="nf">_build_Sigma_inv</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<a id="__codelineno-0-2287" name="__codelineno-0-2287"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Computes the cholesky decomposition of</span>
<a id="__codelineno-0-2288" name="__codelineno-0-2288"></a><span class="sd">            \\[</span>
<a id="__codelineno-0-2289" name="__codelineno-0-2289"></a><span class="sd">                K_{MM} + \\Lambda_{MM}^{-1}.</span>
<a id="__codelineno-0-2290" name="__codelineno-0-2290"></a><span class="sd">            \\]</span>
<a id="__codelineno-0-2291" name="__codelineno-0-2291"></a><span class="sd">            See See [Improving predictions of Bayesian neural nets via local linearization (Immer et al., 2021)](https://arxiv.org/abs/2008.08400)</span>
<a id="__codelineno-0-2292" name="__codelineno-0-2292"></a><span class="sd">            Equation 15 for more information.</span>
<a id="__codelineno-0-2293" name="__codelineno-0-2293"></a>
<a id="__codelineno-0-2294" name="__codelineno-0-2294"></a><span class="sd">    &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</span>
<a id="__codelineno-0-2295" name="__codelineno-0-2295"></a><span class="sd">            As the diagonal approximation is performed with \\Lambda_{MM} (which is stored in self.L),</span>
<a id="__codelineno-0-2296" name="__codelineno-0-2296"></a><span class="sd">    =======</span>
<a id="__codelineno-0-2297" name="__codelineno-0-2297"></a><span class="sd">            As the diagonal approximation is performed with \\(\\Lambda_{MM}\\) (which is stored in self.L),</span>
<a id="__codelineno-0-2298" name="__codelineno-0-2298"></a><span class="sd">    &gt;&gt;&gt;&gt;&gt;&gt;&gt; main</span>
<a id="__codelineno-0-2299" name="__codelineno-0-2299"></a><span class="sd">            the code is greatly simplified.</span>
<a id="__codelineno-0-2300" name="__codelineno-0-2300"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-2301" name="__codelineno-0-2301"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">independent_outputs</span><span class="p">:</span>
<a id="__codelineno-0-2302" name="__codelineno-0-2302"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">Sigma_inv</span> <span class="o">=</span> <span class="p">[</span>
<a id="__codelineno-0-2303" name="__codelineno-0-2303"></a>            <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span>
<a id="__codelineno-0-2304" name="__codelineno-0-2304"></a>                <span class="bp">self</span><span class="o">.</span><span class="n">gp_kernel_prior_variance</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">K_MM</span><span class="p">[</span><span class="n">c</span><span class="p">]</span>
<a id="__codelineno-0-2305" name="__codelineno-0-2305"></a>                <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span>
<a id="__codelineno-0-2306" name="__codelineno-0-2306"></a>                    <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_H_factor</span> <span class="o">*</span> <span class="n">lambda_c</span><span class="p">),</span> <span class="n">posinf</span><span class="o">=</span><span class="mf">10.0</span><span class="p">)</span>
<a id="__codelineno-0-2307" name="__codelineno-0-2307"></a>                <span class="p">)</span>
<a id="__codelineno-0-2308" name="__codelineno-0-2308"></a>            <span class="p">)</span>
<a id="__codelineno-0-2309" name="__codelineno-0-2309"></a>            <span class="k">for</span> <span class="n">c</span><span class="p">,</span> <span class="n">lambda_c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">)</span>
<a id="__codelineno-0-2310" name="__codelineno-0-2310"></a>        <span class="p">]</span>
<a id="__codelineno-0-2311" name="__codelineno-0-2311"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-2312" name="__codelineno-0-2312"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">Sigma_inv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span>
<a id="__codelineno-0-2313" name="__codelineno-0-2313"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">gp_kernel_prior_variance</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">K_MM</span>
<a id="__codelineno-0-2314" name="__codelineno-0-2314"></a>            <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span>
<a id="__codelineno-0-2315" name="__codelineno-0-2315"></a>                <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_H_factor</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">),</span> <span class="n">posinf</span><span class="o">=</span><span class="mf">10.0</span><span class="p">)</span>
<a id="__codelineno-0-2316" name="__codelineno-0-2316"></a>            <span class="p">)</span>
<a id="__codelineno-0-2317" name="__codelineno-0-2317"></a>        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="laplace.baselaplace.FunctionalLaplace._get_SoD_data_loader" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">_get_SoD_data_loader</span>


<a href="#laplace.baselaplace.FunctionalLaplace._get_SoD_data_loader" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nf">_get_SoD_data_loader</span><span class="p">(</span><span class="n">train_loader</span><span class="p">:</span> <span class="n"><span title="torch.utils.data.DataLoader">DataLoader</span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="torch.utils.data.DataLoader">DataLoader</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Subset-of-Datapoints data loader</p>

            <details class="quote">
              <summary>Source code in <code>laplace/baselaplace.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-2319">2319</a></span>
<span class="normal"><a href="#__codelineno-0-2320">2320</a></span>
<span class="normal"><a href="#__codelineno-0-2321">2321</a></span>
<span class="normal"><a href="#__codelineno-0-2322">2322</a></span>
<span class="normal"><a href="#__codelineno-0-2323">2323</a></span>
<span class="normal"><a href="#__codelineno-0-2324">2324</a></span>
<span class="normal"><a href="#__codelineno-0-2325">2325</a></span>
<span class="normal"><a href="#__codelineno-0-2326">2326</a></span>
<span class="normal"><a href="#__codelineno-0-2327">2327</a></span>
<span class="normal"><a href="#__codelineno-0-2328">2328</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-2319" name="__codelineno-0-2319"></a><span class="k">def</span> <span class="nf">_get_SoD_data_loader</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataLoader</span><span class="p">:</span>
<a id="__codelineno-0-2320" name="__codelineno-0-2320"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Subset-of-Datapoints data loader&quot;&quot;&quot;</span>
<a id="__codelineno-0-2321" name="__codelineno-0-2321"></a>    <span class="k">return</span> <span class="n">DataLoader</span><span class="p">(</span>
<a id="__codelineno-0-2322" name="__codelineno-0-2322"></a>        <span class="n">dataset</span><span class="o">=</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span>
<a id="__codelineno-0-2323" name="__codelineno-0-2323"></a>        <span class="n">batch_size</span><span class="o">=</span><span class="n">train_loader</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
<a id="__codelineno-0-2324" name="__codelineno-0-2324"></a>        <span class="n">sampler</span><span class="o">=</span><span class="n">SoDSampler</span><span class="p">(</span>
<a id="__codelineno-0-2325" name="__codelineno-0-2325"></a>            <span class="n">N</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">),</span> <span class="n">M</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_subset</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">seed</span>
<a id="__codelineno-0-2326" name="__codelineno-0-2326"></a>        <span class="p">),</span>
<a id="__codelineno-0-2327" name="__codelineno-0-2327"></a>        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-0-2328" name="__codelineno-0-2328"></a>    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="laplace.baselaplace.FunctionalLaplace.fit" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">fit</span>


<a href="#laplace.baselaplace.FunctionalLaplace.fit" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nf">fit</span><span class="p">(</span><span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace.fit(train_loader)">train_loader</a></span><span class="p">:</span> <span class="n"><span title="torch.utils.data.DataLoader">DataLoader</span></span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" title="collections.abc.MutableMapping" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.MutableMapping">MutableMapping</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace.fit(progress_bar)">progress_bar</a></span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Fit the Laplace approximation of a GP posterior.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace.fit(train_loader)" class="doc doc-heading doc-heading-parameter">              <b><code>train_loader</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace.fit(train_loader)" class="headerlink" title="Permanent link">#</a></h4>              (<code><span title="torch.data.utils.DataLoader">DataLoader</span></code>)
          –
          <div class="doc-md-description">
            <p><code>train_loader.dataset</code> needs to be set to access <span class="arithmatex">\(N\)</span>, size of the data set
<code>train_loader.batch_size</code> needs to be set to access <span class="arithmatex">\(b\)</span> batch_size</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace.fit(progress_bar)" class="doc doc-heading doc-heading-parameter">              <b><code>progress_bar</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace.fit(progress_bar)" class="headerlink" title="Permanent link">#</a></h4>              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>whether to show a progress bar during the fitting process.</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>laplace/baselaplace.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-2330">2330</a></span>
<span class="normal"><a href="#__codelineno-0-2331">2331</a></span>
<span class="normal"><a href="#__codelineno-0-2332">2332</a></span>
<span class="normal"><a href="#__codelineno-0-2333">2333</a></span>
<span class="normal"><a href="#__codelineno-0-2334">2334</a></span>
<span class="normal"><a href="#__codelineno-0-2335">2335</a></span>
<span class="normal"><a href="#__codelineno-0-2336">2336</a></span>
<span class="normal"><a href="#__codelineno-0-2337">2337</a></span>
<span class="normal"><a href="#__codelineno-0-2338">2338</a></span>
<span class="normal"><a href="#__codelineno-0-2339">2339</a></span>
<span class="normal"><a href="#__codelineno-0-2340">2340</a></span>
<span class="normal"><a href="#__codelineno-0-2341">2341</a></span>
<span class="normal"><a href="#__codelineno-0-2342">2342</a></span>
<span class="normal"><a href="#__codelineno-0-2343">2343</a></span>
<span class="normal"><a href="#__codelineno-0-2344">2344</a></span>
<span class="normal"><a href="#__codelineno-0-2345">2345</a></span>
<span class="normal"><a href="#__codelineno-0-2346">2346</a></span>
<span class="normal"><a href="#__codelineno-0-2347">2347</a></span>
<span class="normal"><a href="#__codelineno-0-2348">2348</a></span>
<span class="normal"><a href="#__codelineno-0-2349">2349</a></span>
<span class="normal"><a href="#__codelineno-0-2350">2350</a></span>
<span class="normal"><a href="#__codelineno-0-2351">2351</a></span>
<span class="normal"><a href="#__codelineno-0-2352">2352</a></span>
<span class="normal"><a href="#__codelineno-0-2353">2353</a></span>
<span class="normal"><a href="#__codelineno-0-2354">2354</a></span>
<span class="normal"><a href="#__codelineno-0-2355">2355</a></span>
<span class="normal"><a href="#__codelineno-0-2356">2356</a></span>
<span class="normal"><a href="#__codelineno-0-2357">2357</a></span>
<span class="normal"><a href="#__codelineno-0-2358">2358</a></span>
<span class="normal"><a href="#__codelineno-0-2359">2359</a></span>
<span class="normal"><a href="#__codelineno-0-2360">2360</a></span>
<span class="normal"><a href="#__codelineno-0-2361">2361</a></span>
<span class="normal"><a href="#__codelineno-0-2362">2362</a></span>
<span class="normal"><a href="#__codelineno-0-2363">2363</a></span>
<span class="normal"><a href="#__codelineno-0-2364">2364</a></span>
<span class="normal"><a href="#__codelineno-0-2365">2365</a></span>
<span class="normal"><a href="#__codelineno-0-2366">2366</a></span>
<span class="normal"><a href="#__codelineno-0-2367">2367</a></span>
<span class="normal"><a href="#__codelineno-0-2368">2368</a></span>
<span class="normal"><a href="#__codelineno-0-2369">2369</a></span>
<span class="normal"><a href="#__codelineno-0-2370">2370</a></span>
<span class="normal"><a href="#__codelineno-0-2371">2371</a></span>
<span class="normal"><a href="#__codelineno-0-2372">2372</a></span>
<span class="normal"><a href="#__codelineno-0-2373">2373</a></span>
<span class="normal"><a href="#__codelineno-0-2374">2374</a></span>
<span class="normal"><a href="#__codelineno-0-2375">2375</a></span>
<span class="normal"><a href="#__codelineno-0-2376">2376</a></span>
<span class="normal"><a href="#__codelineno-0-2377">2377</a></span>
<span class="normal"><a href="#__codelineno-0-2378">2378</a></span>
<span class="normal"><a href="#__codelineno-0-2379">2379</a></span>
<span class="normal"><a href="#__codelineno-0-2380">2380</a></span>
<span class="normal"><a href="#__codelineno-0-2381">2381</a></span>
<span class="normal"><a href="#__codelineno-0-2382">2382</a></span>
<span class="normal"><a href="#__codelineno-0-2383">2383</a></span>
<span class="normal"><a href="#__codelineno-0-2384">2384</a></span>
<span class="normal"><a href="#__codelineno-0-2385">2385</a></span>
<span class="normal"><a href="#__codelineno-0-2386">2386</a></span>
<span class="normal"><a href="#__codelineno-0-2387">2387</a></span>
<span class="normal"><a href="#__codelineno-0-2388">2388</a></span>
<span class="normal"><a href="#__codelineno-0-2389">2389</a></span>
<span class="normal"><a href="#__codelineno-0-2390">2390</a></span>
<span class="normal"><a href="#__codelineno-0-2391">2391</a></span>
<span class="normal"><a href="#__codelineno-0-2392">2392</a></span>
<span class="normal"><a href="#__codelineno-0-2393">2393</a></span>
<span class="normal"><a href="#__codelineno-0-2394">2394</a></span>
<span class="normal"><a href="#__codelineno-0-2395">2395</a></span>
<span class="normal"><a href="#__codelineno-0-2396">2396</a></span>
<span class="normal"><a href="#__codelineno-0-2397">2397</a></span>
<span class="normal"><a href="#__codelineno-0-2398">2398</a></span>
<span class="normal"><a href="#__codelineno-0-2399">2399</a></span>
<span class="normal"><a href="#__codelineno-0-2400">2400</a></span>
<span class="normal"><a href="#__codelineno-0-2401">2401</a></span>
<span class="normal"><a href="#__codelineno-0-2402">2402</a></span>
<span class="normal"><a href="#__codelineno-0-2403">2403</a></span>
<span class="normal"><a href="#__codelineno-0-2404">2404</a></span>
<span class="normal"><a href="#__codelineno-0-2405">2405</a></span>
<span class="normal"><a href="#__codelineno-0-2406">2406</a></span>
<span class="normal"><a href="#__codelineno-0-2407">2407</a></span>
<span class="normal"><a href="#__codelineno-0-2408">2408</a></span>
<span class="normal"><a href="#__codelineno-0-2409">2409</a></span>
<span class="normal"><a href="#__codelineno-0-2410">2410</a></span>
<span class="normal"><a href="#__codelineno-0-2411">2411</a></span>
<span class="normal"><a href="#__codelineno-0-2412">2412</a></span>
<span class="normal"><a href="#__codelineno-0-2413">2413</a></span>
<span class="normal"><a href="#__codelineno-0-2414">2414</a></span>
<span class="normal"><a href="#__codelineno-0-2415">2415</a></span>
<span class="normal"><a href="#__codelineno-0-2416">2416</a></span>
<span class="normal"><a href="#__codelineno-0-2417">2417</a></span>
<span class="normal"><a href="#__codelineno-0-2418">2418</a></span>
<span class="normal"><a href="#__codelineno-0-2419">2419</a></span>
<span class="normal"><a href="#__codelineno-0-2420">2420</a></span>
<span class="normal"><a href="#__codelineno-0-2421">2421</a></span>
<span class="normal"><a href="#__codelineno-0-2422">2422</a></span>
<span class="normal"><a href="#__codelineno-0-2423">2423</a></span>
<span class="normal"><a href="#__codelineno-0-2424">2424</a></span>
<span class="normal"><a href="#__codelineno-0-2425">2425</a></span>
<span class="normal"><a href="#__codelineno-0-2426">2426</a></span>
<span class="normal"><a href="#__codelineno-0-2427">2427</a></span>
<span class="normal"><a href="#__codelineno-0-2428">2428</a></span>
<span class="normal"><a href="#__codelineno-0-2429">2429</a></span>
<span class="normal"><a href="#__codelineno-0-2430">2430</a></span>
<span class="normal"><a href="#__codelineno-0-2431">2431</a></span>
<span class="normal"><a href="#__codelineno-0-2432">2432</a></span>
<span class="normal"><a href="#__codelineno-0-2433">2433</a></span>
<span class="normal"><a href="#__codelineno-0-2434">2434</a></span>
<span class="normal"><a href="#__codelineno-0-2435">2435</a></span>
<span class="normal"><a href="#__codelineno-0-2436">2436</a></span>
<span class="normal"><a href="#__codelineno-0-2437">2437</a></span>
<span class="normal"><a href="#__codelineno-0-2438">2438</a></span>
<span class="normal"><a href="#__codelineno-0-2439">2439</a></span>
<span class="normal"><a href="#__codelineno-0-2440">2440</a></span>
<span class="normal"><a href="#__codelineno-0-2441">2441</a></span>
<span class="normal"><a href="#__codelineno-0-2442">2442</a></span>
<span class="normal"><a href="#__codelineno-0-2443">2443</a></span>
<span class="normal"><a href="#__codelineno-0-2444">2444</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-2330" name="__codelineno-0-2330"></a><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span>
<a id="__codelineno-0-2331" name="__codelineno-0-2331"></a>    <span class="bp">self</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">:</span> <span class="n">DataLoader</span> <span class="o">|</span> <span class="n">MutableMapping</span><span class="p">,</span> <span class="n">progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<a id="__codelineno-0-2332" name="__codelineno-0-2332"></a><span class="p">):</span>
<a id="__codelineno-0-2333" name="__codelineno-0-2333"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Fit the Laplace approximation of a GP posterior.</span>
<a id="__codelineno-0-2334" name="__codelineno-0-2334"></a>
<a id="__codelineno-0-2335" name="__codelineno-0-2335"></a><span class="sd">    Parameters</span>
<a id="__codelineno-0-2336" name="__codelineno-0-2336"></a><span class="sd">    ----------</span>
<a id="__codelineno-0-2337" name="__codelineno-0-2337"></a><span class="sd">    train_loader : torch.data.utils.DataLoader</span>
<a id="__codelineno-0-2338" name="__codelineno-0-2338"></a><span class="sd">        `train_loader.dataset` needs to be set to access \\(N\\), size of the data set</span>
<a id="__codelineno-0-2339" name="__codelineno-0-2339"></a><span class="sd">        `train_loader.batch_size` needs to be set to access \\(b\\) batch_size</span>
<a id="__codelineno-0-2340" name="__codelineno-0-2340"></a><span class="sd">    progress_bar : bool</span>
<a id="__codelineno-0-2341" name="__codelineno-0-2341"></a><span class="sd">        whether to show a progress bar during the fitting process.</span>
<a id="__codelineno-0-2342" name="__codelineno-0-2342"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-2343" name="__codelineno-0-2343"></a>    <span class="c1"># Set model to evaluation mode</span>
<a id="__codelineno-0-2344" name="__codelineno-0-2344"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<a id="__codelineno-0-2345" name="__codelineno-0-2345"></a>
<a id="__codelineno-0-2346" name="__codelineno-0-2346"></a>    <span class="n">data</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<a id="__codelineno-0-2347" name="__codelineno-0-2347"></a>    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
<a id="__codelineno-0-2348" name="__codelineno-0-2348"></a>        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">MutableMapping</span><span class="p">):</span>  <span class="c1"># To support Huggingface dataset</span>
<a id="__codelineno-0-2349" name="__codelineno-0-2349"></a>            <span class="k">if</span> <span class="s2">&quot;backpack&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backend_cls</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
<a id="__codelineno-0-2350" name="__codelineno-0-2350"></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<a id="__codelineno-0-2351" name="__codelineno-0-2351"></a>                    <span class="s2">&quot;Currently BackPACK backend is not supported &quot;</span>
<a id="__codelineno-0-2352" name="__codelineno-0-2352"></a>                    <span class="o">+</span> <span class="s2">&quot;for custom models with non-tensor inputs &quot;</span>
<a id="__codelineno-0-2353" name="__codelineno-0-2353"></a>                    <span class="o">+</span> <span class="s2">&quot;(https://github.com/pytorch/functorch/issues/159). Consider &quot;</span>
<a id="__codelineno-0-2354" name="__codelineno-0-2354"></a>                    <span class="o">+</span> <span class="s2">&quot;using AsdlGGN backend instead.&quot;</span>
<a id="__codelineno-0-2355" name="__codelineno-0-2355"></a>                <span class="p">)</span>
<a id="__codelineno-0-2356" name="__codelineno-0-2356"></a>
<a id="__codelineno-0-2357" name="__codelineno-0-2357"></a>            <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<a id="__codelineno-0-2358" name="__codelineno-0-2358"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-2359" name="__codelineno-0-2359"></a>            <span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<a id="__codelineno-0-2360" name="__codelineno-0-2360"></a>            <span class="k">try</span><span class="p">:</span>
<a id="__codelineno-0-2361" name="__codelineno-0-2361"></a>                <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">))</span>
<a id="__codelineno-0-2362" name="__codelineno-0-2362"></a>            <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">AttributeError</span><span class="p">):</span>
<a id="__codelineno-0-2363" name="__codelineno-0-2363"></a>                <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">))</span>
<a id="__codelineno-0-2364" name="__codelineno-0-2364"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<a id="__codelineno-0-2365" name="__codelineno-0-2365"></a>    <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;output_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">)</span>
<a id="__codelineno-0-2366" name="__codelineno-0-2366"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">train_loader</span><span class="o">.</span><span class="n">batch_size</span>
<a id="__codelineno-0-2367" name="__codelineno-0-2367"></a>
<a id="__codelineno-0-2368" name="__codelineno-0-2368"></a>    <span class="k">if</span> <span class="p">(</span>
<a id="__codelineno-0-2369" name="__codelineno-0-2369"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span> <span class="o">==</span> <span class="s2">&quot;regression&quot;</span>
<a id="__codelineno-0-2370" name="__codelineno-0-2370"></a>        <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span> <span class="o">&gt;</span> <span class="mi">1</span>
<a id="__codelineno-0-2371" name="__codelineno-0-2371"></a>        <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">independent_outputs</span>
<a id="__codelineno-0-2372" name="__codelineno-0-2372"></a>    <span class="p">):</span>
<a id="__codelineno-0-2373" name="__codelineno-0-2373"></a>        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
<a id="__codelineno-0-2374" name="__codelineno-0-2374"></a>            <span class="s2">&quot;Using FunctionalLaplace with the diagonal approximation of a GP kernel is not recommended &quot;</span>
<a id="__codelineno-0-2375" name="__codelineno-0-2375"></a>            <span class="s2">&quot;in the case of multivariate regression. Predictive variance will likely be overestimated.&quot;</span>
<a id="__codelineno-0-2376" name="__codelineno-0-2376"></a>        <span class="p">)</span>
<a id="__codelineno-0-2377" name="__codelineno-0-2377"></a>
<a id="__codelineno-0-2378" name="__codelineno-0-2378"></a>    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
<a id="__codelineno-0-2379" name="__codelineno-0-2379"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">n_data</span> <span class="o">=</span> <span class="n">N</span>
<a id="__codelineno-0-2380" name="__codelineno-0-2380"></a>
<a id="__codelineno-0-2381" name="__codelineno-0-2381"></a>    <span class="k">assert</span> <span class="p">(</span>
<a id="__codelineno-0-2382" name="__codelineno-0-2382"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">n_subset</span> <span class="o">&lt;=</span> <span class="n">N</span>
<a id="__codelineno-0-2383" name="__codelineno-0-2383"></a>    <span class="p">),</span> <span class="s2">&quot;`num_data` must be less than or equal to the original number of data points.&quot;</span>
<a id="__codelineno-0-2384" name="__codelineno-0-2384"></a>
<a id="__codelineno-0-2385" name="__codelineno-0-2385"></a>    <span class="n">train_loader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_SoD_data_loader</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
<a id="__codelineno-0-2386" name="__codelineno-0-2386"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">train_loader</span>
<a id="__codelineno-0-2387" name="__codelineno-0-2387"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">_prior_factor_sod</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_subset</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_data</span>
<a id="__codelineno-0-2388" name="__codelineno-0-2388"></a>
<a id="__codelineno-0-2389" name="__codelineno-0-2389"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">_init_K_MM</span><span class="p">()</span>
<a id="__codelineno-0-2390" name="__codelineno-0-2390"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">_init_Sigma_inv</span><span class="p">()</span>
<a id="__codelineno-0-2391" name="__codelineno-0-2391"></a>
<a id="__codelineno-0-2392" name="__codelineno-0-2392"></a>    <span class="n">f</span><span class="p">,</span> <span class="n">lambdas</span><span class="p">,</span> <span class="n">mu</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
<a id="__codelineno-0-2393" name="__codelineno-0-2393"></a>
<a id="__codelineno-0-2394" name="__codelineno-0-2394"></a>    <span class="k">if</span> <span class="n">progress_bar</span><span class="p">:</span>
<a id="__codelineno-0-2395" name="__codelineno-0-2395"></a>        <span class="n">loader</span> <span class="o">=</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Fitting&quot;</span><span class="p">))</span>
<a id="__codelineno-0-2396" name="__codelineno-0-2396"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-2397" name="__codelineno-0-2397"></a>        <span class="n">loader</span> <span class="o">=</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
<a id="__codelineno-0-2398" name="__codelineno-0-2398"></a>
<a id="__codelineno-0-2399" name="__codelineno-0-2399"></a>    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
<a id="__codelineno-0-2400" name="__codelineno-0-2400"></a>        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">MutableMapping</span><span class="p">):</span>  <span class="c1"># To support Huggingface dataset</span>
<a id="__codelineno-0-2401" name="__codelineno-0-2401"></a>            <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">dict_key_y</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
<a id="__codelineno-0-2402" name="__codelineno-0-2402"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-2403" name="__codelineno-0-2403"></a>            <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span>
<a id="__codelineno-0-2404" name="__codelineno-0-2404"></a>            <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
<a id="__codelineno-0-2405" name="__codelineno-0-2405"></a>
<a id="__codelineno-0-2406" name="__codelineno-0-2406"></a>        <span class="n">Js_batch</span><span class="p">,</span> <span class="n">f_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jacobians</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">enable_backprop</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<a id="__codelineno-0-2407" name="__codelineno-0-2407"></a>
<a id="__codelineno-0-2408" name="__codelineno-0-2408"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span> <span class="o">==</span> <span class="n">Likelihood</span><span class="o">.</span><span class="n">REGRESSION</span> <span class="ow">and</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="n">out</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
<a id="__codelineno-0-2409" name="__codelineno-0-2409"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<a id="__codelineno-0-2410" name="__codelineno-0-2410"></a>                <span class="sa">f</span><span class="s2">&quot;The model&#39;s output has </span><span class="si">{</span><span class="n">out</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2"> dims but &quot;</span>
<a id="__codelineno-0-2411" name="__codelineno-0-2411"></a>                <span class="sa">f</span><span class="s2">&quot;the target has </span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2"> dims.&quot;</span>
<a id="__codelineno-0-2412" name="__codelineno-0-2412"></a>            <span class="p">)</span>
<a id="__codelineno-0-2413" name="__codelineno-0-2413"></a>
<a id="__codelineno-0-2414" name="__codelineno-0-2414"></a>        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
<a id="__codelineno-0-2415" name="__codelineno-0-2415"></a>            <span class="n">loss_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">factor</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">lossfunc</span><span class="p">(</span><span class="n">f_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<a id="__codelineno-0-2416" name="__codelineno-0-2416"></a>
<a id="__codelineno-0-2417" name="__codelineno-0-2417"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span> <span class="o">==</span> <span class="n">Likelihood</span><span class="o">.</span><span class="n">REGRESSION</span><span class="p">:</span>
<a id="__codelineno-0-2418" name="__codelineno-0-2418"></a>            <span class="n">b</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">f_batch</span><span class="o">.</span><span class="n">shape</span>
<a id="__codelineno-0-2419" name="__codelineno-0-2419"></a>            <span class="n">lambdas_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span>
<a id="__codelineno-0-2420" name="__codelineno-0-2420"></a>                <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">),</span> <span class="mi">0</span>
<a id="__codelineno-0-2421" name="__codelineno-0-2421"></a>            <span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-2422" name="__codelineno-0-2422"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-2423" name="__codelineno-0-2423"></a>            <span class="c1"># second derivative of log lik is diag(p) - pp^T</span>
<a id="__codelineno-0-2424" name="__codelineno-0-2424"></a>            <span class="n">ps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">f_batch</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-2425" name="__codelineno-0-2425"></a>            <span class="n">lambdas_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag_embed</span><span class="p">(</span><span class="n">ps</span><span class="p">)</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
<a id="__codelineno-0-2426" name="__codelineno-0-2426"></a>                <span class="s2">&quot;mk,mc-&gt;mck&quot;</span><span class="p">,</span> <span class="n">ps</span><span class="p">,</span> <span class="n">ps</span>
<a id="__codelineno-0-2427" name="__codelineno-0-2427"></a>            <span class="p">)</span>
<a id="__codelineno-0-2428" name="__codelineno-0-2428"></a>
<a id="__codelineno-0-2429" name="__codelineno-0-2429"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">+=</span> <span class="n">loss_batch</span>
<a id="__codelineno-0-2430" name="__codelineno-0-2430"></a>        <span class="n">lambdas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lambdas_batch</span><span class="p">)</span>
<a id="__codelineno-0-2431" name="__codelineno-0-2431"></a>        <span class="n">f</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">f_batch</span><span class="p">)</span>
<a id="__codelineno-0-2432" name="__codelineno-0-2432"></a>        <span class="n">mu</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
<a id="__codelineno-0-2433" name="__codelineno-0-2433"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_mean_scatter_term_batch</span><span class="p">(</span><span class="n">Js_batch</span><span class="p">,</span> <span class="n">f_batch</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<a id="__codelineno-0-2434" name="__codelineno-0-2434"></a>        <span class="p">)</span>  <span class="c1"># needed for marginal likelihood</span>
<a id="__codelineno-0-2435" name="__codelineno-0-2435"></a>        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="p">(</span><span class="n">X2</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
<a id="__codelineno-0-2436" name="__codelineno-0-2436"></a>            <span class="k">if</span> <span class="n">j</span> <span class="o">&gt;=</span> <span class="n">i</span><span class="p">:</span>
<a id="__codelineno-0-2437" name="__codelineno-0-2437"></a>                <span class="n">X2</span> <span class="o">=</span> <span class="n">X2</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
<a id="__codelineno-0-2438" name="__codelineno-0-2438"></a>                <span class="n">K_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_batch</span><span class="p">(</span><span class="n">Js_batch</span><span class="p">,</span> <span class="n">X2</span><span class="p">)</span>
<a id="__codelineno-0-2439" name="__codelineno-0-2439"></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_store_K_batch</span><span class="p">(</span><span class="n">K_batch</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>
<a id="__codelineno-0-2440" name="__codelineno-0-2440"></a>
<a id="__codelineno-0-2441" name="__codelineno-0-2441"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">L</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_L</span><span class="p">(</span><span class="n">lambdas</span><span class="p">)</span>
<a id="__codelineno-0-2442" name="__codelineno-0-2442"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<a id="__codelineno-0-2443" name="__codelineno-0-2443"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">_build_Sigma_inv</span><span class="p">()</span>
<a id="__codelineno-0-2444" name="__codelineno-0-2444"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">_fitted</span> <span class="o">=</span> <span class="kc">True</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="laplace.baselaplace.FunctionalLaplace.__call__" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">__call__</span>


<a href="#laplace.baselaplace.FunctionalLaplace.__call__" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="fm">__call__</span><span class="p">(</span><span class="nf"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace.__call__(x)">x</a></span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" title="collections.abc.MutableMapping" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.MutableMapping">MutableMapping</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace.__call__(pred_type)">pred_type</a></span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="laplace.utils.enums.PredType" href="../enums/#laplace.utils.enums.PredType">PredType</a></span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span> <span class="o">=</span> <span class="n"><a class="autorefs autorefs-internal" title="laplace.utils.enums.PredType.GP" href="../enums/#laplace.utils.enums.PredType.GP">GP</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace.__call__(joint)">joint</a></span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace.__call__(link_approx)">link_approx</a></span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="laplace.utils.enums.LinkApprox" href="../enums/#laplace.utils.enums.LinkApprox">LinkApprox</a></span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span> <span class="o">=</span> <span class="n"><a class="autorefs autorefs-internal" title="laplace.utils.enums.LinkApprox.PROBIT" href="../enums/#laplace.utils.enums.LinkApprox.PROBIT">PROBIT</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace.__call__(n_samples)">n_samples</a></span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace.__call__(diagonal_output)">diagonal_output</a></span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace.__call__(generator)">generator</a></span><span class="p">:</span> <span class="n"><span title="torch.Generator">Generator</span></span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace.__call__(fitting)">fitting</a></span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">**model_kwargs</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#dict">dict</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Any" href="https://docs.python.org/3/library/typing.html#typing.Any">Any</a></span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n"><span title="torch.Tensor">Tensor</span></span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#tuple">tuple</a></span><span class="p">[</span><span class="n"><span title="torch.Tensor">Tensor</span></span><span class="p">,</span> <span class="n"><span title="torch.Tensor">Tensor</span></span><span class="p">]</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Compute the posterior predictive on input data <code>x</code>.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace.__call__(x)" class="doc doc-heading doc-heading-parameter">              <b><code>x</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace.__call__(x)" class="headerlink" title="Permanent link">#</a></h4>              (<code><span title="torch.Tensor">Tensor</span> or <a class="autorefs autorefs-external" title="collections.abc.MutableMapping" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.MutableMapping">MutableMapping</a></code>)
          –
          <div class="doc-md-description">
            <p><code>(batch_size, input_shape)</code> if tensor. If MutableMapping, must contain
the said tensor.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace.__call__(pred_type)" class="doc doc-heading doc-heading-parameter">              <b><code>pred_type</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace.__call__(pred_type)" class="headerlink" title="Permanent link">#</a></h4>              (<code>&#39;gp&#39;</code>, default:
                  <code>&#39;gp&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>type of posterior predictive, linearized GLM predictive (GP).
The GP predictive is consistent with
the curvature approximations used here.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace.__call__(link_approx)" class="doc doc-heading doc-heading-parameter">              <b><code>link_approx</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace.__call__(link_approx)" class="headerlink" title="Permanent link">#</a></h4>              (<code>(&#39;mc&#39;, &#39;probit&#39;, &#39;bridge&#39;, &#39;bridge_norm&#39;)</code>, default:
                  <code>&#39;mc&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>how to approximate the classification link function for the <code>'glm'</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace.__call__(joint)" class="doc doc-heading doc-heading-parameter">              <b><code>joint</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace.__call__(joint)" class="headerlink" title="Permanent link">#</a></h4>              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>Whether to output a joint predictive distribution in regression with
<code>pred_type='glm'</code>. If set to <code>True</code>, the predictive distribution
has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).
If <code>False</code>, then only outputs the marginal predictive distribution.
Only available for regression and GLM predictive.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace.__call__(n_samples)" class="doc doc-heading doc-heading-parameter">              <b><code>n_samples</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace.__call__(n_samples)" class="headerlink" title="Permanent link">#</a></h4>              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>, default:
                  <code>100</code>
)
          –
          <div class="doc-md-description">
            <p>number of samples for <code>link_approx='mc'</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace.__call__(diagonal_output)" class="doc doc-heading doc-heading-parameter">              <b><code>diagonal_output</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace.__call__(diagonal_output)" class="headerlink" title="Permanent link">#</a></h4>              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>whether to use a diagonalized posterior predictive on the outputs.
Only works for <code>link_approx='mc'</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace.__call__(generator)" class="doc doc-heading doc-heading-parameter">              <b><code>generator</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace.__call__(generator)" class="headerlink" title="Permanent link">#</a></h4>              (<code><span title="torch.Generator">Generator</span></code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>random number generator to control the samples (if sampling used).</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace.__call__(fitting)" class="doc doc-heading doc-heading-parameter">              <b><code>fitting</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace.__call__(fitting)" class="headerlink" title="Permanent link">#</a></h4>              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>whether or not this predictive call is done during fitting. Only useful for
reward modeling: the likelihood is set to <code>"regression"</code> when <code>False</code> and
<code>"classification"</code> when <code>True</code>.</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>predictive</code></b> (              <code><span title="torch.Tensor">Tensor</span> or Tuple[<span title="torch.Tensor">Tensor</span>]</code>
)          –
          <div class="doc-md-description">
            <p>For <code>likelihood='classification'</code>, a torch.Tensor is returned with
a distribution over classes (similar to a Softmax).
For <code>likelihood='regression'</code>, a tuple of torch.Tensor is returned
with the mean and the predictive variance.
For <code>likelihood='regression'</code> and <code>joint=True</code>, a tuple of torch.Tensor
is returned with the mean and the predictive covariance.</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>laplace/baselaplace.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-2462">2462</a></span>
<span class="normal"><a href="#__codelineno-0-2463">2463</a></span>
<span class="normal"><a href="#__codelineno-0-2464">2464</a></span>
<span class="normal"><a href="#__codelineno-0-2465">2465</a></span>
<span class="normal"><a href="#__codelineno-0-2466">2466</a></span>
<span class="normal"><a href="#__codelineno-0-2467">2467</a></span>
<span class="normal"><a href="#__codelineno-0-2468">2468</a></span>
<span class="normal"><a href="#__codelineno-0-2469">2469</a></span>
<span class="normal"><a href="#__codelineno-0-2470">2470</a></span>
<span class="normal"><a href="#__codelineno-0-2471">2471</a></span>
<span class="normal"><a href="#__codelineno-0-2472">2472</a></span>
<span class="normal"><a href="#__codelineno-0-2473">2473</a></span>
<span class="normal"><a href="#__codelineno-0-2474">2474</a></span>
<span class="normal"><a href="#__codelineno-0-2475">2475</a></span>
<span class="normal"><a href="#__codelineno-0-2476">2476</a></span>
<span class="normal"><a href="#__codelineno-0-2477">2477</a></span>
<span class="normal"><a href="#__codelineno-0-2478">2478</a></span>
<span class="normal"><a href="#__codelineno-0-2479">2479</a></span>
<span class="normal"><a href="#__codelineno-0-2480">2480</a></span>
<span class="normal"><a href="#__codelineno-0-2481">2481</a></span>
<span class="normal"><a href="#__codelineno-0-2482">2482</a></span>
<span class="normal"><a href="#__codelineno-0-2483">2483</a></span>
<span class="normal"><a href="#__codelineno-0-2484">2484</a></span>
<span class="normal"><a href="#__codelineno-0-2485">2485</a></span>
<span class="normal"><a href="#__codelineno-0-2486">2486</a></span>
<span class="normal"><a href="#__codelineno-0-2487">2487</a></span>
<span class="normal"><a href="#__codelineno-0-2488">2488</a></span>
<span class="normal"><a href="#__codelineno-0-2489">2489</a></span>
<span class="normal"><a href="#__codelineno-0-2490">2490</a></span>
<span class="normal"><a href="#__codelineno-0-2491">2491</a></span>
<span class="normal"><a href="#__codelineno-0-2492">2492</a></span>
<span class="normal"><a href="#__codelineno-0-2493">2493</a></span>
<span class="normal"><a href="#__codelineno-0-2494">2494</a></span>
<span class="normal"><a href="#__codelineno-0-2495">2495</a></span>
<span class="normal"><a href="#__codelineno-0-2496">2496</a></span>
<span class="normal"><a href="#__codelineno-0-2497">2497</a></span>
<span class="normal"><a href="#__codelineno-0-2498">2498</a></span>
<span class="normal"><a href="#__codelineno-0-2499">2499</a></span>
<span class="normal"><a href="#__codelineno-0-2500">2500</a></span>
<span class="normal"><a href="#__codelineno-0-2501">2501</a></span>
<span class="normal"><a href="#__codelineno-0-2502">2502</a></span>
<span class="normal"><a href="#__codelineno-0-2503">2503</a></span>
<span class="normal"><a href="#__codelineno-0-2504">2504</a></span>
<span class="normal"><a href="#__codelineno-0-2505">2505</a></span>
<span class="normal"><a href="#__codelineno-0-2506">2506</a></span>
<span class="normal"><a href="#__codelineno-0-2507">2507</a></span>
<span class="normal"><a href="#__codelineno-0-2508">2508</a></span>
<span class="normal"><a href="#__codelineno-0-2509">2509</a></span>
<span class="normal"><a href="#__codelineno-0-2510">2510</a></span>
<span class="normal"><a href="#__codelineno-0-2511">2511</a></span>
<span class="normal"><a href="#__codelineno-0-2512">2512</a></span>
<span class="normal"><a href="#__codelineno-0-2513">2513</a></span>
<span class="normal"><a href="#__codelineno-0-2514">2514</a></span>
<span class="normal"><a href="#__codelineno-0-2515">2515</a></span>
<span class="normal"><a href="#__codelineno-0-2516">2516</a></span>
<span class="normal"><a href="#__codelineno-0-2517">2517</a></span>
<span class="normal"><a href="#__codelineno-0-2518">2518</a></span>
<span class="normal"><a href="#__codelineno-0-2519">2519</a></span>
<span class="normal"><a href="#__codelineno-0-2520">2520</a></span>
<span class="normal"><a href="#__codelineno-0-2521">2521</a></span>
<span class="normal"><a href="#__codelineno-0-2522">2522</a></span>
<span class="normal"><a href="#__codelineno-0-2523">2523</a></span>
<span class="normal"><a href="#__codelineno-0-2524">2524</a></span>
<span class="normal"><a href="#__codelineno-0-2525">2525</a></span>
<span class="normal"><a href="#__codelineno-0-2526">2526</a></span>
<span class="normal"><a href="#__codelineno-0-2527">2527</a></span>
<span class="normal"><a href="#__codelineno-0-2528">2528</a></span>
<span class="normal"><a href="#__codelineno-0-2529">2529</a></span>
<span class="normal"><a href="#__codelineno-0-2530">2530</a></span>
<span class="normal"><a href="#__codelineno-0-2531">2531</a></span>
<span class="normal"><a href="#__codelineno-0-2532">2532</a></span>
<span class="normal"><a href="#__codelineno-0-2533">2533</a></span>
<span class="normal"><a href="#__codelineno-0-2534">2534</a></span>
<span class="normal"><a href="#__codelineno-0-2535">2535</a></span>
<span class="normal"><a href="#__codelineno-0-2536">2536</a></span>
<span class="normal"><a href="#__codelineno-0-2537">2537</a></span>
<span class="normal"><a href="#__codelineno-0-2538">2538</a></span>
<span class="normal"><a href="#__codelineno-0-2539">2539</a></span>
<span class="normal"><a href="#__codelineno-0-2540">2540</a></span>
<span class="normal"><a href="#__codelineno-0-2541">2541</a></span>
<span class="normal"><a href="#__codelineno-0-2542">2542</a></span>
<span class="normal"><a href="#__codelineno-0-2543">2543</a></span>
<span class="normal"><a href="#__codelineno-0-2544">2544</a></span>
<span class="normal"><a href="#__codelineno-0-2545">2545</a></span>
<span class="normal"><a href="#__codelineno-0-2546">2546</a></span>
<span class="normal"><a href="#__codelineno-0-2547">2547</a></span>
<span class="normal"><a href="#__codelineno-0-2548">2548</a></span>
<span class="normal"><a href="#__codelineno-0-2549">2549</a></span>
<span class="normal"><a href="#__codelineno-0-2550">2550</a></span>
<span class="normal"><a href="#__codelineno-0-2551">2551</a></span>
<span class="normal"><a href="#__codelineno-0-2552">2552</a></span>
<span class="normal"><a href="#__codelineno-0-2553">2553</a></span>
<span class="normal"><a href="#__codelineno-0-2554">2554</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-2462" name="__codelineno-0-2462"></a><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
<a id="__codelineno-0-2463" name="__codelineno-0-2463"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-2464" name="__codelineno-0-2464"></a>    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">MutableMapping</span><span class="p">,</span>
<a id="__codelineno-0-2465" name="__codelineno-0-2465"></a>    <span class="n">pred_type</span><span class="p">:</span> <span class="n">PredType</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">PredType</span><span class="o">.</span><span class="n">GP</span><span class="p">,</span>
<a id="__codelineno-0-2466" name="__codelineno-0-2466"></a>    <span class="n">joint</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-0-2467" name="__codelineno-0-2467"></a>    <span class="n">link_approx</span><span class="p">:</span> <span class="n">LinkApprox</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">LinkApprox</span><span class="o">.</span><span class="n">PROBIT</span><span class="p">,</span>
<a id="__codelineno-0-2468" name="__codelineno-0-2468"></a>    <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-0-2469" name="__codelineno-0-2469"></a>    <span class="n">diagonal_output</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-0-2470" name="__codelineno-0-2470"></a>    <span class="n">generator</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-2471" name="__codelineno-0-2471"></a>    <span class="n">fitting</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-0-2472" name="__codelineno-0-2472"></a>    <span class="o">**</span><span class="n">model_kwargs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
<a id="__codelineno-0-2473" name="__codelineno-0-2473"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<a id="__codelineno-0-2474" name="__codelineno-0-2474"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the posterior predictive on input data `x`.</span>
<a id="__codelineno-0-2475" name="__codelineno-0-2475"></a>
<a id="__codelineno-0-2476" name="__codelineno-0-2476"></a><span class="sd">    Parameters</span>
<a id="__codelineno-0-2477" name="__codelineno-0-2477"></a><span class="sd">    ----------</span>
<a id="__codelineno-0-2478" name="__codelineno-0-2478"></a><span class="sd">    x : torch.Tensor or MutableMapping</span>
<a id="__codelineno-0-2479" name="__codelineno-0-2479"></a><span class="sd">        `(batch_size, input_shape)` if tensor. If MutableMapping, must contain</span>
<a id="__codelineno-0-2480" name="__codelineno-0-2480"></a><span class="sd">        the said tensor.</span>
<a id="__codelineno-0-2481" name="__codelineno-0-2481"></a>
<a id="__codelineno-0-2482" name="__codelineno-0-2482"></a><span class="sd">    pred_type : {&#39;gp&#39;}, default=&#39;gp&#39;</span>
<a id="__codelineno-0-2483" name="__codelineno-0-2483"></a><span class="sd">        type of posterior predictive, linearized GLM predictive (GP).</span>
<a id="__codelineno-0-2484" name="__codelineno-0-2484"></a><span class="sd">        The GP predictive is consistent with</span>
<a id="__codelineno-0-2485" name="__codelineno-0-2485"></a><span class="sd">        the curvature approximations used here.</span>
<a id="__codelineno-0-2486" name="__codelineno-0-2486"></a>
<a id="__codelineno-0-2487" name="__codelineno-0-2487"></a><span class="sd">    link_approx : {&#39;mc&#39;, &#39;probit&#39;, &#39;bridge&#39;, &#39;bridge_norm&#39;}</span>
<a id="__codelineno-0-2488" name="__codelineno-0-2488"></a><span class="sd">        how to approximate the classification link function for the `&#39;glm&#39;`.</span>
<a id="__codelineno-0-2489" name="__codelineno-0-2489"></a>
<a id="__codelineno-0-2490" name="__codelineno-0-2490"></a><span class="sd">    joint : bool</span>
<a id="__codelineno-0-2491" name="__codelineno-0-2491"></a><span class="sd">        Whether to output a joint predictive distribution in regression with</span>
<a id="__codelineno-0-2492" name="__codelineno-0-2492"></a><span class="sd">        `pred_type=&#39;glm&#39;`. If set to `True`, the predictive distribution</span>
<a id="__codelineno-0-2493" name="__codelineno-0-2493"></a><span class="sd">        has the same form as GP posterior, i.e. N([f(x1), ...,f(xm)], Cov[f(x1), ..., f(xm)]).</span>
<a id="__codelineno-0-2494" name="__codelineno-0-2494"></a><span class="sd">        If `False`, then only outputs the marginal predictive distribution.</span>
<a id="__codelineno-0-2495" name="__codelineno-0-2495"></a><span class="sd">        Only available for regression and GLM predictive.</span>
<a id="__codelineno-0-2496" name="__codelineno-0-2496"></a>
<a id="__codelineno-0-2497" name="__codelineno-0-2497"></a><span class="sd">    n_samples : int</span>
<a id="__codelineno-0-2498" name="__codelineno-0-2498"></a><span class="sd">        number of samples for `link_approx=&#39;mc&#39;`.</span>
<a id="__codelineno-0-2499" name="__codelineno-0-2499"></a>
<a id="__codelineno-0-2500" name="__codelineno-0-2500"></a><span class="sd">    diagonal_output : bool</span>
<a id="__codelineno-0-2501" name="__codelineno-0-2501"></a><span class="sd">        whether to use a diagonalized posterior predictive on the outputs.</span>
<a id="__codelineno-0-2502" name="__codelineno-0-2502"></a><span class="sd">        Only works for `link_approx=&#39;mc&#39;`.</span>
<a id="__codelineno-0-2503" name="__codelineno-0-2503"></a>
<a id="__codelineno-0-2504" name="__codelineno-0-2504"></a><span class="sd">    generator : torch.Generator, optional</span>
<a id="__codelineno-0-2505" name="__codelineno-0-2505"></a><span class="sd">        random number generator to control the samples (if sampling used).</span>
<a id="__codelineno-0-2506" name="__codelineno-0-2506"></a>
<a id="__codelineno-0-2507" name="__codelineno-0-2507"></a><span class="sd">    fitting : bool, default=False</span>
<a id="__codelineno-0-2508" name="__codelineno-0-2508"></a><span class="sd">        whether or not this predictive call is done during fitting. Only useful for</span>
<a id="__codelineno-0-2509" name="__codelineno-0-2509"></a><span class="sd">        reward modeling: the likelihood is set to `&quot;regression&quot;` when `False` and</span>
<a id="__codelineno-0-2510" name="__codelineno-0-2510"></a><span class="sd">        `&quot;classification&quot;` when `True`.</span>
<a id="__codelineno-0-2511" name="__codelineno-0-2511"></a>
<a id="__codelineno-0-2512" name="__codelineno-0-2512"></a><span class="sd">    Returns</span>
<a id="__codelineno-0-2513" name="__codelineno-0-2513"></a><span class="sd">    -------</span>
<a id="__codelineno-0-2514" name="__codelineno-0-2514"></a><span class="sd">    predictive: torch.Tensor or Tuple[torch.Tensor]</span>
<a id="__codelineno-0-2515" name="__codelineno-0-2515"></a><span class="sd">        For `likelihood=&#39;classification&#39;`, a torch.Tensor is returned with</span>
<a id="__codelineno-0-2516" name="__codelineno-0-2516"></a><span class="sd">        a distribution over classes (similar to a Softmax).</span>
<a id="__codelineno-0-2517" name="__codelineno-0-2517"></a><span class="sd">        For `likelihood=&#39;regression&#39;`, a tuple of torch.Tensor is returned</span>
<a id="__codelineno-0-2518" name="__codelineno-0-2518"></a><span class="sd">        with the mean and the predictive variance.</span>
<a id="__codelineno-0-2519" name="__codelineno-0-2519"></a><span class="sd">        For `likelihood=&#39;regression&#39;` and `joint=True`, a tuple of torch.Tensor</span>
<a id="__codelineno-0-2520" name="__codelineno-0-2520"></a><span class="sd">        is returned with the mean and the predictive covariance.</span>
<a id="__codelineno-0-2521" name="__codelineno-0-2521"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-2522" name="__codelineno-0-2522"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fitted</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
<a id="__codelineno-0-2523" name="__codelineno-0-2523"></a>        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
<a id="__codelineno-0-2524" name="__codelineno-0-2524"></a>            <span class="s2">&quot;Functional Laplace has not been fitted to any &quot;</span>
<a id="__codelineno-0-2525" name="__codelineno-0-2525"></a>            <span class="o">+</span> <span class="s2">&quot;training dataset. Please call .fit method.&quot;</span>
<a id="__codelineno-0-2526" name="__codelineno-0-2526"></a>        <span class="p">)</span>
<a id="__codelineno-0-2527" name="__codelineno-0-2527"></a>
<a id="__codelineno-0-2528" name="__codelineno-0-2528"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_recompute_Sigma</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
<a id="__codelineno-0-2529" name="__codelineno-0-2529"></a>        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
<a id="__codelineno-0-2530" name="__codelineno-0-2530"></a>            <span class="s2">&quot;The prior precision has been changed since fit. &quot;</span>
<a id="__codelineno-0-2531" name="__codelineno-0-2531"></a>            <span class="o">+</span> <span class="s2">&quot;Re-compututing its value...&quot;</span>
<a id="__codelineno-0-2532" name="__codelineno-0-2532"></a>        <span class="p">)</span>
<a id="__codelineno-0-2533" name="__codelineno-0-2533"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_build_Sigma_inv</span><span class="p">()</span>
<a id="__codelineno-0-2534" name="__codelineno-0-2534"></a>
<a id="__codelineno-0-2535" name="__codelineno-0-2535"></a>    <span class="k">if</span> <span class="n">pred_type</span> <span class="o">!=</span> <span class="n">PredType</span><span class="o">.</span><span class="n">GP</span><span class="p">:</span>
<a id="__codelineno-0-2536" name="__codelineno-0-2536"></a>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Only gp supported as prediction types.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-2537" name="__codelineno-0-2537"></a>
<a id="__codelineno-0-2538" name="__codelineno-0-2538"></a>    <span class="k">if</span> <span class="n">link_approx</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">la</span> <span class="k">for</span> <span class="n">la</span> <span class="ow">in</span> <span class="n">LinkApprox</span><span class="p">]:</span>
<a id="__codelineno-0-2539" name="__codelineno-0-2539"></a>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported link approximation </span><span class="si">{</span><span class="n">link_approx</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-2540" name="__codelineno-0-2540"></a>
<a id="__codelineno-0-2541" name="__codelineno-0-2541"></a>    <span class="k">if</span> <span class="n">generator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-2542" name="__codelineno-0-2542"></a>        <span class="k">if</span> <span class="p">(</span>
<a id="__codelineno-0-2543" name="__codelineno-0-2543"></a>            <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">generator</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">)</span>
<a id="__codelineno-0-2544" name="__codelineno-0-2544"></a>            <span class="ow">or</span> <span class="n">generator</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span>
<a id="__codelineno-0-2545" name="__codelineno-0-2545"></a>        <span class="p">):</span>
<a id="__codelineno-0-2546" name="__codelineno-0-2546"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid random generator (check type and device).&quot;</span><span class="p">)</span>
<a id="__codelineno-0-2547" name="__codelineno-0-2547"></a>
<a id="__codelineno-0-2548" name="__codelineno-0-2548"></a>    <span class="n">likelihood</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span>
<a id="__codelineno-0-2549" name="__codelineno-0-2549"></a>    <span class="k">if</span> <span class="n">likelihood</span> <span class="o">==</span> <span class="n">Likelihood</span><span class="o">.</span><span class="n">REWARD_MODELING</span><span class="p">:</span>
<a id="__codelineno-0-2550" name="__codelineno-0-2550"></a>        <span class="n">likelihood</span> <span class="o">=</span> <span class="n">Likelihood</span><span class="o">.</span><span class="n">CLASSIFICATION</span> <span class="k">if</span> <span class="n">fitting</span> <span class="k">else</span> <span class="n">Likelihood</span><span class="o">.</span><span class="n">REGRESSION</span>
<a id="__codelineno-0-2551" name="__codelineno-0-2551"></a>
<a id="__codelineno-0-2552" name="__codelineno-0-2552"></a>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_glm_forward_call</span><span class="p">(</span>
<a id="__codelineno-0-2553" name="__codelineno-0-2553"></a>        <span class="n">x</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">,</span> <span class="n">joint</span><span class="p">,</span> <span class="n">link_approx</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">diagonal_output</span>
<a id="__codelineno-0-2554" name="__codelineno-0-2554"></a>    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="laplace.baselaplace.FunctionalLaplace.functional_samples" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">functional_samples</span>


<a href="#laplace.baselaplace.FunctionalLaplace.functional_samples" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nf">functional_samples</span><span class="p">(</span><span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace.functional_samples(x)">x</a></span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" title="collections.abc.MutableMapping" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.MutableMapping">MutableMapping</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span><span class="p">,</span> <span class="n"><span title="torch.Tensor">Tensor</span></span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Any" href="https://docs.python.org/3/library/typing.html#typing.Any">Any</a></span><span class="p">],</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace.functional_samples(pred_type)">pred_type</a></span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="laplace.utils.enums.PredType" href="../enums/#laplace.utils.enums.PredType">PredType</a></span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span> <span class="o">=</span> <span class="n"><a class="autorefs autorefs-internal" title="laplace.utils.enums.PredType.GLM" href="../enums/#laplace.utils.enums.PredType.GLM">GLM</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace.functional_samples(n_samples)">n_samples</a></span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace.functional_samples(diagonal_output)">diagonal_output</a></span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace.functional_samples(generator)">generator</a></span><span class="p">:</span> <span class="n"><span title="torch.Generator">Generator</span></span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="torch.Tensor">Tensor</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Sample from the functional posterior on input data <code>x</code>.
Can be used, for example, for Thompson sampling.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace.functional_samples(x)" class="doc doc-heading doc-heading-parameter">              <b><code>x</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace.functional_samples(x)" class="headerlink" title="Permanent link">#</a></h4>              (<code><span title="torch.Tensor">Tensor</span> or <a class="autorefs autorefs-external" title="collections.abc.MutableMapping" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.MutableMapping">MutableMapping</a></code>)
          –
          <div class="doc-md-description">
            <p>input data <code>(batch_size, input_shape)</code></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace.functional_samples(pred_type)" class="doc doc-heading doc-heading-parameter">              <b><code>pred_type</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace.functional_samples(pred_type)" class="headerlink" title="Permanent link">#</a></h4>              (<code>&#39;glm&#39;</code>, default:
                  <code>&#39;glm&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>type of posterior predictive, linearized GLM predictive.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace.functional_samples(n_samples)" class="doc doc-heading doc-heading-parameter">              <b><code>n_samples</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace.functional_samples(n_samples)" class="headerlink" title="Permanent link">#</a></h4>              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>, default:
                  <code>100</code>
)
          –
          <div class="doc-md-description">
            <p>number of samples</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace.functional_samples(diagonal_output)" class="doc doc-heading doc-heading-parameter">              <b><code>diagonal_output</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace.functional_samples(diagonal_output)" class="headerlink" title="Permanent link">#</a></h4>              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>whether to use a diagonalized glm posterior predictive on the outputs.
Only applies when <code>pred_type='glm'</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace.functional_samples(generator)" class="doc doc-heading doc-heading-parameter">              <b><code>generator</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace.functional_samples(generator)" class="headerlink" title="Permanent link">#</a></h4>              (<code><span title="torch.Generator">Generator</span></code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>random number generator to control the samples (if sampling used)</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>samples</code></b> (              <code><span title="torch.Tensor">Tensor</span></code>
)          –
          <div class="doc-md-description">
            <p>samples <code>(n_samples, batch_size, output_shape)</code></p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>laplace/baselaplace.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-2556">2556</a></span>
<span class="normal"><a href="#__codelineno-0-2557">2557</a></span>
<span class="normal"><a href="#__codelineno-0-2558">2558</a></span>
<span class="normal"><a href="#__codelineno-0-2559">2559</a></span>
<span class="normal"><a href="#__codelineno-0-2560">2560</a></span>
<span class="normal"><a href="#__codelineno-0-2561">2561</a></span>
<span class="normal"><a href="#__codelineno-0-2562">2562</a></span>
<span class="normal"><a href="#__codelineno-0-2563">2563</a></span>
<span class="normal"><a href="#__codelineno-0-2564">2564</a></span>
<span class="normal"><a href="#__codelineno-0-2565">2565</a></span>
<span class="normal"><a href="#__codelineno-0-2566">2566</a></span>
<span class="normal"><a href="#__codelineno-0-2567">2567</a></span>
<span class="normal"><a href="#__codelineno-0-2568">2568</a></span>
<span class="normal"><a href="#__codelineno-0-2569">2569</a></span>
<span class="normal"><a href="#__codelineno-0-2570">2570</a></span>
<span class="normal"><a href="#__codelineno-0-2571">2571</a></span>
<span class="normal"><a href="#__codelineno-0-2572">2572</a></span>
<span class="normal"><a href="#__codelineno-0-2573">2573</a></span>
<span class="normal"><a href="#__codelineno-0-2574">2574</a></span>
<span class="normal"><a href="#__codelineno-0-2575">2575</a></span>
<span class="normal"><a href="#__codelineno-0-2576">2576</a></span>
<span class="normal"><a href="#__codelineno-0-2577">2577</a></span>
<span class="normal"><a href="#__codelineno-0-2578">2578</a></span>
<span class="normal"><a href="#__codelineno-0-2579">2579</a></span>
<span class="normal"><a href="#__codelineno-0-2580">2580</a></span>
<span class="normal"><a href="#__codelineno-0-2581">2581</a></span>
<span class="normal"><a href="#__codelineno-0-2582">2582</a></span>
<span class="normal"><a href="#__codelineno-0-2583">2583</a></span>
<span class="normal"><a href="#__codelineno-0-2584">2584</a></span>
<span class="normal"><a href="#__codelineno-0-2585">2585</a></span>
<span class="normal"><a href="#__codelineno-0-2586">2586</a></span>
<span class="normal"><a href="#__codelineno-0-2587">2587</a></span>
<span class="normal"><a href="#__codelineno-0-2588">2588</a></span>
<span class="normal"><a href="#__codelineno-0-2589">2589</a></span>
<span class="normal"><a href="#__codelineno-0-2590">2590</a></span>
<span class="normal"><a href="#__codelineno-0-2591">2591</a></span>
<span class="normal"><a href="#__codelineno-0-2592">2592</a></span>
<span class="normal"><a href="#__codelineno-0-2593">2593</a></span>
<span class="normal"><a href="#__codelineno-0-2594">2594</a></span>
<span class="normal"><a href="#__codelineno-0-2595">2595</a></span>
<span class="normal"><a href="#__codelineno-0-2596">2596</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-2556" name="__codelineno-0-2556"></a><span class="k">def</span> <span class="nf">functional_samples</span><span class="p">(</span>
<a id="__codelineno-0-2557" name="__codelineno-0-2557"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-2558" name="__codelineno-0-2558"></a>    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">MutableMapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">Any</span><span class="p">],</span>
<a id="__codelineno-0-2559" name="__codelineno-0-2559"></a>    <span class="n">pred_type</span><span class="p">:</span> <span class="n">PredType</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">PredType</span><span class="o">.</span><span class="n">GLM</span><span class="p">,</span>
<a id="__codelineno-0-2560" name="__codelineno-0-2560"></a>    <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-0-2561" name="__codelineno-0-2561"></a>    <span class="n">diagonal_output</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-0-2562" name="__codelineno-0-2562"></a>    <span class="n">generator</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-2563" name="__codelineno-0-2563"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-2564" name="__codelineno-0-2564"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Sample from the functional posterior on input data `x`.</span>
<a id="__codelineno-0-2565" name="__codelineno-0-2565"></a><span class="sd">    Can be used, for example, for Thompson sampling.</span>
<a id="__codelineno-0-2566" name="__codelineno-0-2566"></a>
<a id="__codelineno-0-2567" name="__codelineno-0-2567"></a><span class="sd">    Parameters</span>
<a id="__codelineno-0-2568" name="__codelineno-0-2568"></a><span class="sd">    ----------</span>
<a id="__codelineno-0-2569" name="__codelineno-0-2569"></a><span class="sd">    x : torch.Tensor or MutableMapping</span>
<a id="__codelineno-0-2570" name="__codelineno-0-2570"></a><span class="sd">        input data `(batch_size, input_shape)`</span>
<a id="__codelineno-0-2571" name="__codelineno-0-2571"></a>
<a id="__codelineno-0-2572" name="__codelineno-0-2572"></a><span class="sd">    pred_type : {&#39;glm&#39;}, default=&#39;glm&#39;</span>
<a id="__codelineno-0-2573" name="__codelineno-0-2573"></a><span class="sd">        type of posterior predictive, linearized GLM predictive.</span>
<a id="__codelineno-0-2574" name="__codelineno-0-2574"></a>
<a id="__codelineno-0-2575" name="__codelineno-0-2575"></a><span class="sd">    n_samples : int</span>
<a id="__codelineno-0-2576" name="__codelineno-0-2576"></a><span class="sd">        number of samples</span>
<a id="__codelineno-0-2577" name="__codelineno-0-2577"></a>
<a id="__codelineno-0-2578" name="__codelineno-0-2578"></a><span class="sd">    diagonal_output : bool</span>
<a id="__codelineno-0-2579" name="__codelineno-0-2579"></a><span class="sd">        whether to use a diagonalized glm posterior predictive on the outputs.</span>
<a id="__codelineno-0-2580" name="__codelineno-0-2580"></a><span class="sd">        Only applies when `pred_type=&#39;glm&#39;`.</span>
<a id="__codelineno-0-2581" name="__codelineno-0-2581"></a>
<a id="__codelineno-0-2582" name="__codelineno-0-2582"></a><span class="sd">    generator : torch.Generator, optional</span>
<a id="__codelineno-0-2583" name="__codelineno-0-2583"></a><span class="sd">        random number generator to control the samples (if sampling used)</span>
<a id="__codelineno-0-2584" name="__codelineno-0-2584"></a>
<a id="__codelineno-0-2585" name="__codelineno-0-2585"></a><span class="sd">    Returns</span>
<a id="__codelineno-0-2586" name="__codelineno-0-2586"></a><span class="sd">    -------</span>
<a id="__codelineno-0-2587" name="__codelineno-0-2587"></a><span class="sd">    samples : torch.Tensor</span>
<a id="__codelineno-0-2588" name="__codelineno-0-2588"></a><span class="sd">        samples `(n_samples, batch_size, output_shape)`</span>
<a id="__codelineno-0-2589" name="__codelineno-0-2589"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-2590" name="__codelineno-0-2590"></a>    <span class="k">if</span> <span class="n">pred_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">PredType</span><span class="o">.</span><span class="n">__members__</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
<a id="__codelineno-0-2591" name="__codelineno-0-2591"></a>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Only glm  supported as prediction type.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-2592" name="__codelineno-0-2592"></a>
<a id="__codelineno-0-2593" name="__codelineno-0-2593"></a>    <span class="n">f_mu</span><span class="p">,</span> <span class="n">f_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_glm_predictive_distribution</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-0-2594" name="__codelineno-0-2594"></a>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_glm_functional_samples</span><span class="p">(</span>
<a id="__codelineno-0-2595" name="__codelineno-0-2595"></a>        <span class="n">f_mu</span><span class="p">,</span> <span class="n">f_var</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">diagonal_output</span><span class="p">,</span> <span class="n">generator</span>
<a id="__codelineno-0-2596" name="__codelineno-0-2596"></a>    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="laplace.baselaplace.FunctionalLaplace.predictive_samples" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">predictive_samples</span>


<a href="#laplace.baselaplace.FunctionalLaplace.predictive_samples" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nf">predictive_samples</span><span class="p">(</span><span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace.predictive_samples(x)">x</a></span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" title="collections.abc.MutableMapping" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.MutableMapping">MutableMapping</a></span><span class="p">[</span><span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span><span class="p">,</span> <span class="n"><span title="torch.Tensor">Tensor</span></span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Any" href="https://docs.python.org/3/library/typing.html#typing.Any">Any</a></span><span class="p">],</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace.predictive_samples(pred_type)">pred_type</a></span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="laplace.utils.enums.PredType" href="../enums/#laplace.utils.enums.PredType">PredType</a></span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span> <span class="o">=</span> <span class="n"><a class="autorefs autorefs-internal" title="laplace.utils.enums.PredType.GLM" href="../enums/#laplace.utils.enums.PredType.GLM">GLM</a></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace.predictive_samples(n_samples)">n_samples</a></span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace.predictive_samples(diagonal_output)">diagonal_output</a></span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace.predictive_samples(generator)">generator</a></span><span class="p">:</span> <span class="n"><span title="torch.Generator">Generator</span></span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="torch.Tensor">Tensor</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Sample from the posterior predictive on input data <code>x</code>.
I.e., the corresponding inverse-link function is applied on top of the
functional sample. Can be used, for example, for Thompson sampling.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace.predictive_samples(x)" class="doc doc-heading doc-heading-parameter">              <b><code>x</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace.predictive_samples(x)" class="headerlink" title="Permanent link">#</a></h4>              (<code><span title="torch.Tensor">Tensor</span> or <a class="autorefs autorefs-external" title="collections.abc.MutableMapping" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.MutableMapping">MutableMapping</a></code>)
          –
          <div class="doc-md-description">
            <p>input data <code>(batch_size, input_shape)</code></p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace.predictive_samples(pred_type)" class="doc doc-heading doc-heading-parameter">              <b><code>pred_type</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace.predictive_samples(pred_type)" class="headerlink" title="Permanent link">#</a></h4>              (<code>&#39;glm&#39;</code>, default:
                  <code>&#39;glm&#39;</code>
)
          –
          <div class="doc-md-description">
            <p>type of posterior predictive, linearized GLM predictive.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace.predictive_samples(n_samples)" class="doc doc-heading doc-heading-parameter">              <b><code>n_samples</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace.predictive_samples(n_samples)" class="headerlink" title="Permanent link">#</a></h4>              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></code>, default:
                  <code>100</code>
)
          –
          <div class="doc-md-description">
            <p>number of samples</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace.predictive_samples(diagonal_output)" class="doc doc-heading doc-heading-parameter">              <b><code>diagonal_output</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace.predictive_samples(diagonal_output)" class="headerlink" title="Permanent link">#</a></h4>              (<code><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <p>whether to use a diagonalized glm posterior predictive on the outputs.
Only applies when <code>pred_type='glm'</code>.</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace.predictive_samples(generator)" class="doc doc-heading doc-heading-parameter">              <b><code>generator</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace.predictive_samples(generator)" class="headerlink" title="Permanent link">#</a></h4>              (<code><span title="torch.Generator">Generator</span></code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>random number generator to control the samples (if sampling used)</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>samples</code></b> (              <code><span title="torch.Tensor">Tensor</span></code>
)          –
          <div class="doc-md-description">
            <p>samples <code>(n_samples, batch_size, output_shape)</code></p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>laplace/baselaplace.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-2598">2598</a></span>
<span class="normal"><a href="#__codelineno-0-2599">2599</a></span>
<span class="normal"><a href="#__codelineno-0-2600">2600</a></span>
<span class="normal"><a href="#__codelineno-0-2601">2601</a></span>
<span class="normal"><a href="#__codelineno-0-2602">2602</a></span>
<span class="normal"><a href="#__codelineno-0-2603">2603</a></span>
<span class="normal"><a href="#__codelineno-0-2604">2604</a></span>
<span class="normal"><a href="#__codelineno-0-2605">2605</a></span>
<span class="normal"><a href="#__codelineno-0-2606">2606</a></span>
<span class="normal"><a href="#__codelineno-0-2607">2607</a></span>
<span class="normal"><a href="#__codelineno-0-2608">2608</a></span>
<span class="normal"><a href="#__codelineno-0-2609">2609</a></span>
<span class="normal"><a href="#__codelineno-0-2610">2610</a></span>
<span class="normal"><a href="#__codelineno-0-2611">2611</a></span>
<span class="normal"><a href="#__codelineno-0-2612">2612</a></span>
<span class="normal"><a href="#__codelineno-0-2613">2613</a></span>
<span class="normal"><a href="#__codelineno-0-2614">2614</a></span>
<span class="normal"><a href="#__codelineno-0-2615">2615</a></span>
<span class="normal"><a href="#__codelineno-0-2616">2616</a></span>
<span class="normal"><a href="#__codelineno-0-2617">2617</a></span>
<span class="normal"><a href="#__codelineno-0-2618">2618</a></span>
<span class="normal"><a href="#__codelineno-0-2619">2619</a></span>
<span class="normal"><a href="#__codelineno-0-2620">2620</a></span>
<span class="normal"><a href="#__codelineno-0-2621">2621</a></span>
<span class="normal"><a href="#__codelineno-0-2622">2622</a></span>
<span class="normal"><a href="#__codelineno-0-2623">2623</a></span>
<span class="normal"><a href="#__codelineno-0-2624">2624</a></span>
<span class="normal"><a href="#__codelineno-0-2625">2625</a></span>
<span class="normal"><a href="#__codelineno-0-2626">2626</a></span>
<span class="normal"><a href="#__codelineno-0-2627">2627</a></span>
<span class="normal"><a href="#__codelineno-0-2628">2628</a></span>
<span class="normal"><a href="#__codelineno-0-2629">2629</a></span>
<span class="normal"><a href="#__codelineno-0-2630">2630</a></span>
<span class="normal"><a href="#__codelineno-0-2631">2631</a></span>
<span class="normal"><a href="#__codelineno-0-2632">2632</a></span>
<span class="normal"><a href="#__codelineno-0-2633">2633</a></span>
<span class="normal"><a href="#__codelineno-0-2634">2634</a></span>
<span class="normal"><a href="#__codelineno-0-2635">2635</a></span>
<span class="normal"><a href="#__codelineno-0-2636">2636</a></span>
<span class="normal"><a href="#__codelineno-0-2637">2637</a></span>
<span class="normal"><a href="#__codelineno-0-2638">2638</a></span>
<span class="normal"><a href="#__codelineno-0-2639">2639</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-2598" name="__codelineno-0-2598"></a><span class="k">def</span> <span class="nf">predictive_samples</span><span class="p">(</span>
<a id="__codelineno-0-2599" name="__codelineno-0-2599"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-2600" name="__codelineno-0-2600"></a>    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">MutableMapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">Any</span><span class="p">],</span>
<a id="__codelineno-0-2601" name="__codelineno-0-2601"></a>    <span class="n">pred_type</span><span class="p">:</span> <span class="n">PredType</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">PredType</span><span class="o">.</span><span class="n">GLM</span><span class="p">,</span>
<a id="__codelineno-0-2602" name="__codelineno-0-2602"></a>    <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-0-2603" name="__codelineno-0-2603"></a>    <span class="n">diagonal_output</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-0-2604" name="__codelineno-0-2604"></a>    <span class="n">generator</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-2605" name="__codelineno-0-2605"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-2606" name="__codelineno-0-2606"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Sample from the posterior predictive on input data `x`.</span>
<a id="__codelineno-0-2607" name="__codelineno-0-2607"></a><span class="sd">    I.e., the corresponding inverse-link function is applied on top of the</span>
<a id="__codelineno-0-2608" name="__codelineno-0-2608"></a><span class="sd">    functional sample. Can be used, for example, for Thompson sampling.</span>
<a id="__codelineno-0-2609" name="__codelineno-0-2609"></a>
<a id="__codelineno-0-2610" name="__codelineno-0-2610"></a><span class="sd">    Parameters</span>
<a id="__codelineno-0-2611" name="__codelineno-0-2611"></a><span class="sd">    ----------</span>
<a id="__codelineno-0-2612" name="__codelineno-0-2612"></a><span class="sd">    x : torch.Tensor or MutableMapping</span>
<a id="__codelineno-0-2613" name="__codelineno-0-2613"></a><span class="sd">        input data `(batch_size, input_shape)`</span>
<a id="__codelineno-0-2614" name="__codelineno-0-2614"></a>
<a id="__codelineno-0-2615" name="__codelineno-0-2615"></a><span class="sd">    pred_type : {&#39;glm&#39;}, default=&#39;glm&#39;</span>
<a id="__codelineno-0-2616" name="__codelineno-0-2616"></a><span class="sd">        type of posterior predictive, linearized GLM predictive.</span>
<a id="__codelineno-0-2617" name="__codelineno-0-2617"></a>
<a id="__codelineno-0-2618" name="__codelineno-0-2618"></a><span class="sd">    n_samples : int</span>
<a id="__codelineno-0-2619" name="__codelineno-0-2619"></a><span class="sd">        number of samples</span>
<a id="__codelineno-0-2620" name="__codelineno-0-2620"></a>
<a id="__codelineno-0-2621" name="__codelineno-0-2621"></a><span class="sd">    diagonal_output : bool</span>
<a id="__codelineno-0-2622" name="__codelineno-0-2622"></a><span class="sd">        whether to use a diagonalized glm posterior predictive on the outputs.</span>
<a id="__codelineno-0-2623" name="__codelineno-0-2623"></a><span class="sd">        Only applies when `pred_type=&#39;glm&#39;`.</span>
<a id="__codelineno-0-2624" name="__codelineno-0-2624"></a>
<a id="__codelineno-0-2625" name="__codelineno-0-2625"></a><span class="sd">    generator : torch.Generator, optional</span>
<a id="__codelineno-0-2626" name="__codelineno-0-2626"></a><span class="sd">        random number generator to control the samples (if sampling used)</span>
<a id="__codelineno-0-2627" name="__codelineno-0-2627"></a>
<a id="__codelineno-0-2628" name="__codelineno-0-2628"></a><span class="sd">    Returns</span>
<a id="__codelineno-0-2629" name="__codelineno-0-2629"></a><span class="sd">    -------</span>
<a id="__codelineno-0-2630" name="__codelineno-0-2630"></a><span class="sd">    samples : torch.Tensor</span>
<a id="__codelineno-0-2631" name="__codelineno-0-2631"></a><span class="sd">        samples `(n_samples, batch_size, output_shape)`</span>
<a id="__codelineno-0-2632" name="__codelineno-0-2632"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-2633" name="__codelineno-0-2633"></a>    <span class="k">if</span> <span class="n">pred_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">PredType</span><span class="o">.</span><span class="n">__members__</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
<a id="__codelineno-0-2634" name="__codelineno-0-2634"></a>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Only glm  supported as prediction type.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-2635" name="__codelineno-0-2635"></a>
<a id="__codelineno-0-2636" name="__codelineno-0-2636"></a>    <span class="n">f_mu</span><span class="p">,</span> <span class="n">f_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_glm_predictive_distribution</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-0-2637" name="__codelineno-0-2637"></a>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_glm_predictive_samples</span><span class="p">(</span>
<a id="__codelineno-0-2638" name="__codelineno-0-2638"></a>        <span class="n">f_mu</span><span class="p">,</span> <span class="n">f_var</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">diagonal_output</span><span class="p">,</span> <span class="n">generator</span>
<a id="__codelineno-0-2639" name="__codelineno-0-2639"></a>    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="laplace.baselaplace.FunctionalLaplace.functional_variance" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">functional_variance</span>


<a href="#laplace.baselaplace.FunctionalLaplace.functional_variance" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nf">functional_variance</span><span class="p">(</span><span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace.functional_variance(Js_star)">Js_star</a></span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="torch.Tensor">Tensor</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>GP posterior variance:</p>
<div class="arithmatex">\[
    k_{**} - K_{*M} (K_{MM}+ L_{MM}^{-1})^{-1} K_{M*}
\]</div>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace.functional_variance(Js_star)" class="doc doc-heading doc-heading-parameter">              <b><code>Js_star</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace.functional_variance(Js_star)" class="headerlink" title="Permanent link">#</a></h4>              (<code>torch.Tensor of shape (N*, C, P)</code>)
          –
          <div class="doc-md-description">
            <div class="highlight"><pre><span></span><code>  Jacobians of test data points
</code></pre></div>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>f_var</code></b> (              <code>torch.Tensor of shape (N*,C, C)</code>
)          –
          <div class="doc-md-description">
            <p>Contains the posterior variances of N* testing points.</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>laplace/baselaplace.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-2645">2645</a></span>
<span class="normal"><a href="#__codelineno-0-2646">2646</a></span>
<span class="normal"><a href="#__codelineno-0-2647">2647</a></span>
<span class="normal"><a href="#__codelineno-0-2648">2648</a></span>
<span class="normal"><a href="#__codelineno-0-2649">2649</a></span>
<span class="normal"><a href="#__codelineno-0-2650">2650</a></span>
<span class="normal"><a href="#__codelineno-0-2651">2651</a></span>
<span class="normal"><a href="#__codelineno-0-2652">2652</a></span>
<span class="normal"><a href="#__codelineno-0-2653">2653</a></span>
<span class="normal"><a href="#__codelineno-0-2654">2654</a></span>
<span class="normal"><a href="#__codelineno-0-2655">2655</a></span>
<span class="normal"><a href="#__codelineno-0-2656">2656</a></span>
<span class="normal"><a href="#__codelineno-0-2657">2657</a></span>
<span class="normal"><a href="#__codelineno-0-2658">2658</a></span>
<span class="normal"><a href="#__codelineno-0-2659">2659</a></span>
<span class="normal"><a href="#__codelineno-0-2660">2660</a></span>
<span class="normal"><a href="#__codelineno-0-2661">2661</a></span>
<span class="normal"><a href="#__codelineno-0-2662">2662</a></span>
<span class="normal"><a href="#__codelineno-0-2663">2663</a></span>
<span class="normal"><a href="#__codelineno-0-2664">2664</a></span>
<span class="normal"><a href="#__codelineno-0-2665">2665</a></span>
<span class="normal"><a href="#__codelineno-0-2666">2666</a></span>
<span class="normal"><a href="#__codelineno-0-2667">2667</a></span>
<span class="normal"><a href="#__codelineno-0-2668">2668</a></span>
<span class="normal"><a href="#__codelineno-0-2669">2669</a></span>
<span class="normal"><a href="#__codelineno-0-2670">2670</a></span>
<span class="normal"><a href="#__codelineno-0-2671">2671</a></span>
<span class="normal"><a href="#__codelineno-0-2672">2672</a></span>
<span class="normal"><a href="#__codelineno-0-2673">2673</a></span>
<span class="normal"><a href="#__codelineno-0-2674">2674</a></span>
<span class="normal"><a href="#__codelineno-0-2675">2675</a></span>
<span class="normal"><a href="#__codelineno-0-2676">2676</a></span>
<span class="normal"><a href="#__codelineno-0-2677">2677</a></span>
<span class="normal"><a href="#__codelineno-0-2678">2678</a></span>
<span class="normal"><a href="#__codelineno-0-2679">2679</a></span>
<span class="normal"><a href="#__codelineno-0-2680">2680</a></span>
<span class="normal"><a href="#__codelineno-0-2681">2681</a></span>
<span class="normal"><a href="#__codelineno-0-2682">2682</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-2645" name="__codelineno-0-2645"></a><span class="k">def</span> <span class="nf">functional_variance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Js_star</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-2646" name="__codelineno-0-2646"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;GP posterior variance:</span>
<a id="__codelineno-0-2647" name="__codelineno-0-2647"></a>
<a id="__codelineno-0-2648" name="__codelineno-0-2648"></a><span class="sd">    $$</span>
<a id="__codelineno-0-2649" name="__codelineno-0-2649"></a><span class="sd">        k_{**} - K_{*M} (K_{MM}+ L_{MM}^{-1})^{-1} K_{M*}</span>
<a id="__codelineno-0-2650" name="__codelineno-0-2650"></a><span class="sd">    $$</span>
<a id="__codelineno-0-2651" name="__codelineno-0-2651"></a>
<a id="__codelineno-0-2652" name="__codelineno-0-2652"></a><span class="sd">    Parameters</span>
<a id="__codelineno-0-2653" name="__codelineno-0-2653"></a><span class="sd">    ----------</span>
<a id="__codelineno-0-2654" name="__codelineno-0-2654"></a><span class="sd">    Js_star : torch.Tensor of shape (N*, C, P)</span>
<a id="__codelineno-0-2655" name="__codelineno-0-2655"></a><span class="sd">              Jacobians of test data points</span>
<a id="__codelineno-0-2656" name="__codelineno-0-2656"></a>
<a id="__codelineno-0-2657" name="__codelineno-0-2657"></a><span class="sd">    Returns</span>
<a id="__codelineno-0-2658" name="__codelineno-0-2658"></a><span class="sd">    -------</span>
<a id="__codelineno-0-2659" name="__codelineno-0-2659"></a><span class="sd">    f_var : torch.Tensor of shape (N*,C, C)</span>
<a id="__codelineno-0-2660" name="__codelineno-0-2660"></a><span class="sd">            Contains the posterior variances of N* testing points.</span>
<a id="__codelineno-0-2661" name="__codelineno-0-2661"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-2662" name="__codelineno-0-2662"></a>    <span class="c1"># Compute K_{**}</span>
<a id="__codelineno-0-2663" name="__codelineno-0-2663"></a>    <span class="n">K_star</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp_kernel_prior_variance</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_star</span><span class="p">(</span><span class="n">Js_star</span><span class="p">)</span>
<a id="__codelineno-0-2664" name="__codelineno-0-2664"></a>
<a id="__codelineno-0-2665" name="__codelineno-0-2665"></a>    <span class="c1"># Compute K_{*M}</span>
<a id="__codelineno-0-2666" name="__codelineno-0-2666"></a>    <span class="n">K_M_star</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-0-2667" name="__codelineno-0-2667"></a>    <span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="p">:</span>
<a id="__codelineno-0-2668" name="__codelineno-0-2668"></a>        <span class="n">K_M_star_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp_kernel_prior_variance</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_batch_star</span><span class="p">(</span>
<a id="__codelineno-0-2669" name="__codelineno-0-2669"></a>            <span class="n">Js_star</span><span class="p">,</span> <span class="n">X_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
<a id="__codelineno-0-2670" name="__codelineno-0-2670"></a>        <span class="p">)</span>
<a id="__codelineno-0-2671" name="__codelineno-0-2671"></a>        <span class="n">K_M_star</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">K_M_star_batch</span><span class="p">)</span>
<a id="__codelineno-0-2672" name="__codelineno-0-2672"></a>        <span class="k">del</span> <span class="n">X_batch</span>
<a id="__codelineno-0-2673" name="__codelineno-0-2673"></a>
<a id="__codelineno-0-2674" name="__codelineno-0-2674"></a>    <span class="c1"># Build_K_star_M computes K_{*M} (K_{MM}+ L_{MM}^{-1})^{-1} K_{M*}</span>
<a id="__codelineno-0-2675" name="__codelineno-0-2675"></a>    <span class="n">f_var</span> <span class="o">=</span> <span class="n">K_star</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_K_star_M</span><span class="p">(</span><span class="n">K_M_star</span><span class="p">)</span>
<a id="__codelineno-0-2676" name="__codelineno-0-2676"></a>
<a id="__codelineno-0-2677" name="__codelineno-0-2677"></a>    <span class="c1"># If the considered kernel is diagonal, embed the covariances.</span>
<a id="__codelineno-0-2678" name="__codelineno-0-2678"></a>    <span class="c1"># from (N*, C) -&gt; (N*, C, C)</span>
<a id="__codelineno-0-2679" name="__codelineno-0-2679"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">independent_outputs</span><span class="p">:</span>
<a id="__codelineno-0-2680" name="__codelineno-0-2680"></a>        <span class="n">f_var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag_embed</span><span class="p">(</span><span class="n">f_var</span><span class="p">)</span>
<a id="__codelineno-0-2681" name="__codelineno-0-2681"></a>
<a id="__codelineno-0-2682" name="__codelineno-0-2682"></a>    <span class="k">return</span> <span class="n">f_var</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="laplace.baselaplace.FunctionalLaplace.functional_covariance" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">functional_covariance</span>


<a href="#laplace.baselaplace.FunctionalLaplace.functional_covariance" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nf">functional_covariance</span><span class="p">(</span><span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace.functional_covariance(Js_star)">Js_star</a></span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="torch.Tensor">Tensor</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>GP posterior covariance:</p>
<div class="arithmatex">\[
    k_{**} - K_{*M} (K_{MM}+ L_{MM}^{-1})^{-1} K_{M*}
\]</div>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace.functional_covariance(Js_star)" class="doc doc-heading doc-heading-parameter">              <b><code>Js_star</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace.functional_covariance(Js_star)" class="headerlink" title="Permanent link">#</a></h4>              (<code>torch.Tensor of shape (N*, C, P)</code>)
          –
          <div class="doc-md-description">
            <div class="highlight"><pre><span></span><code>  Jacobians of test data points
</code></pre></div>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>f_var</code></b> (              <code>torch.Tensor of shape (N*xC, N*xC)</code>
)          –
          <div class="doc-md-description">
            <p>Contains the posterior covariances of N* testing points.</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>laplace/baselaplace.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-2684">2684</a></span>
<span class="normal"><a href="#__codelineno-0-2685">2685</a></span>
<span class="normal"><a href="#__codelineno-0-2686">2686</a></span>
<span class="normal"><a href="#__codelineno-0-2687">2687</a></span>
<span class="normal"><a href="#__codelineno-0-2688">2688</a></span>
<span class="normal"><a href="#__codelineno-0-2689">2689</a></span>
<span class="normal"><a href="#__codelineno-0-2690">2690</a></span>
<span class="normal"><a href="#__codelineno-0-2691">2691</a></span>
<span class="normal"><a href="#__codelineno-0-2692">2692</a></span>
<span class="normal"><a href="#__codelineno-0-2693">2693</a></span>
<span class="normal"><a href="#__codelineno-0-2694">2694</a></span>
<span class="normal"><a href="#__codelineno-0-2695">2695</a></span>
<span class="normal"><a href="#__codelineno-0-2696">2696</a></span>
<span class="normal"><a href="#__codelineno-0-2697">2697</a></span>
<span class="normal"><a href="#__codelineno-0-2698">2698</a></span>
<span class="normal"><a href="#__codelineno-0-2699">2699</a></span>
<span class="normal"><a href="#__codelineno-0-2700">2700</a></span>
<span class="normal"><a href="#__codelineno-0-2701">2701</a></span>
<span class="normal"><a href="#__codelineno-0-2702">2702</a></span>
<span class="normal"><a href="#__codelineno-0-2703">2703</a></span>
<span class="normal"><a href="#__codelineno-0-2704">2704</a></span>
<span class="normal"><a href="#__codelineno-0-2705">2705</a></span>
<span class="normal"><a href="#__codelineno-0-2706">2706</a></span>
<span class="normal"><a href="#__codelineno-0-2707">2707</a></span>
<span class="normal"><a href="#__codelineno-0-2708">2708</a></span>
<span class="normal"><a href="#__codelineno-0-2709">2709</a></span>
<span class="normal"><a href="#__codelineno-0-2710">2710</a></span>
<span class="normal"><a href="#__codelineno-0-2711">2711</a></span>
<span class="normal"><a href="#__codelineno-0-2712">2712</a></span>
<span class="normal"><a href="#__codelineno-0-2713">2713</a></span>
<span class="normal"><a href="#__codelineno-0-2714">2714</a></span>
<span class="normal"><a href="#__codelineno-0-2715">2715</a></span>
<span class="normal"><a href="#__codelineno-0-2716">2716</a></span>
<span class="normal"><a href="#__codelineno-0-2717">2717</a></span>
<span class="normal"><a href="#__codelineno-0-2718">2718</a></span>
<span class="normal"><a href="#__codelineno-0-2719">2719</a></span>
<span class="normal"><a href="#__codelineno-0-2720">2720</a></span>
<span class="normal"><a href="#__codelineno-0-2721">2721</a></span>
<span class="normal"><a href="#__codelineno-0-2722">2722</a></span>
<span class="normal"><a href="#__codelineno-0-2723">2723</a></span>
<span class="normal"><a href="#__codelineno-0-2724">2724</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-2684" name="__codelineno-0-2684"></a><span class="k">def</span> <span class="nf">functional_covariance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Js_star</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-2685" name="__codelineno-0-2685"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;GP posterior covariance:</span>
<a id="__codelineno-0-2686" name="__codelineno-0-2686"></a>
<a id="__codelineno-0-2687" name="__codelineno-0-2687"></a><span class="sd">    $$</span>
<a id="__codelineno-0-2688" name="__codelineno-0-2688"></a><span class="sd">        k_{**} - K_{*M} (K_{MM}+ L_{MM}^{-1})^{-1} K_{M*}</span>
<a id="__codelineno-0-2689" name="__codelineno-0-2689"></a><span class="sd">    $$</span>
<a id="__codelineno-0-2690" name="__codelineno-0-2690"></a>
<a id="__codelineno-0-2691" name="__codelineno-0-2691"></a><span class="sd">    Parameters</span>
<a id="__codelineno-0-2692" name="__codelineno-0-2692"></a><span class="sd">    ----------</span>
<a id="__codelineno-0-2693" name="__codelineno-0-2693"></a><span class="sd">    Js_star : torch.Tensor of shape (N*, C, P)</span>
<a id="__codelineno-0-2694" name="__codelineno-0-2694"></a><span class="sd">              Jacobians of test data points</span>
<a id="__codelineno-0-2695" name="__codelineno-0-2695"></a>
<a id="__codelineno-0-2696" name="__codelineno-0-2696"></a><span class="sd">    Returns</span>
<a id="__codelineno-0-2697" name="__codelineno-0-2697"></a><span class="sd">    -------</span>
<a id="__codelineno-0-2698" name="__codelineno-0-2698"></a><span class="sd">    f_var : torch.Tensor of shape (N*xC, N*xC)</span>
<a id="__codelineno-0-2699" name="__codelineno-0-2699"></a><span class="sd">            Contains the posterior covariances of N* testing points.</span>
<a id="__codelineno-0-2700" name="__codelineno-0-2700"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-2701" name="__codelineno-0-2701"></a>    <span class="c1"># Compute K_{**}</span>
<a id="__codelineno-0-2702" name="__codelineno-0-2702"></a>    <span class="n">K_star</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp_kernel_prior_variance</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_star</span><span class="p">(</span><span class="n">Js_star</span><span class="p">,</span> <span class="n">joint</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<a id="__codelineno-0-2703" name="__codelineno-0-2703"></a>
<a id="__codelineno-0-2704" name="__codelineno-0-2704"></a>    <span class="c1"># Compute K_{*M}</span>
<a id="__codelineno-0-2705" name="__codelineno-0-2705"></a>    <span class="n">K_M_star</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-0-2706" name="__codelineno-0-2706"></a>    <span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="p">:</span>
<a id="__codelineno-0-2707" name="__codelineno-0-2707"></a>        <span class="n">K_M_star_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gp_kernel_prior_variance</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_batch_star</span><span class="p">(</span>
<a id="__codelineno-0-2708" name="__codelineno-0-2708"></a>            <span class="n">Js_star</span><span class="p">,</span> <span class="n">X_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>
<a id="__codelineno-0-2709" name="__codelineno-0-2709"></a>        <span class="p">)</span>
<a id="__codelineno-0-2710" name="__codelineno-0-2710"></a>        <span class="n">K_M_star</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">K_M_star_batch</span><span class="p">)</span>
<a id="__codelineno-0-2711" name="__codelineno-0-2711"></a>        <span class="k">del</span> <span class="n">X_batch</span>
<a id="__codelineno-0-2712" name="__codelineno-0-2712"></a>
<a id="__codelineno-0-2713" name="__codelineno-0-2713"></a>    <span class="c1"># Build_K_star_M computes K_{*M} (K_{MM}+ L_{MM}^{-1})^{-1} K_{M*}</span>
<a id="__codelineno-0-2714" name="__codelineno-0-2714"></a>    <span class="n">f_var</span> <span class="o">=</span> <span class="n">K_star</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_K_star_M</span><span class="p">(</span><span class="n">K_M_star</span><span class="p">,</span> <span class="n">joint</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<a id="__codelineno-0-2715" name="__codelineno-0-2715"></a>
<a id="__codelineno-0-2716" name="__codelineno-0-2716"></a>    <span class="c1"># If the considered kernel is diagonal, embed the covariances.</span>
<a id="__codelineno-0-2717" name="__codelineno-0-2717"></a>    <span class="c1"># from (N*, N*, C) -&gt; (N*, N*, C, C)</span>
<a id="__codelineno-0-2718" name="__codelineno-0-2718"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">independent_outputs</span><span class="p">:</span>
<a id="__codelineno-0-2719" name="__codelineno-0-2719"></a>        <span class="n">f_var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag_embed</span><span class="p">(</span><span class="n">f_var</span><span class="p">)</span>
<a id="__codelineno-0-2720" name="__codelineno-0-2720"></a>
<a id="__codelineno-0-2721" name="__codelineno-0-2721"></a>    <span class="c1"># Reshape from (N*, N*, C, C) to (N*xC, N*xC)</span>
<a id="__codelineno-0-2722" name="__codelineno-0-2722"></a>    <span class="n">f_var</span> <span class="o">=</span> <span class="n">f_var</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-0-2723" name="__codelineno-0-2723"></a>
<a id="__codelineno-0-2724" name="__codelineno-0-2724"></a>    <span class="k">return</span> <span class="n">f_var</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="laplace.baselaplace.FunctionalLaplace._build_K_star_M" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">_build_K_star_M</span>


<a href="#laplace.baselaplace.FunctionalLaplace._build_K_star_M" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nf">_build_K_star_M</span><span class="p">(</span><span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._build_K_star_M(K_M_star)">K_M_star</a></span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._build_K_star_M(joint)">joint</a></span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="torch.Tensor">Tensor</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Computes K_{<em>M} (K_{MM}+ L_{MM}^{-1})^{-1} K_{M</em>} given K_{M*}.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._build_K_star_M(K_M_star)" class="doc doc-heading doc-heading-parameter">              <b><code>K_M_star</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._build_K_star_M(K_M_star)" class="headerlink" title="Permanent link">#</a></h4>              (<code>list of torch.Tensor</code>)
          –
          <div class="doc-md-description">
            <div class="highlight"><pre><span></span><code>   Contains K_{M*}. Tensors have shape (N_test, C, C)
   or (N_test, C) for diagonal kernel.
</code></pre></div>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._build_K_star_M(joint)" class="doc doc-heading doc-heading-parameter">              <b><code>joint</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._build_K_star_M(joint)" class="headerlink" title="Permanent link">#</a></h4>              (<code>boolean</code>, default:
                  <code>False</code>
)
          –
          <div class="doc-md-description">
            <div class="highlight"><pre><span></span><code>Wether to compute cross covariances or not.
</code></pre></div>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
              <code>torch.tensor of shape (N_test, N_test, C) for joint diagonal,</code>
          –
          <div class="doc-md-description">
            
          </div>
        </li>
        <li class="doc-section-item field-body">
              <code>(N_test, C) for non-joint diagonal, (N_test, N_test, C, C) for</code>
          –
          <div class="doc-md-description">
            
          </div>
        </li>
        <li class="doc-section-item field-body">
              <code>joint non-diagonal and (N_test, C, C) for non-joint non-diagonal.</code>
          –
          <div class="doc-md-description">
            
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>laplace/baselaplace.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-2726">2726</a></span>
<span class="normal"><a href="#__codelineno-0-2727">2727</a></span>
<span class="normal"><a href="#__codelineno-0-2728">2728</a></span>
<span class="normal"><a href="#__codelineno-0-2729">2729</a></span>
<span class="normal"><a href="#__codelineno-0-2730">2730</a></span>
<span class="normal"><a href="#__codelineno-0-2731">2731</a></span>
<span class="normal"><a href="#__codelineno-0-2732">2732</a></span>
<span class="normal"><a href="#__codelineno-0-2733">2733</a></span>
<span class="normal"><a href="#__codelineno-0-2734">2734</a></span>
<span class="normal"><a href="#__codelineno-0-2735">2735</a></span>
<span class="normal"><a href="#__codelineno-0-2736">2736</a></span>
<span class="normal"><a href="#__codelineno-0-2737">2737</a></span>
<span class="normal"><a href="#__codelineno-0-2738">2738</a></span>
<span class="normal"><a href="#__codelineno-0-2739">2739</a></span>
<span class="normal"><a href="#__codelineno-0-2740">2740</a></span>
<span class="normal"><a href="#__codelineno-0-2741">2741</a></span>
<span class="normal"><a href="#__codelineno-0-2742">2742</a></span>
<span class="normal"><a href="#__codelineno-0-2743">2743</a></span>
<span class="normal"><a href="#__codelineno-0-2744">2744</a></span>
<span class="normal"><a href="#__codelineno-0-2745">2745</a></span>
<span class="normal"><a href="#__codelineno-0-2746">2746</a></span>
<span class="normal"><a href="#__codelineno-0-2747">2747</a></span>
<span class="normal"><a href="#__codelineno-0-2748">2748</a></span>
<span class="normal"><a href="#__codelineno-0-2749">2749</a></span>
<span class="normal"><a href="#__codelineno-0-2750">2750</a></span>
<span class="normal"><a href="#__codelineno-0-2751">2751</a></span>
<span class="normal"><a href="#__codelineno-0-2752">2752</a></span>
<span class="normal"><a href="#__codelineno-0-2753">2753</a></span>
<span class="normal"><a href="#__codelineno-0-2754">2754</a></span>
<span class="normal"><a href="#__codelineno-0-2755">2755</a></span>
<span class="normal"><a href="#__codelineno-0-2756">2756</a></span>
<span class="normal"><a href="#__codelineno-0-2757">2757</a></span>
<span class="normal"><a href="#__codelineno-0-2758">2758</a></span>
<span class="normal"><a href="#__codelineno-0-2759">2759</a></span>
<span class="normal"><a href="#__codelineno-0-2760">2760</a></span>
<span class="normal"><a href="#__codelineno-0-2761">2761</a></span>
<span class="normal"><a href="#__codelineno-0-2762">2762</a></span>
<span class="normal"><a href="#__codelineno-0-2763">2763</a></span>
<span class="normal"><a href="#__codelineno-0-2764">2764</a></span>
<span class="normal"><a href="#__codelineno-0-2765">2765</a></span>
<span class="normal"><a href="#__codelineno-0-2766">2766</a></span>
<span class="normal"><a href="#__codelineno-0-2767">2767</a></span>
<span class="normal"><a href="#__codelineno-0-2768">2768</a></span>
<span class="normal"><a href="#__codelineno-0-2769">2769</a></span>
<span class="normal"><a href="#__codelineno-0-2770">2770</a></span>
<span class="normal"><a href="#__codelineno-0-2771">2771</a></span>
<span class="normal"><a href="#__codelineno-0-2772">2772</a></span>
<span class="normal"><a href="#__codelineno-0-2773">2773</a></span>
<span class="normal"><a href="#__codelineno-0-2774">2774</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-2726" name="__codelineno-0-2726"></a><span class="k">def</span> <span class="nf">_build_K_star_M</span><span class="p">(</span>
<a id="__codelineno-0-2727" name="__codelineno-0-2727"></a>    <span class="bp">self</span><span class="p">,</span> <span class="n">K_M_star</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">joint</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<a id="__codelineno-0-2728" name="__codelineno-0-2728"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-2729" name="__codelineno-0-2729"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Computes K_{*M} (K_{MM}+ L_{MM}^{-1})^{-1} K_{M*} given K_{M*}.</span>
<a id="__codelineno-0-2730" name="__codelineno-0-2730"></a>
<a id="__codelineno-0-2731" name="__codelineno-0-2731"></a><span class="sd">    Parameters</span>
<a id="__codelineno-0-2732" name="__codelineno-0-2732"></a><span class="sd">    ----------</span>
<a id="__codelineno-0-2733" name="__codelineno-0-2733"></a><span class="sd">    K_M_star : list of torch.Tensor</span>
<a id="__codelineno-0-2734" name="__codelineno-0-2734"></a><span class="sd">               Contains K_{M*}. Tensors have shape (N_test, C, C)</span>
<a id="__codelineno-0-2735" name="__codelineno-0-2735"></a><span class="sd">               or (N_test, C) for diagonal kernel.</span>
<a id="__codelineno-0-2736" name="__codelineno-0-2736"></a>
<a id="__codelineno-0-2737" name="__codelineno-0-2737"></a><span class="sd">    joint : boolean</span>
<a id="__codelineno-0-2738" name="__codelineno-0-2738"></a><span class="sd">            Wether to compute cross covariances or not.</span>
<a id="__codelineno-0-2739" name="__codelineno-0-2739"></a>
<a id="__codelineno-0-2740" name="__codelineno-0-2740"></a><span class="sd">    Returns</span>
<a id="__codelineno-0-2741" name="__codelineno-0-2741"></a><span class="sd">    -------</span>
<a id="__codelineno-0-2742" name="__codelineno-0-2742"></a><span class="sd">    torch.tensor of shape (N_test, N_test, C) for joint diagonal,</span>
<a id="__codelineno-0-2743" name="__codelineno-0-2743"></a><span class="sd">    (N_test, C) for non-joint diagonal, (N_test, N_test, C, C) for</span>
<a id="__codelineno-0-2744" name="__codelineno-0-2744"></a><span class="sd">    joint non-diagonal and (N_test, C, C) for non-joint non-diagonal.</span>
<a id="__codelineno-0-2745" name="__codelineno-0-2745"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-2746" name="__codelineno-0-2746"></a>    <span class="c1"># Shape (N_test, N, C, C) or (N_test, N, C) for diagonal</span>
<a id="__codelineno-0-2747" name="__codelineno-0-2747"></a>    <span class="n">K_M_star</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">K_M_star</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-2748" name="__codelineno-0-2748"></a>
<a id="__codelineno-0-2749" name="__codelineno-0-2749"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">independent_outputs</span><span class="p">:</span>
<a id="__codelineno-0-2750" name="__codelineno-0-2750"></a>        <span class="n">prods</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-0-2751" name="__codelineno-0-2751"></a>        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">):</span>
<a id="__codelineno-0-2752" name="__codelineno-0-2752"></a>            <span class="c1"># Compute K_{*M}L^{-1}</span>
<a id="__codelineno-0-2753" name="__codelineno-0-2753"></a>            <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span>
<a id="__codelineno-0-2754" name="__codelineno-0-2754"></a>                <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span>
<a id="__codelineno-0-2755" name="__codelineno-0-2755"></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">Sigma_inv</span><span class="p">[</span><span class="n">c</span><span class="p">],</span> <span class="n">K_M_star</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-0-2756" name="__codelineno-0-2756"></a>                <span class="p">),</span>
<a id="__codelineno-0-2757" name="__codelineno-0-2757"></a>                <span class="mi">2</span><span class="p">,</span>
<a id="__codelineno-0-2758" name="__codelineno-0-2758"></a>            <span class="p">)</span>
<a id="__codelineno-0-2759" name="__codelineno-0-2759"></a>            <span class="k">if</span> <span class="n">joint</span><span class="p">:</span>
<a id="__codelineno-0-2760" name="__codelineno-0-2760"></a>                <span class="n">prod</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bm,am-&gt;ba&quot;</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<a id="__codelineno-0-2761" name="__codelineno-0-2761"></a>            <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-2762" name="__codelineno-0-2762"></a>                <span class="n">prod</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bm,bm-&gt;b&quot;</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<a id="__codelineno-0-2763" name="__codelineno-0-2763"></a>            <span class="n">prods</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">prod</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<a id="__codelineno-0-2764" name="__codelineno-0-2764"></a>        <span class="n">prods</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">prods</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-2765" name="__codelineno-0-2765"></a>        <span class="k">return</span> <span class="n">prods</span>
<a id="__codelineno-0-2766" name="__codelineno-0-2766"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-2767" name="__codelineno-0-2767"></a>        <span class="c1"># Reshape to (N_test, NxC, C) or (N_test, N, C)</span>
<a id="__codelineno-0-2768" name="__codelineno-0-2768"></a>        <span class="n">K_M_star</span> <span class="o">=</span> <span class="n">K_M_star</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">K_M_star</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">K_M_star</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<a id="__codelineno-0-2769" name="__codelineno-0-2769"></a>        <span class="c1"># Compute K_{*M}L^{-1}</span>
<a id="__codelineno-0-2770" name="__codelineno-0-2770"></a>        <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Sigma_inv</span><span class="p">,</span> <span class="n">K_M_star</span><span class="p">)</span>
<a id="__codelineno-0-2771" name="__codelineno-0-2771"></a>        <span class="k">if</span> <span class="n">joint</span><span class="p">:</span>
<a id="__codelineno-0-2772" name="__codelineno-0-2772"></a>            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;acm,bcn-&gt;abmn&quot;</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<a id="__codelineno-0-2773" name="__codelineno-0-2773"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-2774" name="__codelineno-0-2774"></a>            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bcm,bcn-&gt;bmn&quot;</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="laplace.baselaplace.FunctionalLaplace.optimize_prior_precision" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">optimize_prior_precision</span>


<a href="#laplace.baselaplace.FunctionalLaplace.optimize_prior_precision" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nf">optimize_prior_precision</span><span class="p">(</span><span class="n">pred_type</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="laplace.utils.enums.PredType" href="../enums/#laplace.utils.enums.PredType">PredType</a></span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span> <span class="o">=</span> <span class="n"><a class="autorefs autorefs-internal" title="laplace.utils.enums.PredType.GP" href="../enums/#laplace.utils.enums.PredType.GP">GP</a></span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="laplace.utils.enums.TuningMethod" href="../enums/#laplace.utils.enums.TuningMethod">TuningMethod</a></span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span> <span class="o">=</span> <span class="n"><a class="autorefs autorefs-internal" title="laplace.utils.enums.TuningMethod.MARGLIK" href="../enums/#laplace.utils.enums.TuningMethod.MARGLIK">MARGLIK</a></span><span class="p">,</span> <span class="n">n_steps</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">lr</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">init_prior_prec</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span> <span class="o">|</span> <span class="n"><span title="torch.Tensor">Tensor</span></span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">prior_structure</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="laplace.utils.enums.PriorStructure" href="../enums/#laplace.utils.enums.PriorStructure">PriorStructure</a></span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span> <span class="o">=</span> <span class="n"><a class="autorefs autorefs-internal" title="laplace.utils.enums.PriorStructure.SCALAR" href="../enums/#laplace.utils.enums.PriorStructure.SCALAR">SCALAR</a></span><span class="p">,</span> <span class="n">val_loader</span><span class="p">:</span> <span class="n"><span title="torch.utils.data.DataLoader">DataLoader</span></span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="n"><span title="torchmetrics.Metric">Metric</span></span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" title="typing.Callable" href="https://docs.python.org/3/library/typing.html#typing.Callable">Callable</a></span><span class="p">[[</span><span class="n"><span title="torch.Tensor">Tensor</span></span><span class="p">],</span> <span class="n"><span title="torch.Tensor">Tensor</span></span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">log_prior_prec_min</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="n">log_prior_prec_max</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#float">float</a></span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">grid_size</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">link_approx</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-internal" title="laplace.utils.enums.LinkApprox" href="../enums/#laplace.utils.enums.LinkApprox">LinkApprox</a></span> <span class="o">|</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#str">str</a></span> <span class="o">=</span> <span class="n"><a class="autorefs autorefs-internal" title="laplace.utils.enums.LinkApprox.PROBIT" href="../enums/#laplace.utils.enums.LinkApprox.PROBIT">PROBIT</a></span><span class="p">,</span> <span class="n">n_samples</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#int">int</a></span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">progress_bar</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p><code>optimize_prior_precision_base</code> from <code>BaseLaplace</code> with <code>pred_type='gp'</code></p>

            <details class="quote">
              <summary>Source code in <code>laplace/baselaplace.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-2890">2890</a></span>
<span class="normal"><a href="#__codelineno-0-2891">2891</a></span>
<span class="normal"><a href="#__codelineno-0-2892">2892</a></span>
<span class="normal"><a href="#__codelineno-0-2893">2893</a></span>
<span class="normal"><a href="#__codelineno-0-2894">2894</a></span>
<span class="normal"><a href="#__codelineno-0-2895">2895</a></span>
<span class="normal"><a href="#__codelineno-0-2896">2896</a></span>
<span class="normal"><a href="#__codelineno-0-2897">2897</a></span>
<span class="normal"><a href="#__codelineno-0-2898">2898</a></span>
<span class="normal"><a href="#__codelineno-0-2899">2899</a></span>
<span class="normal"><a href="#__codelineno-0-2900">2900</a></span>
<span class="normal"><a href="#__codelineno-0-2901">2901</a></span>
<span class="normal"><a href="#__codelineno-0-2902">2902</a></span>
<span class="normal"><a href="#__codelineno-0-2903">2903</a></span>
<span class="normal"><a href="#__codelineno-0-2904">2904</a></span>
<span class="normal"><a href="#__codelineno-0-2905">2905</a></span>
<span class="normal"><a href="#__codelineno-0-2906">2906</a></span>
<span class="normal"><a href="#__codelineno-0-2907">2907</a></span>
<span class="normal"><a href="#__codelineno-0-2908">2908</a></span>
<span class="normal"><a href="#__codelineno-0-2909">2909</a></span>
<span class="normal"><a href="#__codelineno-0-2910">2910</a></span>
<span class="normal"><a href="#__codelineno-0-2911">2911</a></span>
<span class="normal"><a href="#__codelineno-0-2912">2912</a></span>
<span class="normal"><a href="#__codelineno-0-2913">2913</a></span>
<span class="normal"><a href="#__codelineno-0-2914">2914</a></span>
<span class="normal"><a href="#__codelineno-0-2915">2915</a></span>
<span class="normal"><a href="#__codelineno-0-2916">2916</a></span>
<span class="normal"><a href="#__codelineno-0-2917">2917</a></span>
<span class="normal"><a href="#__codelineno-0-2918">2918</a></span>
<span class="normal"><a href="#__codelineno-0-2919">2919</a></span>
<span class="normal"><a href="#__codelineno-0-2920">2920</a></span>
<span class="normal"><a href="#__codelineno-0-2921">2921</a></span>
<span class="normal"><a href="#__codelineno-0-2922">2922</a></span>
<span class="normal"><a href="#__codelineno-0-2923">2923</a></span>
<span class="normal"><a href="#__codelineno-0-2924">2924</a></span>
<span class="normal"><a href="#__codelineno-0-2925">2925</a></span>
<span class="normal"><a href="#__codelineno-0-2926">2926</a></span>
<span class="normal"><a href="#__codelineno-0-2927">2927</a></span>
<span class="normal"><a href="#__codelineno-0-2928">2928</a></span>
<span class="normal"><a href="#__codelineno-0-2929">2929</a></span>
<span class="normal"><a href="#__codelineno-0-2930">2930</a></span>
<span class="normal"><a href="#__codelineno-0-2931">2931</a></span>
<span class="normal"><a href="#__codelineno-0-2932">2932</a></span>
<span class="normal"><a href="#__codelineno-0-2933">2933</a></span>
<span class="normal"><a href="#__codelineno-0-2934">2934</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-2890" name="__codelineno-0-2890"></a><span class="k">def</span> <span class="nf">optimize_prior_precision</span><span class="p">(</span>
<a id="__codelineno-0-2891" name="__codelineno-0-2891"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-2892" name="__codelineno-0-2892"></a>    <span class="n">pred_type</span><span class="p">:</span> <span class="n">PredType</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">PredType</span><span class="o">.</span><span class="n">GP</span><span class="p">,</span>
<a id="__codelineno-0-2893" name="__codelineno-0-2893"></a>    <span class="n">method</span><span class="p">:</span> <span class="n">TuningMethod</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">TuningMethod</span><span class="o">.</span><span class="n">MARGLIK</span><span class="p">,</span>
<a id="__codelineno-0-2894" name="__codelineno-0-2894"></a>    <span class="n">n_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-0-2895" name="__codelineno-0-2895"></a>    <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-1</span><span class="p">,</span>
<a id="__codelineno-0-2896" name="__codelineno-0-2896"></a>    <span class="n">init_prior_prec</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
<a id="__codelineno-0-2897" name="__codelineno-0-2897"></a>    <span class="n">prior_structure</span><span class="p">:</span> <span class="n">PriorStructure</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">PriorStructure</span><span class="o">.</span><span class="n">SCALAR</span><span class="p">,</span>
<a id="__codelineno-0-2898" name="__codelineno-0-2898"></a>    <span class="n">val_loader</span><span class="p">:</span> <span class="n">DataLoader</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-2899" name="__codelineno-0-2899"></a>    <span class="n">loss</span><span class="p">:</span> <span class="n">torchmetrics</span><span class="o">.</span><span class="n">Metric</span>
<a id="__codelineno-0-2900" name="__codelineno-0-2900"></a>    <span class="o">|</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">float</span><span class="p">]</span>
<a id="__codelineno-0-2901" name="__codelineno-0-2901"></a>    <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-2902" name="__codelineno-0-2902"></a>    <span class="n">log_prior_prec_min</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span><span class="p">,</span>
<a id="__codelineno-0-2903" name="__codelineno-0-2903"></a>    <span class="n">log_prior_prec_max</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
<a id="__codelineno-0-2904" name="__codelineno-0-2904"></a>    <span class="n">grid_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-0-2905" name="__codelineno-0-2905"></a>    <span class="n">link_approx</span><span class="p">:</span> <span class="n">LinkApprox</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">LinkApprox</span><span class="o">.</span><span class="n">PROBIT</span><span class="p">,</span>
<a id="__codelineno-0-2906" name="__codelineno-0-2906"></a>    <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-0-2907" name="__codelineno-0-2907"></a>    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-0-2908" name="__codelineno-0-2908"></a>    <span class="n">progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-0-2909" name="__codelineno-0-2909"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-2910" name="__codelineno-0-2910"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;`optimize_prior_precision_base` from `BaseLaplace` with `pred_type=&#39;gp&#39;`&quot;&quot;&quot;</span>
<a id="__codelineno-0-2911" name="__codelineno-0-2911"></a>    <span class="k">assert</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="n">PredType</span><span class="o">.</span><span class="n">GP</span>  <span class="c1"># only gp supported</span>
<a id="__codelineno-0-2912" name="__codelineno-0-2912"></a>    <span class="k">assert</span> <span class="n">prior_structure</span> <span class="o">==</span> <span class="s2">&quot;scalar&quot;</span>  <span class="c1"># only isotropic gaussian prior supported</span>
<a id="__codelineno-0-2913" name="__codelineno-0-2913"></a>    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;marglik&quot;</span><span class="p">:</span>
<a id="__codelineno-0-2914" name="__codelineno-0-2914"></a>        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
<a id="__codelineno-0-2915" name="__codelineno-0-2915"></a>            <span class="s2">&quot;Use of method=&#39;marglik&#39; in case of FunctionalLaplace is discouraged, rather use method=&#39;CV&#39;.&quot;</span>
<a id="__codelineno-0-2916" name="__codelineno-0-2916"></a>        <span class="p">)</span>
<a id="__codelineno-0-2917" name="__codelineno-0-2917"></a>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">optimize_prior_precision</span><span class="p">(</span>
<a id="__codelineno-0-2918" name="__codelineno-0-2918"></a>        <span class="n">pred_type</span><span class="p">,</span>
<a id="__codelineno-0-2919" name="__codelineno-0-2919"></a>        <span class="n">method</span><span class="p">,</span>
<a id="__codelineno-0-2920" name="__codelineno-0-2920"></a>        <span class="n">n_steps</span><span class="p">,</span>
<a id="__codelineno-0-2921" name="__codelineno-0-2921"></a>        <span class="n">lr</span><span class="p">,</span>
<a id="__codelineno-0-2922" name="__codelineno-0-2922"></a>        <span class="n">init_prior_prec</span><span class="p">,</span>
<a id="__codelineno-0-2923" name="__codelineno-0-2923"></a>        <span class="n">prior_structure</span><span class="p">,</span>
<a id="__codelineno-0-2924" name="__codelineno-0-2924"></a>        <span class="n">val_loader</span><span class="p">,</span>
<a id="__codelineno-0-2925" name="__codelineno-0-2925"></a>        <span class="n">loss</span><span class="p">,</span>
<a id="__codelineno-0-2926" name="__codelineno-0-2926"></a>        <span class="n">log_prior_prec_min</span><span class="p">,</span>
<a id="__codelineno-0-2927" name="__codelineno-0-2927"></a>        <span class="n">log_prior_prec_max</span><span class="p">,</span>
<a id="__codelineno-0-2928" name="__codelineno-0-2928"></a>        <span class="n">grid_size</span><span class="p">,</span>
<a id="__codelineno-0-2929" name="__codelineno-0-2929"></a>        <span class="n">link_approx</span><span class="p">,</span>
<a id="__codelineno-0-2930" name="__codelineno-0-2930"></a>        <span class="n">n_samples</span><span class="p">,</span>
<a id="__codelineno-0-2931" name="__codelineno-0-2931"></a>        <span class="n">verbose</span><span class="p">,</span>
<a id="__codelineno-0-2932" name="__codelineno-0-2932"></a>        <span class="n">progress_bar</span><span class="p">,</span>
<a id="__codelineno-0-2933" name="__codelineno-0-2933"></a>    <span class="p">)</span>
<a id="__codelineno-0-2934" name="__codelineno-0-2934"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">_build_Sigma_inv</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="laplace.baselaplace.FunctionalLaplace._kernel_batch" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">_kernel_batch</span>


<a href="#laplace.baselaplace.FunctionalLaplace._kernel_batch" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nf">_kernel_batch</span><span class="p">(</span><span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._kernel_batch(jacobians)">jacobians</a></span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._kernel_batch(batch)">batch</a></span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="torch.Tensor">Tensor</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Compute K_bb, which is part of K_MM kernel matrix.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._kernel_batch(jacobians)" class="doc doc-heading doc-heading-parameter">              <b><code>jacobians</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._kernel_batch(jacobians)" class="headerlink" title="Permanent link">#</a></h4>              (<code><span title="torch.Tensor">Tensor</span>(b, C, P)</code>)
          –
          <div class="doc-md-description">
            
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._kernel_batch(batch)" class="doc doc-heading doc-heading-parameter">              <b><code>batch</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._kernel_batch(batch)" class="headerlink" title="Permanent link">#</a></h4>              (<code><span title="torch.Tensor">Tensor</span>(b, C)</code>)
          –
          <div class="doc-md-description">
            
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>kernel</code></b> (              <code><span title="torch.tensor">tensor</span></code>
)          –
          <div class="doc-md-description">
            <p>K_bb with shape (b * C, b * C)</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>laplace/baselaplace.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-2936">2936</a></span>
<span class="normal"><a href="#__codelineno-0-2937">2937</a></span>
<span class="normal"><a href="#__codelineno-0-2938">2938</a></span>
<span class="normal"><a href="#__codelineno-0-2939">2939</a></span>
<span class="normal"><a href="#__codelineno-0-2940">2940</a></span>
<span class="normal"><a href="#__codelineno-0-2941">2941</a></span>
<span class="normal"><a href="#__codelineno-0-2942">2942</a></span>
<span class="normal"><a href="#__codelineno-0-2943">2943</a></span>
<span class="normal"><a href="#__codelineno-0-2944">2944</a></span>
<span class="normal"><a href="#__codelineno-0-2945">2945</a></span>
<span class="normal"><a href="#__codelineno-0-2946">2946</a></span>
<span class="normal"><a href="#__codelineno-0-2947">2947</a></span>
<span class="normal"><a href="#__codelineno-0-2948">2948</a></span>
<span class="normal"><a href="#__codelineno-0-2949">2949</a></span>
<span class="normal"><a href="#__codelineno-0-2950">2950</a></span>
<span class="normal"><a href="#__codelineno-0-2951">2951</a></span>
<span class="normal"><a href="#__codelineno-0-2952">2952</a></span>
<span class="normal"><a href="#__codelineno-0-2953">2953</a></span>
<span class="normal"><a href="#__codelineno-0-2954">2954</a></span>
<span class="normal"><a href="#__codelineno-0-2955">2955</a></span>
<span class="normal"><a href="#__codelineno-0-2956">2956</a></span>
<span class="normal"><a href="#__codelineno-0-2957">2957</a></span>
<span class="normal"><a href="#__codelineno-0-2958">2958</a></span>
<span class="normal"><a href="#__codelineno-0-2959">2959</a></span>
<span class="normal"><a href="#__codelineno-0-2960">2960</a></span>
<span class="normal"><a href="#__codelineno-0-2961">2961</a></span>
<span class="normal"><a href="#__codelineno-0-2962">2962</a></span>
<span class="normal"><a href="#__codelineno-0-2963">2963</a></span>
<span class="normal"><a href="#__codelineno-0-2964">2964</a></span>
<span class="normal"><a href="#__codelineno-0-2965">2965</a></span>
<span class="normal"><a href="#__codelineno-0-2966">2966</a></span>
<span class="normal"><a href="#__codelineno-0-2967">2967</a></span>
<span class="normal"><a href="#__codelineno-0-2968">2968</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-2936" name="__codelineno-0-2936"></a><span class="k">def</span> <span class="nf">_kernel_batch</span><span class="p">(</span>
<a id="__codelineno-0-2937" name="__codelineno-0-2937"></a>    <span class="bp">self</span><span class="p">,</span> <span class="n">jacobians</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<a id="__codelineno-0-2938" name="__codelineno-0-2938"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-2939" name="__codelineno-0-2939"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute K_bb, which is part of K_MM kernel matrix.</span>
<a id="__codelineno-0-2940" name="__codelineno-0-2940"></a>
<a id="__codelineno-0-2941" name="__codelineno-0-2941"></a><span class="sd">    Parameters</span>
<a id="__codelineno-0-2942" name="__codelineno-0-2942"></a><span class="sd">    ----------</span>
<a id="__codelineno-0-2943" name="__codelineno-0-2943"></a><span class="sd">    jacobians : torch.Tensor (b, C, P)</span>
<a id="__codelineno-0-2944" name="__codelineno-0-2944"></a><span class="sd">    batch : torch.Tensor (b, C)</span>
<a id="__codelineno-0-2945" name="__codelineno-0-2945"></a>
<a id="__codelineno-0-2946" name="__codelineno-0-2946"></a><span class="sd">    Returns</span>
<a id="__codelineno-0-2947" name="__codelineno-0-2947"></a><span class="sd">    -------</span>
<a id="__codelineno-0-2948" name="__codelineno-0-2948"></a><span class="sd">    kernel : torch.tensor</span>
<a id="__codelineno-0-2949" name="__codelineno-0-2949"></a><span class="sd">        K_bb with shape (b * C, b * C)</span>
<a id="__codelineno-0-2950" name="__codelineno-0-2950"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-2951" name="__codelineno-0-2951"></a>    <span class="n">jacobians_2</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jacobians</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<a id="__codelineno-0-2952" name="__codelineno-0-2952"></a>    <span class="n">P</span> <span class="o">=</span> <span class="n">jacobians</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># nr model params</span>
<a id="__codelineno-0-2953" name="__codelineno-0-2953"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">independent_outputs</span><span class="p">:</span>
<a id="__codelineno-0-2954" name="__codelineno-0-2954"></a>        <span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
<a id="__codelineno-0-2955" name="__codelineno-0-2955"></a>            <span class="p">(</span><span class="n">jacobians</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">jacobians_2</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">),</span>
<a id="__codelineno-0-2956" name="__codelineno-0-2956"></a>            <span class="n">device</span><span class="o">=</span><span class="n">jacobians</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<a id="__codelineno-0-2957" name="__codelineno-0-2957"></a>            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span>
<a id="__codelineno-0-2958" name="__codelineno-0-2958"></a>        <span class="p">)</span>
<a id="__codelineno-0-2959" name="__codelineno-0-2959"></a>        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">):</span>
<a id="__codelineno-0-2960" name="__codelineno-0-2960"></a>            <span class="n">kernel</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
<a id="__codelineno-0-2961" name="__codelineno-0-2961"></a>                <span class="s2">&quot;bp,ep-&gt;be&quot;</span><span class="p">,</span> <span class="n">jacobians</span><span class="p">[:,</span> <span class="n">c</span><span class="p">,</span> <span class="p">:],</span> <span class="n">jacobians_2</span><span class="p">[:,</span> <span class="n">c</span><span class="p">,</span> <span class="p">:]</span>
<a id="__codelineno-0-2962" name="__codelineno-0-2962"></a>            <span class="p">)</span>
<a id="__codelineno-0-2963" name="__codelineno-0-2963"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-2964" name="__codelineno-0-2964"></a>        <span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
<a id="__codelineno-0-2965" name="__codelineno-0-2965"></a>            <span class="s2">&quot;ap,bp-&gt;ab&quot;</span><span class="p">,</span> <span class="n">jacobians</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">P</span><span class="p">),</span> <span class="n">jacobians_2</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">P</span><span class="p">)</span>
<a id="__codelineno-0-2966" name="__codelineno-0-2966"></a>        <span class="p">)</span>
<a id="__codelineno-0-2967" name="__codelineno-0-2967"></a>    <span class="k">del</span> <span class="n">jacobians_2</span>
<a id="__codelineno-0-2968" name="__codelineno-0-2968"></a>    <span class="k">return</span> <span class="n">kernel</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="laplace.baselaplace.FunctionalLaplace._kernel_star" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">_kernel_star</span>


<a href="#laplace.baselaplace.FunctionalLaplace._kernel_star" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nf">_kernel_star</span><span class="p">(</span><span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._kernel_star(jacobians)">jacobians</a></span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span><span class="p">,</span> <span class="n">joint</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="torch.Tensor">Tensor</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Compute K_star_star kernel matrix.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._kernel_star(jacobians)" class="doc doc-heading doc-heading-parameter">              <b><code>jacobians</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._kernel_star(jacobians)" class="headerlink" title="Permanent link">#</a></h4>              (<code><span title="torch.Tensor">Tensor</span>(b, C, P)</code>)
          –
          <div class="doc-md-description">
            
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>kernel</code></b> (              <code><span title="torch.tensor">tensor</span></code>
)          –
          <div class="doc-md-description">
            <p>K_star with shape (b, C, C)</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>laplace/baselaplace.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-2970">2970</a></span>
<span class="normal"><a href="#__codelineno-0-2971">2971</a></span>
<span class="normal"><a href="#__codelineno-0-2972">2972</a></span>
<span class="normal"><a href="#__codelineno-0-2973">2973</a></span>
<span class="normal"><a href="#__codelineno-0-2974">2974</a></span>
<span class="normal"><a href="#__codelineno-0-2975">2975</a></span>
<span class="normal"><a href="#__codelineno-0-2976">2976</a></span>
<span class="normal"><a href="#__codelineno-0-2977">2977</a></span>
<span class="normal"><a href="#__codelineno-0-2978">2978</a></span>
<span class="normal"><a href="#__codelineno-0-2979">2979</a></span>
<span class="normal"><a href="#__codelineno-0-2980">2980</a></span>
<span class="normal"><a href="#__codelineno-0-2981">2981</a></span>
<span class="normal"><a href="#__codelineno-0-2982">2982</a></span>
<span class="normal"><a href="#__codelineno-0-2983">2983</a></span>
<span class="normal"><a href="#__codelineno-0-2984">2984</a></span>
<span class="normal"><a href="#__codelineno-0-2985">2985</a></span>
<span class="normal"><a href="#__codelineno-0-2986">2986</a></span>
<span class="normal"><a href="#__codelineno-0-2987">2987</a></span>
<span class="normal"><a href="#__codelineno-0-2988">2988</a></span>
<span class="normal"><a href="#__codelineno-0-2989">2989</a></span>
<span class="normal"><a href="#__codelineno-0-2990">2990</a></span>
<span class="normal"><a href="#__codelineno-0-2991">2991</a></span>
<span class="normal"><a href="#__codelineno-0-2992">2992</a></span>
<span class="normal"><a href="#__codelineno-0-2993">2993</a></span>
<span class="normal"><a href="#__codelineno-0-2994">2994</a></span>
<span class="normal"><a href="#__codelineno-0-2995">2995</a></span>
<span class="normal"><a href="#__codelineno-0-2996">2996</a></span>
<span class="normal"><a href="#__codelineno-0-2997">2997</a></span>
<span class="normal"><a href="#__codelineno-0-2998">2998</a></span>
<span class="normal"><a href="#__codelineno-0-2999">2999</a></span>
<span class="normal"><a href="#__codelineno-0-3000">3000</a></span>
<span class="normal"><a href="#__codelineno-0-3001">3001</a></span>
<span class="normal"><a href="#__codelineno-0-3002">3002</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-2970" name="__codelineno-0-2970"></a><span class="k">def</span> <span class="nf">_kernel_star</span><span class="p">(</span>
<a id="__codelineno-0-2971" name="__codelineno-0-2971"></a>    <span class="bp">self</span><span class="p">,</span> <span class="n">jacobians</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">joint</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<a id="__codelineno-0-2972" name="__codelineno-0-2972"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-2973" name="__codelineno-0-2973"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute K_star_star kernel matrix.</span>
<a id="__codelineno-0-2974" name="__codelineno-0-2974"></a>
<a id="__codelineno-0-2975" name="__codelineno-0-2975"></a><span class="sd">    Parameters</span>
<a id="__codelineno-0-2976" name="__codelineno-0-2976"></a><span class="sd">    ----------</span>
<a id="__codelineno-0-2977" name="__codelineno-0-2977"></a><span class="sd">    jacobians : torch.Tensor (b, C, P)</span>
<a id="__codelineno-0-2978" name="__codelineno-0-2978"></a>
<a id="__codelineno-0-2979" name="__codelineno-0-2979"></a><span class="sd">    Returns</span>
<a id="__codelineno-0-2980" name="__codelineno-0-2980"></a><span class="sd">    -------</span>
<a id="__codelineno-0-2981" name="__codelineno-0-2981"></a><span class="sd">    kernel : torch.tensor</span>
<a id="__codelineno-0-2982" name="__codelineno-0-2982"></a><span class="sd">        K_star with shape (b, C, C)</span>
<a id="__codelineno-0-2983" name="__codelineno-0-2983"></a>
<a id="__codelineno-0-2984" name="__codelineno-0-2984"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-2985" name="__codelineno-0-2985"></a>    <span class="k">if</span> <span class="n">joint</span><span class="p">:</span>
<a id="__codelineno-0-2986" name="__codelineno-0-2986"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">independent_outputs</span><span class="p">:</span>
<a id="__codelineno-0-2987" name="__codelineno-0-2987"></a>            <span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;acp,bcp-&gt;abcc&quot;</span><span class="p">,</span> <span class="n">jacobians</span><span class="p">,</span> <span class="n">jacobians</span><span class="p">)</span>
<a id="__codelineno-0-2988" name="__codelineno-0-2988"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-2989" name="__codelineno-0-2989"></a>            <span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;acp,bep-&gt;abce&quot;</span><span class="p">,</span> <span class="n">jacobians</span><span class="p">,</span> <span class="n">jacobians</span><span class="p">)</span>
<a id="__codelineno-0-2990" name="__codelineno-0-2990"></a>
<a id="__codelineno-0-2991" name="__codelineno-0-2991"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-2992" name="__codelineno-0-2992"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">independent_outputs</span><span class="p">:</span>
<a id="__codelineno-0-2993" name="__codelineno-0-2993"></a>            <span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
<a id="__codelineno-0-2994" name="__codelineno-0-2994"></a>                <span class="p">(</span><span class="n">jacobians</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">),</span>
<a id="__codelineno-0-2995" name="__codelineno-0-2995"></a>                <span class="n">device</span><span class="o">=</span><span class="n">jacobians</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<a id="__codelineno-0-2996" name="__codelineno-0-2996"></a>                <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span>
<a id="__codelineno-0-2997" name="__codelineno-0-2997"></a>            <span class="p">)</span>
<a id="__codelineno-0-2998" name="__codelineno-0-2998"></a>            <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">):</span>
<a id="__codelineno-0-2999" name="__codelineno-0-2999"></a>                <span class="n">kernel</span><span class="p">[:,</span> <span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">jacobians</span><span class="p">[:,</span> <span class="n">c</span><span class="p">,</span> <span class="p">:],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
<a id="__codelineno-0-3000" name="__codelineno-0-3000"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-3001" name="__codelineno-0-3001"></a>            <span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bcp,bep-&gt;bce&quot;</span><span class="p">,</span> <span class="n">jacobians</span><span class="p">,</span> <span class="n">jacobians</span><span class="p">)</span>
<a id="__codelineno-0-3002" name="__codelineno-0-3002"></a>    <span class="k">return</span> <span class="n">kernel</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="laplace.baselaplace.FunctionalLaplace._kernel_batch_star" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">_kernel_batch_star</span>


<a href="#laplace.baselaplace.FunctionalLaplace._kernel_batch_star" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nf">_kernel_batch_star</span><span class="p">(</span><span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._kernel_batch_star(jacobians)">jacobians</a></span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._kernel_batch_star(batch)">batch</a></span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="torch.Tensor">Tensor</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Compute K_b_star, which is a part of K_M_star kernel matrix.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._kernel_batch_star(jacobians)" class="doc doc-heading doc-heading-parameter">              <b><code>jacobians</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._kernel_batch_star(jacobians)" class="headerlink" title="Permanent link">#</a></h4>              (<code><span title="torch.Tensor">Tensor</span>(b1, C, P)</code>)
          –
          <div class="doc-md-description">
            
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._kernel_batch_star(batch)" class="doc doc-heading doc-heading-parameter">              <b><code>batch</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._kernel_batch_star(batch)" class="headerlink" title="Permanent link">#</a></h4>              (<code><span title="torch.Tensor">Tensor</span>(b2, C)</code>)
          –
          <div class="doc-md-description">
            
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>kernel</code></b> (              <code><span title="torch.tensor">tensor</span></code>
)          –
          <div class="doc-md-description">
            <p>K_batch_star with shape (b1, b2, C, C)</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>laplace/baselaplace.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-3004">3004</a></span>
<span class="normal"><a href="#__codelineno-0-3005">3005</a></span>
<span class="normal"><a href="#__codelineno-0-3006">3006</a></span>
<span class="normal"><a href="#__codelineno-0-3007">3007</a></span>
<span class="normal"><a href="#__codelineno-0-3008">3008</a></span>
<span class="normal"><a href="#__codelineno-0-3009">3009</a></span>
<span class="normal"><a href="#__codelineno-0-3010">3010</a></span>
<span class="normal"><a href="#__codelineno-0-3011">3011</a></span>
<span class="normal"><a href="#__codelineno-0-3012">3012</a></span>
<span class="normal"><a href="#__codelineno-0-3013">3013</a></span>
<span class="normal"><a href="#__codelineno-0-3014">3014</a></span>
<span class="normal"><a href="#__codelineno-0-3015">3015</a></span>
<span class="normal"><a href="#__codelineno-0-3016">3016</a></span>
<span class="normal"><a href="#__codelineno-0-3017">3017</a></span>
<span class="normal"><a href="#__codelineno-0-3018">3018</a></span>
<span class="normal"><a href="#__codelineno-0-3019">3019</a></span>
<span class="normal"><a href="#__codelineno-0-3020">3020</a></span>
<span class="normal"><a href="#__codelineno-0-3021">3021</a></span>
<span class="normal"><a href="#__codelineno-0-3022">3022</a></span>
<span class="normal"><a href="#__codelineno-0-3023">3023</a></span>
<span class="normal"><a href="#__codelineno-0-3024">3024</a></span>
<span class="normal"><a href="#__codelineno-0-3025">3025</a></span>
<span class="normal"><a href="#__codelineno-0-3026">3026</a></span>
<span class="normal"><a href="#__codelineno-0-3027">3027</a></span>
<span class="normal"><a href="#__codelineno-0-3028">3028</a></span>
<span class="normal"><a href="#__codelineno-0-3029">3029</a></span>
<span class="normal"><a href="#__codelineno-0-3030">3030</a></span>
<span class="normal"><a href="#__codelineno-0-3031">3031</a></span>
<span class="normal"><a href="#__codelineno-0-3032">3032</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-3004" name="__codelineno-0-3004"></a><span class="k">def</span> <span class="nf">_kernel_batch_star</span><span class="p">(</span>
<a id="__codelineno-0-3005" name="__codelineno-0-3005"></a>    <span class="bp">self</span><span class="p">,</span> <span class="n">jacobians</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<a id="__codelineno-0-3006" name="__codelineno-0-3006"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-3007" name="__codelineno-0-3007"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute K_b_star, which is a part of K_M_star kernel matrix.</span>
<a id="__codelineno-0-3008" name="__codelineno-0-3008"></a>
<a id="__codelineno-0-3009" name="__codelineno-0-3009"></a><span class="sd">    Parameters</span>
<a id="__codelineno-0-3010" name="__codelineno-0-3010"></a><span class="sd">    ----------</span>
<a id="__codelineno-0-3011" name="__codelineno-0-3011"></a><span class="sd">    jacobians : torch.Tensor (b1, C, P)</span>
<a id="__codelineno-0-3012" name="__codelineno-0-3012"></a><span class="sd">    batch : torch.Tensor (b2, C)</span>
<a id="__codelineno-0-3013" name="__codelineno-0-3013"></a>
<a id="__codelineno-0-3014" name="__codelineno-0-3014"></a><span class="sd">    Returns</span>
<a id="__codelineno-0-3015" name="__codelineno-0-3015"></a><span class="sd">    -------</span>
<a id="__codelineno-0-3016" name="__codelineno-0-3016"></a><span class="sd">    kernel : torch.tensor</span>
<a id="__codelineno-0-3017" name="__codelineno-0-3017"></a><span class="sd">        K_batch_star with shape (b1, b2, C, C)</span>
<a id="__codelineno-0-3018" name="__codelineno-0-3018"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-3019" name="__codelineno-0-3019"></a>    <span class="n">jacobians_2</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jacobians</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<a id="__codelineno-0-3020" name="__codelineno-0-3020"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">independent_outputs</span><span class="p">:</span>
<a id="__codelineno-0-3021" name="__codelineno-0-3021"></a>        <span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
<a id="__codelineno-0-3022" name="__codelineno-0-3022"></a>            <span class="p">(</span><span class="n">jacobians</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">jacobians_2</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">),</span>
<a id="__codelineno-0-3023" name="__codelineno-0-3023"></a>            <span class="n">device</span><span class="o">=</span><span class="n">jacobians</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<a id="__codelineno-0-3024" name="__codelineno-0-3024"></a>            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span>
<a id="__codelineno-0-3025" name="__codelineno-0-3025"></a>        <span class="p">)</span>
<a id="__codelineno-0-3026" name="__codelineno-0-3026"></a>        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs</span><span class="p">):</span>
<a id="__codelineno-0-3027" name="__codelineno-0-3027"></a>            <span class="n">kernel</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
<a id="__codelineno-0-3028" name="__codelineno-0-3028"></a>                <span class="s2">&quot;bp,ep-&gt;be&quot;</span><span class="p">,</span> <span class="n">jacobians</span><span class="p">[:,</span> <span class="n">c</span><span class="p">,</span> <span class="p">:],</span> <span class="n">jacobians_2</span><span class="p">[:,</span> <span class="n">c</span><span class="p">,</span> <span class="p">:]</span>
<a id="__codelineno-0-3029" name="__codelineno-0-3029"></a>            <span class="p">)</span>
<a id="__codelineno-0-3030" name="__codelineno-0-3030"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-3031" name="__codelineno-0-3031"></a>        <span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bcp,dep-&gt;bdce&quot;</span><span class="p">,</span> <span class="n">jacobians</span><span class="p">,</span> <span class="n">jacobians_2</span><span class="p">)</span>
<a id="__codelineno-0-3032" name="__codelineno-0-3032"></a>    <span class="k">return</span> <span class="n">kernel</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="laplace.baselaplace.FunctionalLaplace._jacobians" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">_jacobians</span>


<a href="#laplace.baselaplace.FunctionalLaplace._jacobians" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nf">_jacobians</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span><span class="p">,</span> <span class="n">enable_backprop</span><span class="p">:</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/functions.html#bool">bool</a></span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><a class="autorefs autorefs-external" href="https://docs.python.org/3/library/stdtypes.html#tuple">tuple</a></span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>A wrapper function to compute jacobians - this enables reusing same
kernel methods (kernel_batch etc.) in FunctionalLaplace and FunctionalLLLaplace
by simply overwriting this method instead of all kernel methods.</p>

            <details class="quote">
              <summary>Source code in <code>laplace/baselaplace.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-3034">3034</a></span>
<span class="normal"><a href="#__codelineno-0-3035">3035</a></span>
<span class="normal"><a href="#__codelineno-0-3036">3036</a></span>
<span class="normal"><a href="#__codelineno-0-3037">3037</a></span>
<span class="normal"><a href="#__codelineno-0-3038">3038</a></span>
<span class="normal"><a href="#__codelineno-0-3039">3039</a></span>
<span class="normal"><a href="#__codelineno-0-3040">3040</a></span>
<span class="normal"><a href="#__codelineno-0-3041">3041</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-3034" name="__codelineno-0-3034"></a><span class="k">def</span> <span class="nf">_jacobians</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">enable_backprop</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
<a id="__codelineno-0-3035" name="__codelineno-0-3035"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;A wrapper function to compute jacobians - this enables reusing same</span>
<a id="__codelineno-0-3036" name="__codelineno-0-3036"></a><span class="sd">    kernel methods (kernel_batch etc.) in FunctionalLaplace and FunctionalLLLaplace</span>
<a id="__codelineno-0-3037" name="__codelineno-0-3037"></a><span class="sd">    by simply overwriting this method instead of all kernel methods.</span>
<a id="__codelineno-0-3038" name="__codelineno-0-3038"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-3039" name="__codelineno-0-3039"></a>    <span class="k">if</span> <span class="n">enable_backprop</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-3040" name="__codelineno-0-3040"></a>        <span class="n">enable_backprop</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_backprop</span>
<a id="__codelineno-0-3041" name="__codelineno-0-3041"></a>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">jacobians</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">enable_backprop</span><span class="o">=</span><span class="n">enable_backprop</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="laplace.baselaplace.FunctionalLaplace._mean_scatter_term_batch" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">_mean_scatter_term_batch</span>


<a href="#laplace.baselaplace.FunctionalLaplace._mean_scatter_term_batch" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nf">_mean_scatter_term_batch</span><span class="p">(</span><span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._mean_scatter_term_batch(Js)">Js</a></span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._mean_scatter_term_batch(f)">f</a></span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace._mean_scatter_term_batch(y)">y</a></span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span><span class="p">)</span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Compute mean vector in the scatter term in the log marginal likelihood</p>
<p>See <code>scatter_lml</code> property above for the exact equations of mean vectors in scatter terms for
both types of likelihood (regression, classification).</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._mean_scatter_term_batch(Js)" class="doc doc-heading doc-heading-parameter">              <b><code>Js</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._mean_scatter_term_batch(Js)" class="headerlink" title="Permanent link">#</a></h4>              (<code><span title="torch.tensor">tensor</span></code>)
          –
          <div class="doc-md-description">
            <p>Jacobians (batch, output_shape, parameters)</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._mean_scatter_term_batch(f)" class="doc doc-heading doc-heading-parameter">              <b><code>f</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._mean_scatter_term_batch(f)" class="headerlink" title="Permanent link">#</a></h4>              (<code><span title="torch.tensor">tensor</span></code>)
          –
          <div class="doc-md-description">
            <p>NN output (batch, output_shape)</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace._mean_scatter_term_batch(y)" class="doc doc-heading doc-heading-parameter">              <b><code>y</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace._mean_scatter_term_batch(y)" class="headerlink" title="Permanent link">#</a></h4>              (<code><span title="torch.Tensor">Tensor</span></code>)
          –
          <div class="doc-md-description">
            <p>data labels (batch, output_shape)</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>mu</code></b> (              <code><span title="torch.tensor">tensor</span></code>
)          –
          <div class="doc-md-description">
            <p>K_batch_star with shape (batch, output_shape)</p>
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>laplace/baselaplace.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-3043">3043</a></span>
<span class="normal"><a href="#__codelineno-0-3044">3044</a></span>
<span class="normal"><a href="#__codelineno-0-3045">3045</a></span>
<span class="normal"><a href="#__codelineno-0-3046">3046</a></span>
<span class="normal"><a href="#__codelineno-0-3047">3047</a></span>
<span class="normal"><a href="#__codelineno-0-3048">3048</a></span>
<span class="normal"><a href="#__codelineno-0-3049">3049</a></span>
<span class="normal"><a href="#__codelineno-0-3050">3050</a></span>
<span class="normal"><a href="#__codelineno-0-3051">3051</a></span>
<span class="normal"><a href="#__codelineno-0-3052">3052</a></span>
<span class="normal"><a href="#__codelineno-0-3053">3053</a></span>
<span class="normal"><a href="#__codelineno-0-3054">3054</a></span>
<span class="normal"><a href="#__codelineno-0-3055">3055</a></span>
<span class="normal"><a href="#__codelineno-0-3056">3056</a></span>
<span class="normal"><a href="#__codelineno-0-3057">3057</a></span>
<span class="normal"><a href="#__codelineno-0-3058">3058</a></span>
<span class="normal"><a href="#__codelineno-0-3059">3059</a></span>
<span class="normal"><a href="#__codelineno-0-3060">3060</a></span>
<span class="normal"><a href="#__codelineno-0-3061">3061</a></span>
<span class="normal"><a href="#__codelineno-0-3062">3062</a></span>
<span class="normal"><a href="#__codelineno-0-3063">3063</a></span>
<span class="normal"><a href="#__codelineno-0-3064">3064</a></span>
<span class="normal"><a href="#__codelineno-0-3065">3065</a></span>
<span class="normal"><a href="#__codelineno-0-3066">3066</a></span>
<span class="normal"><a href="#__codelineno-0-3067">3067</a></span>
<span class="normal"><a href="#__codelineno-0-3068">3068</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-3043" name="__codelineno-0-3043"></a><span class="k">def</span> <span class="nf">_mean_scatter_term_batch</span><span class="p">(</span>
<a id="__codelineno-0-3044" name="__codelineno-0-3044"></a>    <span class="bp">self</span><span class="p">,</span> <span class="n">Js</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">f</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<a id="__codelineno-0-3045" name="__codelineno-0-3045"></a><span class="p">):</span>
<a id="__codelineno-0-3046" name="__codelineno-0-3046"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute mean vector in the scatter term in the log marginal likelihood</span>
<a id="__codelineno-0-3047" name="__codelineno-0-3047"></a>
<a id="__codelineno-0-3048" name="__codelineno-0-3048"></a><span class="sd">    See `scatter_lml` property above for the exact equations of mean vectors in scatter terms for</span>
<a id="__codelineno-0-3049" name="__codelineno-0-3049"></a><span class="sd">    both types of likelihood (regression, classification).</span>
<a id="__codelineno-0-3050" name="__codelineno-0-3050"></a>
<a id="__codelineno-0-3051" name="__codelineno-0-3051"></a><span class="sd">    Parameters</span>
<a id="__codelineno-0-3052" name="__codelineno-0-3052"></a><span class="sd">    ----------</span>
<a id="__codelineno-0-3053" name="__codelineno-0-3053"></a><span class="sd">    Js : torch.tensor</span>
<a id="__codelineno-0-3054" name="__codelineno-0-3054"></a><span class="sd">          Jacobians (batch, output_shape, parameters)</span>
<a id="__codelineno-0-3055" name="__codelineno-0-3055"></a><span class="sd">    f : torch.tensor</span>
<a id="__codelineno-0-3056" name="__codelineno-0-3056"></a><span class="sd">          NN output (batch, output_shape)</span>
<a id="__codelineno-0-3057" name="__codelineno-0-3057"></a><span class="sd">    y: torch.tensor</span>
<a id="__codelineno-0-3058" name="__codelineno-0-3058"></a><span class="sd">          data labels (batch, output_shape)</span>
<a id="__codelineno-0-3059" name="__codelineno-0-3059"></a>
<a id="__codelineno-0-3060" name="__codelineno-0-3060"></a><span class="sd">    Returns</span>
<a id="__codelineno-0-3061" name="__codelineno-0-3061"></a><span class="sd">    -------</span>
<a id="__codelineno-0-3062" name="__codelineno-0-3062"></a><span class="sd">    mu : torch.tensor</span>
<a id="__codelineno-0-3063" name="__codelineno-0-3063"></a><span class="sd">        K_batch_star with shape (batch, output_shape)</span>
<a id="__codelineno-0-3064" name="__codelineno-0-3064"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-3065" name="__codelineno-0-3065"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span> <span class="o">==</span> <span class="n">Likelihood</span><span class="o">.</span><span class="n">REGRESSION</span><span class="p">:</span>
<a id="__codelineno-0-3066" name="__codelineno-0-3066"></a>        <span class="k">return</span> <span class="n">y</span> <span class="o">-</span> <span class="p">(</span><span class="n">f</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bcp,p-&gt;bc&quot;</span><span class="p">,</span> <span class="n">Js</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior_mean</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">))</span>
<a id="__codelineno-0-3067" name="__codelineno-0-3067"></a>    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span> <span class="o">==</span> <span class="n">Likelihood</span><span class="o">.</span><span class="n">CLASSIFICATION</span><span class="p">:</span>
<a id="__codelineno-0-3068" name="__codelineno-0-3068"></a>        <span class="k">return</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bcp,p-&gt;bc&quot;</span><span class="p">,</span> <span class="n">Js</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">prior_mean</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="laplace.baselaplace.FunctionalLaplace.log_marginal_likelihood" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>            <span class="doc doc-object-name doc-function-name">log_marginal_likelihood</span>


<a href="#laplace.baselaplace.FunctionalLaplace.log_marginal_likelihood" class="headerlink" title="Permanent link">#</a></h3>
<div class="doc-signature highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nf">log_marginal_likelihood</span><span class="p">(</span><span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace.log_marginal_likelihood(prior_precision)">prior_precision</a></span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n"><a class="autorefs autorefs-internal" href="#laplace.baselaplace.FunctionalLaplace.log_marginal_likelihood(sigma_noise)">sigma_noise</a></span><span class="p">:</span> <span class="n"><span title="torch.Tensor">Tensor</span></span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n"><span title="torch.Tensor">Tensor</span></span>
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Compute the Laplace approximation to the log marginal likelihood.
Requires that the Laplace approximation has been fit before.
The resulting torch.Tensor is differentiable in <code>prior_precision</code> and
<code>sigma_noise</code> if these have gradients enabled.
By passing <code>prior_precision</code> or <code>sigma_noise</code>, the current value is
overwritten. This is useful for iterating on the log marginal likelihood.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace.log_marginal_likelihood(prior_precision)" class="doc doc-heading doc-heading-parameter">              <b><code>prior_precision</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace.log_marginal_likelihood(prior_precision)" class="headerlink" title="Permanent link">#</a></h4>              (<code><span title="torch.Tensor">Tensor</span></code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>prior precision if should be changed from current <code>prior_precision</code> value</p>
          </div>
        </li>
        <li class="doc-section-item field-body">
<h4 id="laplace.baselaplace.FunctionalLaplace.log_marginal_likelihood(sigma_noise)" class="doc doc-heading doc-heading-parameter">              <b><code>sigma_noise</code></b>
<a href="#laplace.baselaplace.FunctionalLaplace.log_marginal_likelihood(sigma_noise)" class="headerlink" title="Permanent link">#</a></h4>              (<code><span title="torch.Tensor">Tensor</span></code>, default:
                  <code>None</code>
)
          –
          <div class="doc-md-description">
            <p>observation noise standard deviation if should be changed</p>
          </div>
        </li>
    </ul>


<p><span class="doc-section-title">Returns:</span></p>
    <ul>
        <li class="doc-section-item field-body">
<b><code>log_marglik</code></b> (              <code><span title="torch.Tensor">Tensor</span></code>
)          –
          <div class="doc-md-description">
            
          </div>
        </li>
    </ul>

            <details class="quote">
              <summary>Source code in <code>laplace/baselaplace.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-3070">3070</a></span>
<span class="normal"><a href="#__codelineno-0-3071">3071</a></span>
<span class="normal"><a href="#__codelineno-0-3072">3072</a></span>
<span class="normal"><a href="#__codelineno-0-3073">3073</a></span>
<span class="normal"><a href="#__codelineno-0-3074">3074</a></span>
<span class="normal"><a href="#__codelineno-0-3075">3075</a></span>
<span class="normal"><a href="#__codelineno-0-3076">3076</a></span>
<span class="normal"><a href="#__codelineno-0-3077">3077</a></span>
<span class="normal"><a href="#__codelineno-0-3078">3078</a></span>
<span class="normal"><a href="#__codelineno-0-3079">3079</a></span>
<span class="normal"><a href="#__codelineno-0-3080">3080</a></span>
<span class="normal"><a href="#__codelineno-0-3081">3081</a></span>
<span class="normal"><a href="#__codelineno-0-3082">3082</a></span>
<span class="normal"><a href="#__codelineno-0-3083">3083</a></span>
<span class="normal"><a href="#__codelineno-0-3084">3084</a></span>
<span class="normal"><a href="#__codelineno-0-3085">3085</a></span>
<span class="normal"><a href="#__codelineno-0-3086">3086</a></span>
<span class="normal"><a href="#__codelineno-0-3087">3087</a></span>
<span class="normal"><a href="#__codelineno-0-3088">3088</a></span>
<span class="normal"><a href="#__codelineno-0-3089">3089</a></span>
<span class="normal"><a href="#__codelineno-0-3090">3090</a></span>
<span class="normal"><a href="#__codelineno-0-3091">3091</a></span>
<span class="normal"><a href="#__codelineno-0-3092">3092</a></span>
<span class="normal"><a href="#__codelineno-0-3093">3093</a></span>
<span class="normal"><a href="#__codelineno-0-3094">3094</a></span>
<span class="normal"><a href="#__codelineno-0-3095">3095</a></span>
<span class="normal"><a href="#__codelineno-0-3096">3096</a></span>
<span class="normal"><a href="#__codelineno-0-3097">3097</a></span>
<span class="normal"><a href="#__codelineno-0-3098">3098</a></span>
<span class="normal"><a href="#__codelineno-0-3099">3099</a></span>
<span class="normal"><a href="#__codelineno-0-3100">3100</a></span>
<span class="normal"><a href="#__codelineno-0-3101">3101</a></span>
<span class="normal"><a href="#__codelineno-0-3102">3102</a></span>
<span class="normal"><a href="#__codelineno-0-3103">3103</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-3070" name="__codelineno-0-3070"></a><span class="k">def</span> <span class="nf">log_marginal_likelihood</span><span class="p">(</span>
<a id="__codelineno-0-3071" name="__codelineno-0-3071"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-3072" name="__codelineno-0-3072"></a>    <span class="n">prior_precision</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-3073" name="__codelineno-0-3073"></a>    <span class="n">sigma_noise</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-3074" name="__codelineno-0-3074"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-3075" name="__codelineno-0-3075"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the Laplace approximation to the log marginal likelihood.</span>
<a id="__codelineno-0-3076" name="__codelineno-0-3076"></a><span class="sd">    Requires that the Laplace approximation has been fit before.</span>
<a id="__codelineno-0-3077" name="__codelineno-0-3077"></a><span class="sd">    The resulting torch.Tensor is differentiable in `prior_precision` and</span>
<a id="__codelineno-0-3078" name="__codelineno-0-3078"></a><span class="sd">    `sigma_noise` if these have gradients enabled.</span>
<a id="__codelineno-0-3079" name="__codelineno-0-3079"></a><span class="sd">    By passing `prior_precision` or `sigma_noise`, the current value is</span>
<a id="__codelineno-0-3080" name="__codelineno-0-3080"></a><span class="sd">    overwritten. This is useful for iterating on the log marginal likelihood.</span>
<a id="__codelineno-0-3081" name="__codelineno-0-3081"></a>
<a id="__codelineno-0-3082" name="__codelineno-0-3082"></a><span class="sd">    Parameters</span>
<a id="__codelineno-0-3083" name="__codelineno-0-3083"></a><span class="sd">    ----------</span>
<a id="__codelineno-0-3084" name="__codelineno-0-3084"></a><span class="sd">    prior_precision : torch.Tensor, optional</span>
<a id="__codelineno-0-3085" name="__codelineno-0-3085"></a><span class="sd">        prior precision if should be changed from current `prior_precision` value</span>
<a id="__codelineno-0-3086" name="__codelineno-0-3086"></a><span class="sd">    sigma_noise : torch.Tensor, optional</span>
<a id="__codelineno-0-3087" name="__codelineno-0-3087"></a><span class="sd">        observation noise standard deviation if should be changed</span>
<a id="__codelineno-0-3088" name="__codelineno-0-3088"></a>
<a id="__codelineno-0-3089" name="__codelineno-0-3089"></a><span class="sd">    Returns</span>
<a id="__codelineno-0-3090" name="__codelineno-0-3090"></a><span class="sd">    -------</span>
<a id="__codelineno-0-3091" name="__codelineno-0-3091"></a><span class="sd">    log_marglik : torch.Tensor</span>
<a id="__codelineno-0-3092" name="__codelineno-0-3092"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-3093" name="__codelineno-0-3093"></a>    <span class="c1"># update prior precision (useful when iterating on marglik)</span>
<a id="__codelineno-0-3094" name="__codelineno-0-3094"></a>    <span class="k">if</span> <span class="n">prior_precision</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-3095" name="__codelineno-0-3095"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">prior_precision</span> <span class="o">=</span> <span class="n">prior_precision</span>
<a id="__codelineno-0-3096" name="__codelineno-0-3096"></a>
<a id="__codelineno-0-3097" name="__codelineno-0-3097"></a>    <span class="c1"># update sigma_noise (useful when iterating on marglik)</span>
<a id="__codelineno-0-3098" name="__codelineno-0-3098"></a>    <span class="k">if</span> <span class="n">sigma_noise</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-3099" name="__codelineno-0-3099"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span> <span class="o">!=</span> <span class="n">Likelihood</span><span class="o">.</span><span class="n">REGRESSION</span><span class="p">:</span>
<a id="__codelineno-0-3100" name="__codelineno-0-3100"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Can only change sigma_noise for regression.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-3101" name="__codelineno-0-3101"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">sigma_noise</span> <span class="o">=</span> <span class="n">sigma_noise</span>
<a id="__codelineno-0-3102" name="__codelineno-0-3102"></a>
<a id="__codelineno-0-3103" name="__codelineno-0-3103"></a>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_likelihood</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_det_ratio</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">scatter</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../parametriclaplace/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Parametric Laplace">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Parametric Laplace
              </div>
            </div>
          </a>
        
        
          
          <a href="../lllaplace/" class="md-footer__link md-footer__link--next" aria-label="Next: Last-Layer Laplace">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Last-Layer Laplace
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/aleximmer/laplace" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://pypi.org/project/laplace-torch/" target="_blank" rel="noopener" title="pypi.org" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6M286.2 404c11.1 0 20.1 9.1 20.1 20.3 0 11.3-9 20.4-20.1 20.4-11 0-20.1-9.2-20.1-20.4.1-11.3 9.1-20.3 20.1-20.3M167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4m-6.7-142.6c-11.1 0-20.1-9.1-20.1-20.3.1-11.3 9-20.4 20.1-20.4 11 0 20.1 9.2 20.1 20.4s-9 20.3-20.1 20.3"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
      <div class="md-progress" data-md-component="progress" role="progressbar"></div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.instant", "navigation.instant.prefetch", "navigation.instant.progress", "navigation.indexes", "navigation.tracking", "content.code.annotate", "toc.follow", "navigation.footer", "navigation.top", "content.code.copy", "content.tabs.link"], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>